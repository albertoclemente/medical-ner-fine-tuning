{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb06cd0",
   "metadata": {},
   "source": [
    "# Medical NER Fine-Tuning with Llama 3.2 3B + LoRA\n",
    "\n",
    "This notebook implements fine-tuning of Llama 3.2 3B Instruct for medical Named Entity Recognition (NER) using:\n",
    "- **SFT** (Supervised Fine-Tuning)\n",
    "- **LoRA** (Low-Rank Adaptation)\n",
    "- **Hugging Face Hub** integration for checkpoint uploads\n",
    "\n",
    "## Tasks:\n",
    "1. Chemical entity extraction\n",
    "2. Disease entity extraction\n",
    "3. Chemical-Disease relationship extraction\n",
    "\n",
    "## Dataset:\n",
    "- 3,000 medical text examples\n",
    "- 80/10/10 train/validation/test split\n",
    "- **‚ö†Ô∏è CRITICAL**: Data is shuffled before splitting to ensure balanced task distribution\n",
    "- Weights & Biases tracking enabled\n",
    "\n",
    "## Important Note:\n",
    "**Data splitting MUST use `shuffle=True`** to prevent task imbalance. Without shuffling, all relationship extraction examples may cluster in validation/test sets, leading to poor model performance on the most important task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f6f14",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running this notebook!\n",
    "\n",
    "Required:\n",
    "- `HF_TOKEN`: Your Hugging Face token (needed to save models to HF Hub)\n",
    "\n",
    "Optional:\n",
    "- `WANDB_API_KEY`: Your Weights & Biases API key (for training tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Secure token input - use environment variables or prompt for input\n",
    "# Option 1: Set environment variables before running notebook (recommended for automation)\n",
    "# Option 2: Enter tokens when prompted (recommended for interactive use)\n",
    "\n",
    "# Hugging Face Token (required for uploading to HF Hub)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token or hf_token == \"hf_YOUR_TOKEN_HERE\":\n",
    "    hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "# Weights & Biases API Key (optional, for training tracking)\n",
    "wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if not wandb_key:\n",
    "    use_wandb = input(\"Use Weights & Biases for tracking? (y/n): \").lower() == 'y'\n",
    "    if use_wandb:\n",
    "        wandb_key = getpass.getpass(\"Enter your W&B API key (or press Enter to use cached login): \")\n",
    "        if wandb_key:\n",
    "            os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "\n",
    "# Verify environment variables\n",
    "print(\"‚úì Environment variables configured\")\n",
    "print(f\"  HF_TOKEN: {'‚úì Set' if os.environ.get('HF_TOKEN') else '‚úó Not set'}\")\n",
    "print(f\"  WANDB_API_KEY: {'‚úì Set' if os.environ.get('WANDB_API_KEY') else '‚óã Will use cached login or prompt later'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82d981",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers trl scikit-learn\n",
    "!pip install -q scipy sentencepiece protobuf wandb hf_transfer\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84c1fe",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5945c-9f62-4951-8f29-4bcf67268b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ad90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51827af6",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Update `HF_USERNAME` with your Hugging Face username!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Section\n",
    "from datetime import datetime\n",
    "\n",
    "HF_USERNAME = \"albyos\"  # Replace with your HF username\n",
    "\n",
    "# Generate timestamp for checkpoint naming\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "HF_MODEL_ID = f\"{HF_USERNAME}/llama3-medical-ner-lora-{TIMESTAMP}\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MODEL_NAME = BASE_MODEL  # Alias for consistency\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training Configuration\n",
    "NUM_EPOCHS = 5  # Increased from 3 to 5 for better learning of pipe format\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 5e-5  # Reduced from 2e-4 to prevent over-prediction and improve precision\n",
    "\n",
    "# Data Configuration\n",
    "TRAIN_SPLIT_RATIO = 0.9\n",
    "RANDOM_SEED = 42\n",
    "RESHUFFLE_SPLITS_EACH_RUN = True  # When True, create a fresh validation split every run\n",
    "SPLIT_SEED = random.randint(0, 1_000_000) if RESHUFFLE_SPLITS_EACH_RUN else RANDOM_SEED\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  Training timestamp: {TIMESTAMP}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE} (reduced for precision)\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Data split seed: {SPLIT_SEED} ({'reshuffled' if RESHUFFLE_SPLITS_EACH_RUN else 'fixed'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a54d9",
   "metadata": {},
   "source": [
    "## 0) Reusable Utilities\n",
    "\n",
    "These utility functions provide text normalization, hashing for deduplication, parsing, and validation for the data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Utilities: normalization, hashing, parsing =====\n",
    "import re, json, hashlib\n",
    "from collections import Counter\n",
    "\n",
    "def dehyphenate(s: str) -> str:\n",
    "    # Join words broken across lines with hyphens + whitespace\n",
    "    return re.sub(r\"(\\w+)-\\s+(\\w+)\", r\"\\1\\2\", s)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = dehyphenate(s or \"\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\u00A0\\t\\r\\n]+\", \" \", s)     # spaces/newlines\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def prompt_hash(prompt: str) -> str:\n",
    "    return hashlib.md5(normalize_text(prompt).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def parse_bullets(text: str):\n",
    "    items = []\n",
    "    for line in (text or \"\").splitlines():\n",
    "        m = re.match(r\"^\\s*[-*]\\s*(.+?)\\s*$\", line)\n",
    "        if m:\n",
    "            items.append(m.group(1))\n",
    "    return items\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    # Keep hyphens intact (e.g., \"type-2 diabetes\" stays \"type-2 diabetes\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # Only normalize whitespace\n",
    "    s = re.sub(r\"[\\.,;:]+$\", \"\", s).strip()\n",
    "    return s\n",
    "\n",
    "def in_text(item: str, text: str) -> bool:\n",
    "    return normalize_item(item) in normalize_text(text)\n",
    "\n",
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc274801",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Authentication\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token and hf_token != \"hf_YOUR_TOKEN_HERE\":\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úì Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"‚ö† HF_TOKEN not set. Please update Cell 3 before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb723a0b",
   "metadata": {},
   "source": [
    "## 4b. Weights & Biases Setup\n",
    "\n",
    "Initialize W&B to track training metrics, validation loss, and experiments.\n",
    "Get your API key from: https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740959ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Weights & Biases\n",
    "wandb_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "if wandb_key and wandb_key != 'your_wandb_key_here':\n",
    "    wandb.login(key=wandb_key)\n",
    "    print('‚úì Logged in to Weights & Biases using WANDB_API_KEY')\n",
    "else:\n",
    "    print('‚ö† Warning: WANDB_API_KEY not set. Attempting to use cached login...')\n",
    "    try:\n",
    "        wandb.login()\n",
    "        print('‚úì Logged in to Weights & Biases using cached credentials')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö† Warning: Could not login to W&B: {e}')\n",
    "        print('  Run wandb.login() interactively or set WANDB_API_KEY environment variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51134743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"medical-ner-finetuning\",\n",
    "    name=f\"llama3-medical-ner-{TIMESTAMP}\",\n",
    "    config={\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"lora_rank\": LORA_RANK,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Weights & Biases initialized\")\n",
    "print(f\"  Project: medical-ner-finetuning\")\n",
    "print(f\"  Run name: llama3-medical-ner-{TIMESTAMP}\")\n",
    "print(f\"  Dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7c1b6",
   "metadata": {},
   "source": [
    "## 5. Data Exploration\n",
    "\n",
    "Let's examine the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the dataset\n",
    "# Load data\n",
    "with open('both_rel_instruct_all.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"\\nSample structure:\")\n",
    "print(json.dumps(data[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze task distribution\n",
    "# IMPORTANT: Check for \"influences between\" FIRST because those prompts also contain \"chemicals\" and \"diseases\"\n",
    "task_counts = {}\n",
    "for sample in data:\n",
    "    if \"influences between\" in sample['prompt']:\n",
    "        task = \"Relationship Extraction\"\n",
    "    elif \"chemicals mentioned\" in sample['prompt']:\n",
    "        task = \"Chemical Extraction\"\n",
    "    elif \"diseases mentioned\" in sample['prompt']:\n",
    "        task = \"Disease Extraction\"\n",
    "    else:\n",
    "        task = \"Other\"\n",
    "    \n",
    "    task_counts[task] = task_counts.get(task, 0) + 1\n",
    "\n",
    "print(\"Task Distribution:\")\n",
    "for task, count in task_counts.items():\n",
    "    print(f\"  {task}: {count} ({count/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example from each task type\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE: Chemical Extraction\")\n",
    "print(\"=\"*80)\n",
    "chem_example = [s for s in data if \"chemicals mentioned\" in s['prompt'] and \"influences between\" not in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{chem_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{chem_example['completion']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: Disease Extraction\")\n",
    "print(\"=\"*80)\n",
    "disease_example = [s for s in data if \"diseases mentioned\" in s['prompt'] and \"influences between\" not in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{disease_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{disease_example['completion']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: Relationship Extraction\")\n",
    "print(\"=\"*80)\n",
    "rel_example = [s for s in data if \"influences between\" in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{rel_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{rel_example['completion']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e86457",
   "metadata": {},
   "source": [
    "## 6. Leak-Free Stratified Dataset Splitting\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL**: Using **leak-free stratified splitting** with deduplication to prevent data contamination!\n",
    "\n",
    "**Key Improvements**:\n",
    "1. **Deduplication**: Remove duplicate prompts (by normalized hash) before splitting\n",
    "2. **Stratified Splitting**: Ensures exact task distribution across all splits\n",
    "3. **Leakage Prevention**: Hard assertions verify zero overlap between train/val/test\n",
    "\n",
    "Split into:\n",
    "- **80% Training** - for fine-tuning\n",
    "- **10% Validation** - for monitoring during training (W&B)\n",
    "- **10% Test** - for final evaluation after training\n",
    "\n",
    "**Guaranteed distribution in each split** (with stratification):\n",
    "- **Exactly 33.3%** Chemical extraction\n",
    "- **Exactly 33.3%** Disease extraction  \n",
    "- **Exactly 33.3%** Relationship extraction\n",
    "\n",
    "**Why this matters**:\n",
    "- Duplicates can leak between splits, causing overfitting\n",
    "- Normalized prompts ensure semantic uniqueness\n",
    "- Hard assertions catch any data leakage bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) Deduplicate + Stratified Split (80/10/10) =====\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random, os, time\n",
    "\n",
    "random.seed(SPLIT_SEED)\n",
    "\n",
    "# 1.1 Load full dataset into `data` (already loaded above)\n",
    "assert isinstance(data, list) and len(data) > 0, \"Load `data` first\"\n",
    "\n",
    "# 1.1.5 Define task classifier (needed for format conversion)\n",
    "def task_from_prompt(prompt: str) -> str:\n",
    "    p = normalize_text(prompt)\n",
    "    if \"list of extracted chemicals\" in p: return \"chemicals\"\n",
    "    if \"list of extracted diseases\"  in p: return \"diseases\"\n",
    "    if \"list of extracted influences\" in p: return \"influences\"\n",
    "    # Fallback patterns\n",
    "    if \"chemicals mentioned\" in p: return \"chemicals\"\n",
    "    if \"diseases mentioned\" in p: return \"diseases\"\n",
    "    if \"influences between\" in p: return \"influences\"\n",
    "    return \"other\"\n",
    "\n",
    "# 1.2 Deduplicate by normalized prompt\n",
    "seen = set()\n",
    "clean = []\n",
    "duplicates_removed = 0\n",
    "for row in data:\n",
    "    ph = prompt_hash(row[\"prompt\"])\n",
    "    if ph in seen: \n",
    "        duplicates_removed += 1\n",
    "        continue\n",
    "    seen.add(ph)\n",
    "    \n",
    "    # Dedupe and normalize completion lines\n",
    "    items = unique_preserve_order(parse_bullets(row.get(\"completion\",\"\")))\n",
    "    items_norm = []\n",
    "    \n",
    "    # Convert OLD sentence format to NEW pipe format for influences\n",
    "    task = task_from_prompt(row[\"prompt\"])\n",
    "    for item in items:\n",
    "        if not item.strip():\n",
    "            continue\n",
    "        \n",
    "        if task == \"influences\":\n",
    "            # Convert: \"chemical X influences disease Y\" ‚Üí \"X | Y\"\n",
    "            m = re.match(r'^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$', item, re.I)\n",
    "            if m:\n",
    "                chem = normalize_item(m.group(1))\n",
    "                dis = normalize_item(m.group(2))\n",
    "                items_norm.append(f\"{chem} | {dis}\")\n",
    "            else:\n",
    "                # Already in pipe format or other format - keep as-is\n",
    "                items_norm.append(normalize_item(item))\n",
    "        else:\n",
    "            # Chemicals or diseases - just normalize\n",
    "            items_norm.append(normalize_item(item))\n",
    "    \n",
    "    items_norm = unique_preserve_order(items_norm)\n",
    "    row[\"completion\"] = \"\\n\".join(f\"- {x}\" for x in items_norm)\n",
    "    clean.append(row)\n",
    "\n",
    "data = clean\n",
    "print(f\"‚úì Deduplication complete:\")\n",
    "print(f\"  Original samples: {len(data) + duplicates_removed}\")\n",
    "print(f\"  Duplicates removed: {duplicates_removed}\")\n",
    "print(f\"  Unique samples: {len(data)}\")\n",
    "print(f\"  ‚úì Influences converted to pipe format (chemical | disease)\")\n",
    "\n",
    "# 1.3 Get task labels for stratification\n",
    "labels = [task_from_prompt(r[\"prompt\"]) for r in data]\n",
    "print(f\"\\n‚úì Task label distribution: {dict(Counter(labels))}\")\n",
    "\n",
    "# 1.4 First split: 80% train, 20% temp\n",
    "train_data, temp_data, train_y, temp_y = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=SPLIT_SEED, stratify=labels\n",
    ")\n",
    "\n",
    "# 1.5 Second split: 10% val, 10% test (split temp 50/50)\n",
    "val_data, test_data, val_y, test_y = train_test_split(\n",
    "    temp_data, temp_y, test_size=0.5, random_state=SPLIT_SEED+1, stratify=temp_y\n",
    ")\n",
    "\n",
    "# 1.6 Check for leakage\n",
    "def check_leak(a, b, name):\n",
    "    ha = {prompt_hash(r[\"prompt\"]) for r in a}\n",
    "    hb = {prompt_hash(r[\"prompt\"]) for r in b}\n",
    "    overlap = ha & hb\n",
    "    print(f\"  {name}: overlap={len(overlap)}\")\n",
    "    assert len(overlap) == 0, f\"‚ùå Leak detected between {name}\"\n",
    "\n",
    "print(f\"\\n‚úì Checking for data leakage...\")\n",
    "check_leak(train_data, val_data, \"train ‚à© val\")\n",
    "check_leak(train_data, test_data, \"train ‚à© test\")\n",
    "check_leak(val_data,   test_data, \"val ‚à© test\")\n",
    "print(f\"‚úì No leakage detected - all splits are independent!\")\n",
    "\n",
    "# 1.7 Write files\n",
    "out_train = \"train.jsonl\"\n",
    "out_val   = \"validation.jsonl\"\n",
    "out_test  = \"test.jsonl\"\n",
    "\n",
    "for path, split in [(out_train,train_data),(out_val,val_data),(out_test,test_data)]:\n",
    "    with open(path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for r in split:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\n‚úì Dataset split complete (seed={SPLIT_SEED}, stratified=True, deduplicated=True)\")\n",
    "print(f\"  Train: {len(train_data)} samples ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_data)} samples ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_data)} samples ({len(test_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"\\n  Files written: {out_train}, {out_val}, {out_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9df44e",
   "metadata": {},
   "source": [
    "## üîß Critical Format Conversion Applied\n",
    "\n",
    "**Problem Identified:**  \n",
    "The source data uses OLD format for influences:\n",
    "```\n",
    "\"- chemical cyclophosphamide influences disease urinary bladder cancer\"\n",
    "```\n",
    "\n",
    "But for better model performance, we want NEW format:\n",
    "```\n",
    "\"- cyclophosphamide | urinary bladder cancer\"\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "During deduplication, we now automatically convert influences from sentence format to pipe format. This ensures:\n",
    "1. Model learns concise `\"chemical | disease\"` format\n",
    "2. Cleaner outputs with less verbosity\n",
    "3. Easier to parse during evaluation\n",
    "4. Consistent format across all relationship extractions\n",
    "\n",
    "The conversion happens in the deduplication loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify task distribution across splits\n",
    "def get_task_type_display(prompt):\n",
    "    \"\"\"Classify the task type based on prompt for display.\"\"\"\n",
    "    task = task_from_prompt(prompt)\n",
    "    display_map = {\n",
    "        \"chemicals\": \"Chemical Extraction\",\n",
    "        \"diseases\": \"Disease Extraction\", \n",
    "        \"influences\": \"Relationship Extraction\",\n",
    "        \"other\": \"Other\"\n",
    "    }\n",
    "    return display_map.get(task, \"Other\")\n",
    "\n",
    "def verify_split_distribution(split_data, split_name):\n",
    "    \"\"\"Verify task distribution in a split.\"\"\"\n",
    "    task_counts = {}\n",
    "    for sample in split_data:\n",
    "        task = get_task_type_display(sample['prompt'])\n",
    "        task_counts[task] = task_counts.get(task, 0) + 1\n",
    "    \n",
    "    print(f\"\\n{split_name} Distribution ({len(split_data)} samples):\")\n",
    "    for task, count in sorted(task_counts.items()):\n",
    "        print(f\"  {task}: {count} ({count/len(split_data)*100:.1f}%)\")\n",
    "\n",
    "verify_split_distribution(train_data, \"Train\")\n",
    "verify_split_distribution(val_data, \"Validation\")\n",
    "verify_split_distribution(test_data, \"Test\")\n",
    "\n",
    "print(\"\\n‚úì All splits have balanced task distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52498f03",
   "metadata": {},
   "source": [
    "## 6b. Post-Split Leakage Verification\n",
    "\n",
    "Double-check that written files have no overlapping prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Post-split leakage check (standalone verification) =====\n",
    "def hash_file(path):\n",
    "    \"\"\"Load all prompt hashes from a JSONL file.\"\"\"\n",
    "    out=set()\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            r=json.loads(line)\n",
    "            out.add(prompt_hash(r[\"prompt\"]))\n",
    "    return out\n",
    "\n",
    "Htrain = hash_file(\"train.jsonl\")\n",
    "Hval   = hash_file(\"validation.jsonl\")\n",
    "Htest  = hash_file(\"test.jsonl\")\n",
    "\n",
    "print(\"‚úì Verifying written files for leakage...\")\n",
    "print(f\"  train ‚à© val: {len(Htrain & Hval)} overlaps\")\n",
    "print(f\"  train ‚à© test: {len(Htrain & Htest)} overlaps\")\n",
    "print(f\"  val ‚à© test: {len(Hval & Htest)} overlaps\")\n",
    "\n",
    "assert not (Htrain & Hval),   \"‚ùå Leak detected: train ‚à© val\"\n",
    "assert not (Htrain & Htest),  \"‚ùå Leak detected: train ‚à© test\"\n",
    "assert not (Hval & Htest),    \"‚ùå Leak detected: val ‚à© test\"\n",
    "\n",
    "print(\"\\n‚úì‚úì‚úì No cross-split leakage in written files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e0e62",
   "metadata": {},
   "source": [
    "## 7. Data Formatting\n",
    "\n",
    "Format data into Llama 3 chat format with system, user, and assistant roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format data into Llama 3 chat format with strict instruction.\"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Return ONLY entities that appear verbatim in the article.\n",
    "Output one item per line, each starting with '- '.\n",
    "For relationships: use format '- chemical NAME | disease NAME'.\n",
    "For single entities: use format '- ENTITY NAME'.\n",
    "If none exist, return nothing.\n",
    "Do not add explanations or examples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{sample['prompt']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{sample['completion']}<|eot_id|>\"\"\"\n",
    "\n",
    "# Test formatting\n",
    "formatted_example = format_instruction(train_data[0])\n",
    "print(\"Formatted Example:\")\n",
    "print(formatted_example[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format all data\n",
    "train_formatted = [{\"text\": format_instruction(sample)} for sample in train_data]\n",
    "val_formatted = [{\"text\": format_instruction(sample)} for sample in val_data]\n",
    "test_formatted = [{\"text\": format_instruction(sample)} for sample in test_data]\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_formatted)\n",
    "val_dataset = Dataset.from_list(val_formatted)\n",
    "test_dataset = Dataset.from_list(test_formatted)\n",
    "\n",
    "print(f\"‚úì Datasets formatted:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7616fb",
   "metadata": {},
   "source": [
    "## 8. Load Model and Tokenizer\n",
    "\n",
    "Load Llama 3.2 3B with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3981a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Quantization config created (4-bit NF4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c092e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd890d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading model... (this may take a few minutes)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ceb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"‚úì Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b2d6c",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA\n",
    "\n",
    "Apply Low-Rank Adaptation for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b69361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,                   # LoRA rank\n",
    "    lora_alpha=LORA_ALPHA,         # LoRA alpha (scaling)\n",
    "    target_modules=[               # Layers to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,             # Dropout for regularization\n",
    "    bias=\"none\",                   # No bias training\n",
    "    task_type=\"CAUSAL_LM\"          # Causal language modeling\n",
    ")\n",
    "\n",
    "print(f\"‚úì LoRA configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {len(lora_config.target_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úì LoRA applied to model\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665501b",
   "metadata": {},
   "source": [
    "## 10. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6201d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the texts.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set tokenized: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úì Validation set tokenized: {len(tokenized_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚úì Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417fc9",
   "metadata": {},
   "source": [
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./llama3-medical-ner-lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluate every 50 steps\n",
    "    \n",
    "    # Checkpointing - Save every 50 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Checkpoint every 50 steps\n",
    "    save_total_limit=None,  # Keep all checkpoints (will push to HF with unique names)\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=True,\n",
    "    \n",
    "    # Hugging Face Hub - Disable default push (we'll use custom callback)\n",
    "    push_to_hub=False,  # Custom callback will handle timestamped uploads\n",
    "    \n",
    "    # Misc\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",  # Enable Weights & Biases logging\n",
    "    run_name=f\"llama3-medical-ner-{TIMESTAMP}\",  # W&B run name\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Checkpoint frequency: Every {training_args.save_steps} steps\")\n",
    "print(f\"  Base HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  ‚ö†Ô∏è Checkpoints will be pushed to HF with timestamp suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3b101",
   "metadata": {},
   "source": [
    "## 11b. Custom Checkpoint Upload Callback\n",
    "\n",
    "This callback will automatically push each checkpoint to Hugging Face Hub with a unique timestamped name every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a78c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from huggingface_hub import HfApi\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "class CheckpointUploadCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to upload checkpoints to Hugging Face Hub with timestamped names.\n",
    "    Deletes local checkpoints after successful upload to save disk space.\n",
    "    \n",
    "    Each checkpoint will be saved with format:\n",
    "    {HF_USERNAME}/llama3-medical-ner-checkpoint-{step}-{timestamp}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_id, hf_username, delete_after_upload=True):\n",
    "        self.base_model_id = base_model_id\n",
    "        self.hf_username = hf_username\n",
    "        self.delete_after_upload = delete_after_upload\n",
    "        self.api = HfApi()\n",
    "        \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Called when a checkpoint is saved.\n",
    "        Uploads the checkpoint to HF Hub with a timestamped name, then deletes local copy.\n",
    "        \"\"\"\n",
    "        # Get the checkpoint directory that was just saved\n",
    "        checkpoint_dir = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n",
    "        \n",
    "        if not Path(checkpoint_dir).exists():\n",
    "            print(f\"‚ö†Ô∏è Checkpoint directory not found: {checkpoint_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Create timestamped model ID\n",
    "        checkpoint_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_model_id = f\"{self.hf_username}/llama3-medical-ner-checkpoint-{state.global_step}-{checkpoint_timestamp}\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üì§ Uploading checkpoint to Hugging Face Hub\")\n",
    "        print(f\"   Step: {state.global_step}\")\n",
    "        print(f\"   Model ID: {checkpoint_model_id}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Create repository if it doesn't exist\n",
    "            try:\n",
    "                self.api.create_repo(\n",
    "                    repo_id=checkpoint_model_id,\n",
    "                    repo_type=\"model\",\n",
    "                    exist_ok=True,\n",
    "                    private=False\n",
    "                )\n",
    "                print(f\"‚úì Repository created/verified: {checkpoint_model_id}\")\n",
    "            except Exception as create_error:\n",
    "                print(f\"‚ö†Ô∏è  Repository creation note: {create_error}\")\n",
    "            \n",
    "            # Upload the checkpoint folder to HF Hub\n",
    "            self.api.upload_folder(\n",
    "                folder_path=checkpoint_dir,\n",
    "                repo_id=checkpoint_model_id,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=f\"Checkpoint at step {state.global_step}\",\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Checkpoint uploaded successfully!\")\n",
    "            print(f\"   URL: https://huggingface.co/{checkpoint_model_id}\")\n",
    "            \n",
    "            # Delete local checkpoint after successful upload\n",
    "            if self.delete_after_upload:\n",
    "                try:\n",
    "                    shutil.rmtree(checkpoint_dir)\n",
    "                    print(f\"üóëÔ∏è  Local checkpoint deleted to save disk space\")\n",
    "                except Exception as delete_error:\n",
    "                    print(f\"‚ö†Ô∏è  Could not delete local checkpoint: {delete_error}\")\n",
    "            \n",
    "            print()  # Empty line for readability\n",
    "            \n",
    "            # Log to wandb if available\n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\n",
    "                    \"checkpoint_step\": state.global_step,\n",
    "                    \"checkpoint_url\": f\"https://huggingface.co/{checkpoint_model_id}\"\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to upload checkpoint: {e}\")\n",
    "            print(f\"   Checkpoint saved locally at: {checkpoint_dir}\\n\")\n",
    "\n",
    "# Initialize the callback with automatic deletion after upload\n",
    "checkpoint_upload_callback = CheckpointUploadCallback(\n",
    "    base_model_id=HF_MODEL_ID,\n",
    "    hf_username=HF_USERNAME,\n",
    "    delete_after_upload=True  # Delete local checkpoints after upload\n",
    ")\n",
    "\n",
    "print(f\"‚úì Checkpoint upload callback initialized\")\n",
    "print(f\"  Checkpoints will be uploaded to: {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"  Upload frequency: Every {training_args.save_steps} steps\")\n",
    "print(f\"  Local deletion: Enabled (checkpoints deleted after upload)\")\n",
    "print(f\"  Auto-create repos: Enabled (repos will be created automatically)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview expected checkpoint uploads\n",
    "total_steps_estimate = (len(train_data) // (BATCH_SIZE * GRADIENT_ACCUMULATION)) * NUM_EPOCHS\n",
    "checkpoint_count = total_steps_estimate // 50\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHECKPOINT UPLOAD PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Total samples: {len(train_data)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Estimated total steps: ~{total_steps_estimate}\")\n",
    "print(f\"\\nCheckpoint Configuration:\")\n",
    "print(f\"  Frequency: Every 50 steps\")\n",
    "print(f\"  Expected checkpoints: ~{checkpoint_count}\")\n",
    "print(f\"  Local storage: ./llama3-medical-ner-lora/checkpoint-<step>/\")\n",
    "print(f\"\\nHugging Face Upload:\")\n",
    "print(f\"  Format: {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"\\nExample checkpoint names:\")\n",
    "for i, step in enumerate(range(50, min(250, total_steps_estimate), 50), 1):\n",
    "    example_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"  {i}. {HF_USERNAME}/llama3-medical-ner-checkpoint-{step}-{example_time}\")\n",
    "if checkpoint_count > 4:\n",
    "    print(f\"  ... (~{checkpoint_count - 4} more checkpoints)\")\n",
    "    \n",
    "print(f\"\\nFinal model:\")\n",
    "print(f\"  {HF_USERNAME}/llama3-medical-ner-lora-final-<timestamp>\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a993c",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b88564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[checkpoint_upload_callback],  # Add custom checkpoint upload callback\n",
    ")\n",
    "\n",
    "# Configure early stopping to prevent overfitting\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0))\n",
    "\n",
    "# Calculate training steps\n",
    "total_steps = (len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n",
    "\n",
    "print(f\"‚úì Trainer initialized\")\n",
    "print(f\"‚úì Expected training steps: ~{total_steps}\")\n",
    "print(f\"‚úì Expected checkpoints: ~{max(1, total_steps // training_args.save_steps)}\")\n",
    "print(f\"‚úì Checkpoint upload callback enabled\")\n",
    "print(\"‚úì Early stopping enabled (patience = 3 evaluations)\")\n",
    "print(f\"\\nüìã Checkpoint naming format:\")\n",
    "print(f\"   {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"\\n   Example: {HF_USERNAME}/llama3-medical-ner-checkpoint-50-20251104_143022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b853b4",
   "metadata": {},
   "source": [
    "## 13. Start Training\n",
    "\n",
    "‚ö†Ô∏è **This will take 2-3 hours on an A100 GPU**\n",
    "\n",
    "The training will:\n",
    "- **Save checkpoints every 50 steps** to local disk\n",
    "- **Upload each checkpoint to Hugging Face Hub** with timestamped names\n",
    "  - Format: `{username}/llama3-medical-ner-checkpoint-{step}-{timestamp}`\n",
    "  - Example: `albyos/llama3-medical-ner-checkpoint-50-20251104_143022`\n",
    "- Evaluate on validation set every 50 steps\n",
    "- Save the best model based on validation loss\n",
    "- Log all metrics to Weights & Biases\n",
    "\n",
    "**Checkpoint URLs will be printed during training and logged to W&B.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39341273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take 2-3 hours on A100 GPU...\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b3b39",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93373e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "\n",
    "print(f\"‚úì Model saved to: ./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push final model to Hugging Face Hub with timestamped name\n",
    "print(\"Pushing final model to Hugging Face Hub...\")\n",
    "\n",
    "# Create final model ID with timestamp\n",
    "final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_model_id = f\"{HF_USERNAME}/llama3-medical-ner-lora-final-{final_timestamp}\"\n",
    "\n",
    "try:\n",
    "    # Push the final model\n",
    "    model.push_to_hub(\n",
    "        final_model_id,\n",
    "        commit_message=\"Training complete - final model\"\n",
    "    )\n",
    "    tokenizer.push_to_hub(\n",
    "        final_model_id,\n",
    "        commit_message=\"Training complete - final tokenizer\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Final model pushed successfully!\")\n",
    "    print(f\"   Model ID: {final_model_id}\")\n",
    "    print(f\"   URL: https://huggingface.co/{final_model_id}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"final_model_url\": f\"https://huggingface.co/{final_model_id}\",\n",
    "            \"final_model_id\": final_model_id\n",
    "        })\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Failed to push to hub: {e}\")\n",
    "    print(\"  Final model saved locally at: ./final_model\")\n",
    "    print(f\"  You can manually push later using:\")\n",
    "    print(f\"    model.push_to_hub('{final_model_id}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed8e1e",
   "metadata": {},
   "source": [
    "## 15. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract losses\n",
    "train_loss = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "eval_loss = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_loss, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Evaluation Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training metrics plotted and saved to: training_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training steps: {len(train_loss)}\")\n",
    "print(f\"Final training loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {eval_loss[-1]:.4f}\")\n",
    "print(f\"Best validation loss: {min(eval_loss):.4f}\")\n",
    "print(f\"Loss reduction: {((train_loss[0] - train_loss[-1]) / train_loss[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdcba1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Training is complete! Your model has been saved.\n",
    "\n",
    "**To evaluate your model:**\n",
    "1. Open `Medical_NER_Evaluation.ipynb`\n",
    "2. Run the evaluation on the test set\n",
    "3. Test custom examples\n",
    "\n",
    "**Model locations:**\n",
    "- Local: `./final_model`\n",
    "- HuggingFace Hub: Check the output above for your model URL\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
