{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b2ee7b",
   "metadata": {},
   "source": [
    "# Deep Data Exploration for Medical NER Fine-Tuning\n",
    "\n",
    "This notebook performs comprehensive analysis of the medical NER dataset to inform fine-tuning decisions.\n",
    "\n",
    "## Analysis Goals:\n",
    "1. **Format Analysis**: Understand prompt/completion structures\n",
    "2. **Entity Patterns**: Analyze chemical and disease naming patterns\n",
    "3. **Relationship Patterns**: Study chemical-disease influence formats\n",
    "4. **Data Distribution**: Optimal train/val/test splits\n",
    "5. **Complexity Metrics**: Text length, entity counts, vocabulary\n",
    "6. **Quality Assessment**: Identify potential issues or edge cases\n",
    "\n",
    "## Key Questions:\n",
    "- What are the most common entity patterns?\n",
    "- Are there format inconsistencies to normalize?\n",
    "- What's the optimal data split ratio?\n",
    "- What vocabulary size should the model handle?\n",
    "- Are there rare patterns that need attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519df40c",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "data_path = '../data/both_rel_instruct_all.jsonl'\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"✓ Loaded {len(data):,} samples from {data_path}\")\n",
    "print(f\"\\nSample keys: {list(data[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2c104",
   "metadata": {},
   "source": [
    "## 2. Task Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b191e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each sample by task type\n",
    "def classify_task(sample):\n",
    "    \"\"\"Classify the task type based on prompt.\"\"\"\n",
    "    prompt = sample['prompt'].lower()\n",
    "    \n",
    "    # Check for relationship extraction first (most specific)\n",
    "    if 'influences between' in prompt:\n",
    "        return 'Relationship Extraction'\n",
    "    elif 'chemicals mentioned' in prompt:\n",
    "        return 'Chemical Extraction'\n",
    "    elif 'diseases mentioned' in prompt:\n",
    "        return 'Disease Extraction'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Classify all samples\n",
    "task_labels = [classify_task(sample) for sample in data]\n",
    "task_counts = Counter(task_labels)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df = pd.DataFrame(data)\n",
    "df['task_type'] = task_labels\n",
    "\n",
    "print(\"Task Distribution:\")\n",
    "print(\"=\"*50)\n",
    "for task, count in task_counts.most_common():\n",
    "    percentage = (count / len(data)) * 100\n",
    "    print(f\"{task:30s}: {count:5d} ({percentage:5.2f}%)\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette('Set2', len(task_counts))\n",
    "ax.bar(task_counts.keys(), task_counts.values(), color=colors)\n",
    "ax.set_xlabel('Task Type', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.set_title('Task Distribution in Medical NER Dataset', fontsize=14, fontweight='bold')\n",
    "for i, (task, count) in enumerate(task_counts.items()):\n",
    "    ax.text(i, count + 20, f'{count}\\n({count/len(data)*100:.1f}%)', \n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Dataset is {'balanced' if max(task_counts.values()) / min(task_counts.values()) < 1.5 else 'imbalanced'} across tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17284df",
   "metadata": {},
   "source": [
    "## 3. Prompt Format Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prompt structure\n",
    "print(\"=\"*80)\n",
    "print(\"PROMPT FORMAT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get examples of each task type\n",
    "examples = {}\n",
    "for task_type in task_counts.keys():\n",
    "    examples[task_type] = df[df['task_type'] == task_type].iloc[0]\n",
    "\n",
    "for task_type, example in examples.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Task: {task_type}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Show prompt structure\n",
    "    prompt_lines = example['prompt'].split('\\n')\n",
    "    print(f\"\\nPrompt structure ({len(prompt_lines)} lines):\")\n",
    "    print(f\"  First line: {prompt_lines[0][:100]}...\")\n",
    "    print(f\"  Last line: {prompt_lines[-1][:100]}...\")\n",
    "    \n",
    "    # Show full prompt (truncated)\n",
    "    print(f\"\\nFull prompt (first 400 chars):\")\n",
    "    print(example['prompt'][:400] + \"...\\n\")\n",
    "    \n",
    "    # Show completion\n",
    "    print(f\"Completion:\")\n",
    "    print(example['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prompt lengths\n",
    "df['prompt_length'] = df['prompt'].str.len()\n",
    "df['prompt_words'] = df['prompt'].str.split().str.len()\n",
    "\n",
    "print(\"Prompt Length Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(df[['prompt_length', 'prompt_words']].describe())\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character length\n",
    "axes[0].hist(df['prompt_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['prompt_length'].median(), color='red', linestyle='--', label=f'Median: {df[\"prompt_length\"].median():.0f}')\n",
    "axes[0].set_xlabel('Prompt Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Prompt Lengths')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count\n",
    "axes[1].hist(df['prompt_words'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].axvline(df['prompt_words'].median(), color='red', linestyle='--', label=f'Median: {df[\"prompt_words\"].median():.0f}')\n",
    "axes[1].set_xlabel('Prompt Length (words)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Prompt Word Counts')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Prompts range from {df['prompt_length'].min()} to {df['prompt_length'].max()} characters\")\n",
    "print(f\"✓ Median prompt: {df['prompt_length'].median():.0f} chars, {df['prompt_words'].median():.0f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0428c",
   "metadata": {},
   "source": [
    "## 4. Completion Format Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse completion items\n",
    "def parse_completion(completion):\n",
    "    \"\"\"Extract bullet items from completion.\"\"\"\n",
    "    items = []\n",
    "    for line in completion.split('\\n'):\n",
    "        match = re.match(r'^\\s*[-*]\\s*(.+?)\\s*$', line)\n",
    "        if match:\n",
    "            items.append(match.group(1))\n",
    "    return items\n",
    "\n",
    "df['completion_items'] = df['completion'].apply(parse_completion)\n",
    "df['num_items'] = df['completion_items'].apply(len)\n",
    "\n",
    "print(\"Completion Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(df['num_items'].describe())\n",
    "\n",
    "# Plot item counts by task type\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "df.boxplot(column='num_items', by='task_type', ax=ax)\n",
    "ax.set_xlabel('Task Type', fontsize=12)\n",
    "ax.set_ylabel('Number of Extracted Items', fontsize=12)\n",
    "ax.set_title('Distribution of Extracted Items by Task Type', fontsize=14, fontweight='bold')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show statistics per task\n",
    "print(\"\\nItems per task:\")\n",
    "print(df.groupby('task_type')['num_items'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze completion formats - detect OLD vs NEW format\n",
    "print(\"\\nCompletion Format Detection:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def detect_format(items, task_type):\n",
    "    \"\"\"Detect if items use OLD (sentence) or NEW (pipe) format.\"\"\"\n",
    "    if task_type != 'Relationship Extraction':\n",
    "        return 'N/A'\n",
    "    \n",
    "    formats = []\n",
    "    for item in items:\n",
    "        if '|' in item:\n",
    "            formats.append('NEW (pipe)')\n",
    "        elif 'influences' in item.lower():\n",
    "            formats.append('OLD (sentence)')\n",
    "        else:\n",
    "            formats.append('unknown')\n",
    "    \n",
    "    if not formats:\n",
    "        return 'empty'\n",
    "    \n",
    "    # Return most common format\n",
    "    return Counter(formats).most_common(1)[0][0]\n",
    "\n",
    "df['completion_format'] = df.apply(lambda row: detect_format(row['completion_items'], row['task_type']), axis=1)\n",
    "\n",
    "# Show format distribution for relationship extraction\n",
    "rel_df = df[df['task_type'] == 'Relationship Extraction']\n",
    "format_counts = rel_df['completion_format'].value_counts()\n",
    "\n",
    "print(\"\\nRelationship Extraction Format Analysis:\")\n",
    "for fmt, count in format_counts.items():\n",
    "    print(f\"  {fmt:20s}: {count:5d} ({count/len(rel_df)*100:5.2f}%)\")\n",
    "\n",
    "# Show examples of each format\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FORMAT EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for fmt in ['OLD (sentence)', 'NEW (pipe)']:\n",
    "    if fmt in format_counts:\n",
    "        example = rel_df[rel_df['completion_format'] == fmt].iloc[0]\n",
    "        print(f\"\\n{fmt} format:\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        for i, item in enumerate(example['completion_items'][:3], 1):\n",
    "            print(f\"  {i}. {item}\")\n",
    "        if len(example['completion_items']) > 3:\n",
    "            print(f\"  ... ({len(example['completion_items']) - 3} more items)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6cb01",
   "metadata": {},
   "source": [
    "## 5. Entity Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786417e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all entities by type\n",
    "chemicals = []\n",
    "diseases = []\n",
    "influences = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if row['task_type'] == 'Chemical Extraction':\n",
    "        chemicals.extend(row['completion_items'])\n",
    "    elif row['task_type'] == 'Disease Extraction':\n",
    "        diseases.extend(row['completion_items'])\n",
    "    elif row['task_type'] == 'Relationship Extraction':\n",
    "        influences.extend(row['completion_items'])\n",
    "\n",
    "print(\"Entity Counts:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Total chemicals: {len(chemicals):,}\")\n",
    "print(f\"  Total diseases: {len(diseases):,}\")\n",
    "print(f\"  Total influences: {len(influences):,}\")\n",
    "print(f\"\\n  Unique chemicals: {len(set([c.lower() for c in chemicals])):,}\")\n",
    "print(f\"  Unique diseases: {len(set([d.lower() for d in diseases])):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common entities\n",
    "chem_counter = Counter([c.lower() for c in chemicals])\n",
    "disease_counter = Counter([d.lower() for d in diseases])\n",
    "\n",
    "print(\"\\nTop 20 Most Common Chemicals:\")\n",
    "print(\"=\"*50)\n",
    "for i, (chem, count) in enumerate(chem_counter.most_common(20), 1):\n",
    "    print(f\"  {i:2d}. {chem:40s} ({count:3d} occurrences)\")\n",
    "\n",
    "print(\"\\n\\nTop 20 Most Common Diseases:\")\n",
    "print(\"=\"*50)\n",
    "for i, (disease, count) in enumerate(disease_counter.most_common(20), 1):\n",
    "    print(f\"  {i:2d}. {disease:40s} ({count:3d} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a92f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entity name patterns\n",
    "def analyze_naming_patterns(entities, entity_type):\n",
    "    \"\"\"Analyze patterns in entity naming.\"\"\"\n",
    "    patterns = {\n",
    "        'single_word': 0,\n",
    "        'multi_word': 0,\n",
    "        'hyphenated': 0,\n",
    "        'with_numbers': 0,\n",
    "        'with_parentheses': 0,\n",
    "        'with_special_chars': 0,\n",
    "    }\n",
    "    \n",
    "    lengths = []\n",
    "    word_counts = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity = entity.strip()\n",
    "        lengths.append(len(entity))\n",
    "        words = entity.split()\n",
    "        word_counts.append(len(words))\n",
    "        \n",
    "        if len(words) == 1:\n",
    "            patterns['single_word'] += 1\n",
    "        else:\n",
    "            patterns['multi_word'] += 1\n",
    "        \n",
    "        if '-' in entity:\n",
    "            patterns['hyphenated'] += 1\n",
    "        if re.search(r'\\d', entity):\n",
    "            patterns['with_numbers'] += 1\n",
    "        if '(' in entity or ')' in entity:\n",
    "            patterns['with_parentheses'] += 1\n",
    "        if re.search(r'[^a-zA-Z0-9\\s\\-()]', entity):\n",
    "            patterns['with_special_chars'] += 1\n",
    "    \n",
    "    print(f\"\\n{entity_type} Naming Patterns:\")\n",
    "    print(\"=\"*50)\n",
    "    total = len(entities)\n",
    "    for pattern, count in patterns.items():\n",
    "        print(f\"  {pattern:25s}: {count:5d} ({count/total*100:5.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n  Avg length: {np.mean(lengths):.1f} chars\")\n",
    "    print(f\"  Avg word count: {np.mean(word_counts):.2f} words\")\n",
    "    print(f\"  Max word count: {max(word_counts)} words\")\n",
    "    \n",
    "    return patterns, lengths, word_counts\n",
    "\n",
    "chem_patterns, chem_lengths, chem_words = analyze_naming_patterns(chemicals, \"Chemical\")\n",
    "disease_patterns, disease_lengths, disease_words = analyze_naming_patterns(diseases, \"Disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Chemical length distribution\n",
    "axes[0, 0].hist(chem_lengths, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0, 0].axvline(np.mean(chem_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(chem_lengths):.1f}')\n",
    "axes[0, 0].set_xlabel('Character Length')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Chemical Name Length Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Disease length distribution\n",
    "axes[0, 1].hist(disease_lengths, bins=50, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "axes[0, 1].axvline(np.mean(disease_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(disease_lengths):.1f}')\n",
    "axes[0, 1].set_xlabel('Character Length')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Disease Name Length Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Chemical word count\n",
    "axes[1, 0].hist(chem_words, bins=range(1, max(chem_words)+2), edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[1, 0].set_xlabel('Word Count')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Chemical Name Word Count Distribution')\n",
    "axes[1, 0].set_xticks(range(1, min(max(chem_words)+1, 10)))\n",
    "\n",
    "# Disease word count\n",
    "axes[1, 1].hist(disease_words, bins=range(1, max(disease_words)+2), edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "axes[1, 1].set_xlabel('Word Count')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Disease Name Word Count Distribution')\n",
    "axes[1, 1].set_xticks(range(1, min(max(disease_words)+1, 10)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66528d23",
   "metadata": {},
   "source": [
    "## 6. Relationship Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse relationships from both formats\n",
    "def parse_relationship(item):\n",
    "    \"\"\"Extract chemical and disease from relationship item.\"\"\"\n",
    "    # NEW format: \"chemical | disease\"\n",
    "    if '|' in item:\n",
    "        parts = item.split('|')\n",
    "        if len(parts) == 2:\n",
    "            return parts[0].strip(), parts[1].strip(), 'pipe'\n",
    "    \n",
    "    # OLD format: \"chemical X influences disease Y\"\n",
    "    match = re.match(r'^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$', item, re.I)\n",
    "    if match:\n",
    "        return match.group(1).strip(), match.group(2).strip(), 'sentence'\n",
    "    \n",
    "    return None, None, 'unknown'\n",
    "\n",
    "# Parse all relationships\n",
    "relationships = []\n",
    "for influence in influences:\n",
    "    chem, disease, fmt = parse_relationship(influence)\n",
    "    if chem and disease:\n",
    "        relationships.append({\n",
    "            'chemical': chem.lower(),\n",
    "            'disease': disease.lower(),\n",
    "            'format': fmt,\n",
    "            'raw': influence\n",
    "        })\n",
    "\n",
    "rel_df_analysis = pd.DataFrame(relationships)\n",
    "\n",
    "print(f\"Parsed Relationships: {len(rel_df_analysis):,}\")\n",
    "print(f\"\\nFormat distribution:\")\n",
    "print(rel_df_analysis['format'].value_counts())\n",
    "\n",
    "print(f\"\\n\\nMost Common Chemical-Disease Pairs:\")\n",
    "print(\"=\"*80)\n",
    "pair_counts = Counter([(r['chemical'], r['disease']) for r in relationships])\n",
    "for i, ((chem, disease), count) in enumerate(pair_counts.most_common(15), 1):\n",
    "    print(f\"  {i:2d}. {chem:30s} → {disease:35s} ({count:2d}x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chemical and disease frequencies in relationships\n",
    "rel_chemicals = Counter([r['chemical'] for r in relationships])\n",
    "rel_diseases = Counter([r['disease'] for r in relationships])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top chemicals in relationships\n",
    "top_chem = rel_chemicals.most_common(15)\n",
    "axes[0].barh([c[0] for c in top_chem][::-1], [c[1] for c in top_chem][::-1], color='skyblue')\n",
    "axes[0].set_xlabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Top 15 Chemicals in Relationships', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Top diseases in relationships\n",
    "top_dis = rel_diseases.most_common(15)\n",
    "axes[1].barh([d[0] for d in top_dis][::-1], [d[1] for d in top_dis][::-1], color='lightcoral')\n",
    "axes[1].set_xlabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Top 15 Diseases in Relationships', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ {len(rel_chemicals)} unique chemicals in relationships\")\n",
    "print(f\"✓ {len(rel_diseases)} unique diseases in relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edc9381",
   "metadata": {},
   "source": [
    "## 7. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential issues\n",
    "print(\"Data Quality Checks:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Empty completions\n",
    "empty_completions = df[df['num_items'] == 0]\n",
    "print(f\"\\n1. Empty completions: {len(empty_completions)} ({len(empty_completions)/len(df)*100:.2f}%)\")\n",
    "if len(empty_completions) > 0:\n",
    "    print(f\"   Task distribution:\")\n",
    "    print(empty_completions['task_type'].value_counts())\n",
    "\n",
    "# 2. Very short prompts (potentially incomplete)\n",
    "short_prompts = df[df['prompt_length'] < 100]\n",
    "print(f\"\\n2. Very short prompts (<100 chars): {len(short_prompts)} ({len(short_prompts)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# 3. Very long prompts (potential truncation issues)\n",
    "long_prompts = df[df['prompt_length'] > 2000]\n",
    "print(f\"\\n3. Very long prompts (>2000 chars): {len(long_prompts)} ({len(long_prompts)/len(df)*100:.2f}%)\")\n",
    "if len(long_prompts) > 0:\n",
    "    print(f\"   Max length: {long_prompts['prompt_length'].max()} chars\")\n",
    "    print(f\"   Recommended max_length for tokenization: {int(long_prompts['prompt_length'].max() * 1.3)}\")\n",
    "\n",
    "# 4. Duplicate prompts\n",
    "duplicate_prompts = df[df.duplicated(subset='prompt', keep=False)]\n",
    "print(f\"\\n4. Duplicate prompts: {len(duplicate_prompts)} ({len(duplicate_prompts)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# 5. Format inconsistencies in relationships\n",
    "if 'completion_format' in df.columns:\n",
    "    unknown_format = df[df['completion_format'] == 'unknown']\n",
    "    print(f\"\\n5. Unknown format relationships: {len(unknown_format)} ({len(unknown_format)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for special characters that might need normalization\n",
    "def find_special_chars(text_list):\n",
    "    \"\"\"Find all special characters in text.\"\"\"\n",
    "    all_chars = set()\n",
    "    for text in text_list:\n",
    "        all_chars.update(set(text))\n",
    "    \n",
    "    # Filter to non-alphanumeric\n",
    "    special = {c for c in all_chars if not c.isalnum() and not c.isspace()}\n",
    "    return sorted(special)\n",
    "\n",
    "all_entities = chemicals + diseases\n",
    "special_chars = find_special_chars(all_entities)\n",
    "\n",
    "print(\"\\nSpecial Characters in Entities:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Found {len(special_chars)} unique special characters:\")\n",
    "print(f\"  {', '.join(repr(c) for c in special_chars)}\")\n",
    "\n",
    "# Show examples with each special char\n",
    "print(\"\\nExamples:\")\n",
    "for char in special_chars[:5]:  # Show first 5\n",
    "    examples = [e for e in all_entities if char in e][:3]\n",
    "    print(f\"  {repr(char)}: {', '.join(examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fae13c",
   "metadata": {},
   "source": [
    "## 8. Optimal Split Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recommended split sizes\n",
    "total_samples = len(df)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Different split strategies\n",
    "strategies = [\n",
    "    (\"80/10/10\", 0.80, 0.10, 0.10),\n",
    "    (\"70/15/15\", 0.70, 0.15, 0.15),\n",
    "    (\"85/10/5\", 0.85, 0.10, 0.05),\n",
    "    (\"90/5/5\", 0.90, 0.05, 0.05),\n",
    "]\n",
    "\n",
    "for name, train_pct, val_pct, test_pct in strategies:\n",
    "    train_size = int(total_samples * train_pct)\n",
    "    val_size = int(total_samples * val_pct)\n",
    "    test_size = total_samples - train_size - val_size\n",
    "    \n",
    "    print(f\"\\n{name} Split:\")\n",
    "    print(f\"  Train: {train_size:4d} samples ({train_size/total_samples*100:.1f}%)\")\n",
    "    print(f\"  Val:   {val_size:4d} samples ({val_size/total_samples*100:.1f}%)\")\n",
    "    print(f\"  Test:  {test_size:4d} samples ({test_size/total_samples*100:.1f}%)\")\n",
    "    \n",
    "    # Check if validation set is large enough for reliable metrics\n",
    "    if val_size < 100:\n",
    "        print(f\"  ⚠️  Validation set may be too small (<100 samples)\")\n",
    "    if test_size < 100:\n",
    "        print(f\"  ⚠️  Test set may be too small (<100 samples)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ Use 80/10/10 split (current approach)\")\n",
    "print(f\"✓ Train: ~{int(total_samples * 0.8)} samples (sufficient for fine-tuning)\")\n",
    "print(f\"✓ Val:   ~{int(total_samples * 0.1)} samples (reliable for early stopping)\")\n",
    "print(f\"✓ Test:  ~{int(total_samples * 0.1)} samples (statistically significant)\")\n",
    "print(f\"\\n✓ CRITICAL: Use stratified splitting to maintain task balance!\")\n",
    "print(f\"✓ CRITICAL: Deduplicate before splitting to prevent leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f7058",
   "metadata": {},
   "source": [
    "## 9. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4363410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from all text\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize_simple(text):\n",
    "    \"\"\"Simple word tokenization.\"\"\"\n",
    "    # Keep hyphens in words like \"type-2\"\n",
    "    words = re.findall(r'\\b[\\w-]+\\b', text.lower())\n",
    "    return words\n",
    "\n",
    "# Collect all words from prompts and completions\n",
    "all_words = []\n",
    "for _, row in df.iterrows():\n",
    "    all_words.extend(tokenize_simple(row['prompt']))\n",
    "    all_words.extend(tokenize_simple(row['completion']))\n",
    "\n",
    "vocab = Counter(all_words)\n",
    "\n",
    "print(\"Vocabulary Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total words: {len(all_words):,}\")\n",
    "print(f\"Unique words: {len(vocab):,}\")\n",
    "print(f\"Vocabulary size (>1 occurrence): {len([w for w, c in vocab.items() if c > 1]):,}\")\n",
    "print(f\"Rare words (1 occurrence): {len([w for w, c in vocab.items() if c == 1]):,}\")\n",
    "\n",
    "# Most common words\n",
    "print(f\"\\nTop 30 Most Common Words:\")\n",
    "print(\"=\"*50)\n",
    "for i, (word, count) in enumerate(vocab.most_common(30), 1):\n",
    "    print(f\"  {i:2d}. {word:25s} ({count:5d} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency distribution\n",
    "frequencies = list(vocab.values())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale\n",
    "axes[0].hist(frequencies, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Word Frequency')\n",
    "axes[0].set_ylabel('Number of Words')\n",
    "axes[0].set_title('Word Frequency Distribution (Linear Scale)')\n",
    "axes[0].set_xlim(0, 100)\n",
    "\n",
    "# Log scale\n",
    "axes[1].hist(frequencies, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Word Frequency')\n",
    "axes[1].set_ylabel('Number of Words')\n",
    "axes[1].set_title('Word Frequency Distribution (Log Scale)')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Typical Zipf's law distribution observed\")\n",
    "print(f\"✓ Most words are rare (long tail)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6382b",
   "metadata": {},
   "source": [
    "## 10. Key Findings & Recommendations Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\"*80)\n",
    "print(\"DEEP DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(\"   \" + \"─\"*76)\n",
    "print(f\"   • Total samples: {len(df):,}\")\n",
    "print(f\"   • Task distribution: {dict(task_counts)}\")\n",
    "print(f\"   • Balance: {'✓ Well-balanced' if max(task_counts.values())/min(task_counts.values()) < 1.5 else '⚠️ Imbalanced'}\")\n",
    "\n",
    "print(\"\\n2. PROMPT CHARACTERISTICS\")\n",
    "print(\"   \" + \"─\"*76)\n",
    "print(f\"   • Median length: {df['prompt_length'].median():.0f} characters\")\n",
    "print(f\"   • Median words: {df['prompt_words'].median():.0f} words\")\n",
    "print(f\"   • Range: {df['prompt_length'].min()}-{df['prompt_length'].max()} characters\")\n",
    "print(f\"   • Recommended max_length: 2048 tokens\")\n",
    "\n",
    "print(\"\\n3. COMPLETION PATTERNS\")\n",
    "print(\"   \" + \"─\"*76)\n",
    "print(f\"   • Median items per sample: {df['num_items'].median():.0f}\")\n",
    "print(f\"   • Total unique chemicals: {len(set([c.lower() for c in chemicals])):,}\")\n",
    "print(f\"   • Total unique diseases: {len(set([d.lower() for d in diseases])):,}\")\n",
    "print(f\"   • Relationship format: {rel_df_analysis['format'].value_counts().to_dict() if len(rel_df_analysis) > 0 else 'N/A'}\")\n",
    "\n",
    "print(\"\\n4. ENTITY NAMING PATTERNS\")\n",
    "print(\"   \" + \"─\"*76)\n",
    "print(f\"   • Avg chemical name: {np.mean(chem_lengths):.1f} chars, {np.mean(chem_words):.1f} words\")\n",
    "print(f\"   • Avg disease name: {np.mean(disease_lengths):.1f} chars, {np.mean(disease_words):.1f} words\")\n",
    "print(f\"   • Hyphenated entities: ~{chem_patterns['hyphenated']+disease_patterns['hyphenated']:,}\")\n",
    "print(f\"   • Special characters found: {len(special_chars)}\")\n",
    "\n",
    "print(\"\\n5. DATA QUALITY\")\n",
    "print(\"   \" + \"─\"*76)\n",
    "print(f\"   • Empty completions: {len(empty_completions)} ({len(empty_completions)/len(df)*100:.2f}%)\")\n",
    "print(f\"   • Duplicate prompts: {len(duplicate_prompts)} ({len(duplicate_prompts)/len(df)*100:.2f}%)\")\n",
    "print(f\"   • Quality score: {'✓ High' if len(empty_completions) < 10 and len(duplicate_prompts)/len(df) < 0.05 else '⚠️ Needs attention'}\")\n",
    "\n",
    "print(\"\\n6. VOCABULARY\")\n",
    "print(\"   \" + \"─\"*76)\n",
    "print(f\"   • Total vocabulary: {len(vocab):,} unique words\")\n",
    "print(f\"   • Active vocabulary (>1 occurrence): {len([w for w, c in vocab.items() if c > 1]):,}\")\n",
    "print(f\"   • Rare words: {len([w for w, c in vocab.items() if c == 1]):,} (singleton)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY RECOMMENDATIONS FOR FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✓ DATA SPLIT\")\n",
    "print(\"  • Use 80/10/10 train/val/test split\")\n",
    "print(\"  • CRITICAL: Apply stratified splitting (maintain task distribution)\")\n",
    "print(\"  • CRITICAL: Deduplicate prompts before splitting\")\n",
    "print(\"  • Expected sizes: ~{:,} train, ~{:,} val, ~{:,} test\".format(\n",
    "    int(len(df)*0.8), int(len(df)*0.1), int(len(df)*0.1)\n",
    "))\n",
    "\n",
    "print(\"\\n✓ FORMAT STANDARDIZATION\")\n",
    "if 'completion_format' in df.columns:\n",
    "    old_format_count = len(df[df['completion_format'] == 'OLD (sentence)'])\n",
    "    if old_format_count > 0:\n",
    "        print(\"  • ⚠️ CONVERT relationships from OLD to NEW format during preprocessing\")\n",
    "        print(\"    OLD: 'chemical X influences disease Y'\")\n",
    "        print(\"    NEW: 'X | Y'\")\n",
    "    else:\n",
    "        print(\"  • ✓ Format already standardized\")\n",
    "print(\"  • Preserve hyphens in entity names (e.g., 'type-2 diabetes')\")\n",
    "print(\"  • System prompt should specify: '- chemical | disease' format\")\n",
    "\n",
    "print(\"\\n✓ TOKENIZATION\")\n",
    "print(\"  • Recommended max_length: 2048 tokens\")\n",
    "print(\"  • Padding side: right\")\n",
    "print(\"  • Truncation: enabled\")\n",
    "\n",
    "print(\"\\n✓ TRAINING PARAMETERS\")\n",
    "print(\"  • Batch size: 4-8 per device\")\n",
    "print(\"  • Gradient accumulation: 4 (effective batch = 16-32)\")\n",
    "print(\"  • Learning rate: 5e-5 (conservative for LoRA)\")\n",
    "print(\"  • Epochs: 5 (sufficient for format learning)\")\n",
    "print(\"  • Scheduler: cosine decay with warmup\")\n",
    "\n",
    "print(\"\\n✓ EVALUATION STRATEGY\")\n",
    "print(\"  • Metric: F1 score per task type\")\n",
    "print(\"  • Normalize entities before comparison (lowercase, trim)\")\n",
    "print(\"  • Use word boundaries for matching (prevent substring false positives)\")\n",
    "print(\"  • Track chemical F1, disease F1, influence F1 separately\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ Analysis complete! Ready for fine-tuning.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398c507",
   "metadata": {},
   "source": [
    "## 11. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "output_dir = Path('../docs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export summary statistics\n",
    "summary_stats = {\n",
    "    'total_samples': len(df),\n",
    "    'task_distribution': dict(task_counts),\n",
    "    'prompt_median_length': float(df['prompt_length'].median()),\n",
    "    'prompt_median_words': float(df['prompt_words'].median()),\n",
    "    'unique_chemicals': len(set([c.lower() for c in chemicals])),\n",
    "    'unique_diseases': len(set([d.lower() for d in diseases])),\n",
    "    'vocabulary_size': len(vocab),\n",
    "    'duplicate_prompts': len(duplicate_prompts),\n",
    "    'empty_completions': len(empty_completions),\n",
    "    'recommended_split': '80/10/10',\n",
    "    'recommended_max_length': 2048,\n",
    "}\n",
    "\n",
    "with open(output_dir / 'data_exploration_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary statistics saved to: {output_dir / 'data_exploration_summary.json'}\")\n",
    "\n",
    "# Export top entities\n",
    "entity_lists = {\n",
    "    'top_chemicals': [{'name': c, 'count': count} for c, count in chem_counter.most_common(50)],\n",
    "    'top_diseases': [{'name': d, 'count': count} for d, count in disease_counter.most_common(50)],\n",
    "    'top_chemical_disease_pairs': [\n",
    "        {'chemical': c, 'disease': d, 'count': count} \n",
    "        for (c, d), count in pair_counts.most_common(50)\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(output_dir / 'top_entities.json', 'w') as f:\n",
    "    json.dump(entity_lists, f, indent=2)\n",
    "\n",
    "print(f\"✓ Top entities saved to: {output_dir / 'top_entities.json'}\")\n",
    "\n",
    "print(\"\\n✅ All analysis results exported!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
