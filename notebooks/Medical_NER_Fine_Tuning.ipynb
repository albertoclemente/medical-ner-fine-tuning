{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb06cd0",
   "metadata": {},
   "source": [
    "# Medical NER Fine-Tuning with Llama 3.2 3B + LoRA\n",
    "\n",
    "This notebook implements fine-tuning of Llama 3.2 3B Instruct for medical Named Entity Recognition (NER) using:\n",
    "- **SFT** (Supervised Fine-Tuning)\n",
    "- **LoRA** (Low-Rank Adaptation)\n",
    "- **Hugging Face Hub** integration for checkpoint uploads\n",
    "\n",
    "## Tasks:\n",
    "1. Chemical entity extraction\n",
    "2. Disease entity extraction\n",
    "3. Chemical-Disease relationship extraction\n",
    "\n",
    "## Dataset:\n",
    "- 3,000 medical text examples\n",
    "- 80/10/10 train/validation/test split\n",
    "- Weights & Biases tracking enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82d981",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers trl scikit-learn\n",
    "!pip install -q scipy sentencepiece protobuf wandb\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84c1fe",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ad90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51827af6",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Update `HF_USERNAME` with your Hugging Face username!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Section\n",
    "from datetime import datetime\n",
    "\n",
    "HF_USERNAME = \"albyos\"  # Replace with your HF username\n",
    "\n",
    "# Generate timestamp for checkpoint naming\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "HF_MODEL_ID = f\"{HF_USERNAME}/llama3-medical-ner-lora-{TIMESTAMP}\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MODEL_NAME = BASE_MODEL  # Alias for consistency\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training Configuration\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Data Configuration\n",
    "TRAIN_SPLIT_RATIO = 0.9\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  Training timestamp: {TIMESTAMP}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc274801",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Authentication\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Set token as environment variable (recommended)\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Option 2: Interactive login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "print(\"‚úì Logged in to Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb723a0b",
   "metadata": {},
   "source": [
    "## 4b. Weights & Biases Setup\n",
    "\n",
    "Initialize W&B to track training metrics, validation loss, and experiments.\n",
    "Get your API key from: https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740959ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Weights & Biases\n",
    "import os\n",
    "\n",
    "if os.getenv('d88df098d85360ac924ec2bf8dcf5520d745c411'):\n",
    "    wandb.login(key=os.getenv('d88df098d85360ac924ec2bf8dcf5520d745c411'))\n",
    "    print('‚úì Logged in to Weights & Biases using WANDB_API_KEY')\n",
    "else:\n",
    "    print('‚ö† Warning: WANDB_API_KEY not found. Attempting to use cached login...')\n",
    "    try:\n",
    "        wandb.login()\n",
    "        print('‚úì Logged in to Weights & Biases using cached credentials')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö† Warning: Could not login to W&B: {e}')\n",
    "        print('  Run wandb.login() interactively or set WANDB_API_KEY environment variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51134743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"medical-ner-finetuning\",\n",
    "    name=f\"llama3-medical-ner-{TIMESTAMP}\",\n",
    "    config={\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"lora_rank\": LORA_RANK,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Weights & Biases initialized\")\n",
    "print(f\"  Project: medical-ner-finetuning\")\n",
    "print(f\"  Run name: llama3-medical-ner-{TIMESTAMP}\")\n",
    "print(f\"  Dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7c1b6",
   "metadata": {},
   "source": [
    "## 5. Data Exploration\n",
    "\n",
    "Let's examine the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the dataset\n",
    "# Load data\n",
    "with open('../data/both_rel_instruct_all.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"\\nSample structure:\")\n",
    "print(json.dumps(data[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze task distribution\n",
    "task_counts = {}\n",
    "for sample in data:\n",
    "    if \"chemicals mentioned\" in sample['prompt']:\n",
    "        task = \"Chemical Extraction\"\n",
    "    elif \"diseases mentioned\" in sample['prompt']:\n",
    "        task = \"Disease Extraction\"\n",
    "    elif \"influences between\" in sample['prompt']:\n",
    "        task = \"Relationship Extraction\"\n",
    "    else:\n",
    "        task = \"Other\"\n",
    "    \n",
    "    task_counts[task] = task_counts.get(task, 0) + 1\n",
    "\n",
    "print(\"Task Distribution:\")\n",
    "for task, count in task_counts.items():\n",
    "    print(f\"  {task}: {count} ({count/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example from each task type\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE: Chemical Extraction\")\n",
    "print(\"=\"*80)\n",
    "chem_example = [s for s in data if \"chemicals mentioned\" in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{chem_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{chem_example['completion']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: Disease Extraction\")\n",
    "print(\"=\"*80)\n",
    "disease_example = [s for s in data if \"diseases mentioned\" in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{disease_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{disease_example['completion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e86457",
   "metadata": {},
   "source": [
    "## 6. Dataset Splitting\n",
    "\n",
    "Split into:\n",
    "- **80% Training** (2,400 samples) - for fine-tuning\n",
    "- **10% Validation** (300 samples) - for monitoring during training (W&B)\n",
    "- **10% Test** (300 samples) - for final evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/val/test (80/10/10)\n",
    "random.seed(42)\n",
    "\n",
    "# First split: 80% train, 20% temp (for val + test)\n",
    "train_data, temp_data = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,  # 20% for validation + test\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: split the 20% into 10% val, 10% test\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    test_size=0.5,  # 50% of 20% = 10% of total\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Save splits\n",
    "with open('../data/train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('../data/validation.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('../data/test.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"‚úì Dataset split complete:\")\n",
    "print(f\"  Train samples: {len(train_data)} ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"  Validation samples: {len(val_data)} ({len(val_data)/len(data)*100:.1f}%) - for training monitoring\")\n",
    "print(f\"  Test samples: {len(test_data)} ({len(test_data)/len(data)*100:.1f}%) - for final evaluation\")\n",
    "print(f\"\\nüìä Usage:\")\n",
    "print(f\"  - Train: Used for fine-tuning\")\n",
    "print(f\"  - Validation: Monitored during training (shown in W&B)\")\n",
    "print(f\"  - Test: Used ONLY after training for final evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e0e62",
   "metadata": {},
   "source": [
    "## 7. Data Formatting\n",
    "\n",
    "Format data into Llama 3 chat format with system, user, and assistant roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format data into Llama 3 chat format.\"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{sample['prompt']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{sample['completion']}<|eot_id|>\"\"\"\n",
    "\n",
    "# Test formatting\n",
    "formatted_example = format_instruction(train_data[0])\n",
    "print(\"Formatted Example:\")\n",
    "print(formatted_example[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format all data\n",
    "train_formatted = [{\"text\": format_instruction(sample)} for sample in train_data]\n",
    "val_formatted = [{\"text\": format_instruction(sample)} for sample in val_data]\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_formatted)\n",
    "val_dataset = Dataset.from_list(val_formatted)\n",
    "\n",
    "print(f\"‚úì Datasets formatted:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7616fb",
   "metadata": {},
   "source": [
    "## 8. Load Model and Tokenizer\n",
    "\n",
    "Load Llama 3.2 3B with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3981a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Quantization config created (4-bit NF4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c092e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd890d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading model... (this may take a few minutes)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ceb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"‚úì Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b2d6c",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA\n",
    "\n",
    "Apply Low-Rank Adaptation for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b69361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,                   # LoRA rank\n",
    "    lora_alpha=LORA_ALPHA,         # LoRA alpha (scaling)\n",
    "    target_modules=[               # Layers to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,             # Dropout for regularization\n",
    "    bias=\"none\",                   # No bias training\n",
    "    task_type=\"CAUSAL_LM\"          # Causal language modeling\n",
    ")\n",
    "\n",
    "print(f\"‚úì LoRA configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {len(lora_config.target_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úì LoRA applied to model\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665501b",
   "metadata": {},
   "source": [
    "## 10. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6201d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the texts.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set tokenized: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úì Validation set tokenized: {len(tokenized_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚úì Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417fc9",
   "metadata": {},
   "source": [
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./llama3-medical-ner-lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=True,\n",
    "    \n",
    "    # Hugging Face Hub\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HF_MODEL_ID,\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    hub_private_repo=False,\n",
    "    \n",
    "    # Misc\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",  # Enable Weights & Biases logging\n",
    "    run_name=f\"llama3-medical-ner-{TIMESTAMP}\",  # W&B run name\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Hub model ID: {HF_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a993c",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b88564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Calculate training steps\n",
    "total_steps = (len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n",
    "\n",
    "print(f\"‚úì Trainer initialized\")\n",
    "print(f\"‚úì Expected training steps: ~{total_steps}\")\n",
    "print(f\"‚úì Expected checkpoints: ~{total_steps // training_args.save_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b853b4",
   "metadata": {},
   "source": [
    "## 13. Start Training\n",
    "\n",
    "‚ö†Ô∏è **This will take 2-3 hours on an A100 GPU**\n",
    "\n",
    "The training will:\n",
    "- Save checkpoints every 100 steps\n",
    "- Upload checkpoints to Hugging Face Hub\n",
    "- Evaluate on validation set every 100 steps\n",
    "- Save the best model based on validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39341273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take 2-3 hours on A100 GPU...\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b3b39",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93373e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "\n",
    "print(f\"‚úì Model saved to: ./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "print(\"Pushing to Hugging Face Hub...\")\n",
    "\n",
    "try:\n",
    "    trainer.push_to_hub(commit_message=\"Training complete - final model\")\n",
    "    print(f\"‚úì Model pushed to: https://huggingface.co/{HF_MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Failed to push to hub: {e}\")\n",
    "    print(\"  You can manually push later using: trainer.push_to_hub()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed8e1e",
   "metadata": {},
   "source": [
    "## 15. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract losses\n",
    "train_loss = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "eval_loss = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_loss, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Evaluation Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training metrics plotted and saved to: training_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training steps: {len(train_loss)}\")\n",
    "print(f\"Final training loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {eval_loss[-1]:.4f}\")\n",
    "print(f\"Best validation loss: {min(eval_loss):.4f}\")\n",
    "print(f\"Loss reduction: {((train_loss[0] - train_loss[-1]) / train_loss[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6265a9c",
   "metadata": {},
   "source": [
    "## 16. Final Evaluation on Test Set\n",
    "\n",
    "Test the fine-tuned model on **test samples that were NOT seen during training or validation**.\n",
    "The test set is completely separate - it was never used for training or monitoring.\n",
    "This gives us the truest measure of the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "print(\"Loading fine-tuned model for validation...\")\n",
    "\n",
    "# Clear GPU memory first\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./final_model\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úì Fine-tuned model loaded for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt_text, max_new_tokens=512):\n",
    "    \"\"\"Generate a response for a given prompt.\"\"\"\n",
    "    formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    if \"assistant\\n\\n\" in response:\n",
    "        response = response.split(\"assistant\\n\\n\")[-1]\n",
    "    elif \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"‚úì Inference function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6043fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on COMPLETELY UNSEEN test samples\n",
    "# The test set was not used for training OR validation monitoring\n",
    "with open('../data/test.jsonl', 'r', encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "num_test_samples = 5\n",
    "print(f\"Testing on {num_test_samples} samples from TEST SET\")\n",
    "print(f\"Total test set size: {len(test_data)}\")\n",
    "print(f\"\\n‚ö†Ô∏è  IMPORTANT:\")\n",
    "print(f\"  - Training set (80%): Used for fine-tuning\")\n",
    "print(f\"  - Validation set (10%): Monitored during training (W&B)\")\n",
    "print(f\"  - Test set (10%): Used ONLY NOW for final evaluation\")\n",
    "\n",
    "# Aggregate metrics\n",
    "total_correct = 0\n",
    "total_predicted = 0\n",
    "total_expected = 0\n",
    "\n",
    "for i, sample in enumerate(test_data[:num_test_samples]):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL TEST EXAMPLE {i+1}/{num_test_samples}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show prompt (truncated for readability)\n",
    "    print(f\"\\nüìù PROMPT:\")\n",
    "    prompt_preview = sample['prompt'][:250] + \"...\" if len(sample['prompt']) > 250 else sample['prompt']\n",
    "    print(f\"{prompt_preview}\")\n",
    "    \n",
    "    # Show expected output\n",
    "    print(f\"\\n‚úÖ EXPECTED OUTPUT:\")\n",
    "    print(f\"{sample['completion']}\")\n",
    "    \n",
    "    # Generate prediction\n",
    "    print(f\"\\nü§ñ MODEL PREDICTION:\")\n",
    "    prediction = generate_response(sample['prompt'])\n",
    "    print(f\"{prediction}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    expected_items = set([item.strip() for item in sample['completion'].split('\\n') if item.strip()])\n",
    "    predicted_items = set([item.strip() for item in prediction.split('\\n') if item.strip()])\n",
    "    \n",
    "    common = expected_items & predicted_items\n",
    "    missing = expected_items - predicted_items\n",
    "    extra = predicted_items - expected_items\n",
    "    \n",
    "    # Update aggregate counts\n",
    "    total_correct += len(common)\n",
    "    total_predicted += len(predicted_items)\n",
    "    total_expected += len(expected_items)\n",
    "    \n",
    "    # Per-sample metrics\n",
    "    accuracy = len(common) / len(expected_items) * 100 if len(expected_items) > 0 else 0\n",
    "    precision = len(common) / len(predicted_items) * 100 if len(predicted_items) > 0 else 0\n",
    "    recall = len(common) / len(expected_items) * 100 if len(expected_items) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä EVALUATION METRICS:\")\n",
    "    print(f\"  ‚úì Correct extractions: {len(common)}/{len(expected_items)}\")\n",
    "    print(f\"  ‚úó Missed extractions: {len(missing)}\")\n",
    "    print(f\"  ‚ö† Extra extractions: {len(extra)}\")\n",
    "    print(f\"\\n  üìà Per-Sample Metrics:\")\n",
    "    print(f\"    Accuracy:  {accuracy:.1f}%\")\n",
    "    print(f\"    Precision: {precision:.1f}%\")\n",
    "    print(f\"    Recall:    {recall:.1f}%\")\n",
    "    print(f\"    F1 Score:  {f1:.1f}%\")\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"\\n  Missed items: {list(missing)[:3]}\")\n",
    "    if extra:\n",
    "        print(f\"  Extra items: {list(extra)[:3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Metrics across all test samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATE METRICS ACROSS TEST SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "aggregate_precision = total_correct / total_predicted * 100 if total_predicted > 0 else 0\n",
    "aggregate_recall = total_correct / total_expected * 100 if total_expected > 0 else 0\n",
    "aggregate_f1 = 2 * (aggregate_precision * aggregate_recall) / (aggregate_precision + aggregate_recall) if (aggregate_precision + aggregate_recall) > 0 else 0\n",
    "aggregate_accuracy = total_correct / total_expected * 100 if total_expected > 0 else 0\n",
    "\n",
    "print(f\"\\nEvaluated on {num_test_samples} test samples:\")\n",
    "print(f\"\\nüìä Overall Performance:\")\n",
    "print(f\"  Total expected entities:  {total_expected}\")\n",
    "print(f\"  Total predicted entities: {total_predicted}\")\n",
    "print(f\"  Correctly predicted:      {total_correct}\")\n",
    "\n",
    "print(f\"\\nüìà Aggregate Metrics:\")\n",
    "print(f\"  Accuracy:  {aggregate_accuracy:.2f}%\")\n",
    "print(f\"  Precision: {aggregate_precision:.2f}% (fewer false positives)\")\n",
    "print(f\"  Recall:    {aggregate_recall:.2f}% (fewer false negatives)\")\n",
    "print(f\"  F1 Score:  {aggregate_f1:.2f}% (balanced metric)\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"  - Accuracy: {aggregate_accuracy:.1f}% of expected entities were found\")\n",
    "print(f\"  - Precision: Of all entities predicted, {aggregate_precision:.1f}% were correct\")\n",
    "print(f\"  - Recall: Of all actual entities, {aggregate_recall:.1f}% were found\")\n",
    "print(f\"  - F1: Harmonic mean balancing precision and recall\")\n",
    "\n",
    "print(f\"\\nüéØ What these metrics mean:\")\n",
    "print(f\"  - High Precision, Low Recall ‚Üí Model is conservative (misses entities)\")\n",
    "print(f\"  - Low Precision, High Recall ‚Üí Model is aggressive (predicts too many)\")\n",
    "print(f\"  - High F1 Score ‚Üí Good balance between precision and recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8545782",
   "metadata": {},
   "source": [
    "## 16b. Understanding the Metrics\n",
    "\n",
    "### Accuracy\n",
    "- **Formula**: `Correct / Total Expected`\n",
    "- **Meaning**: Percentage of expected entities that were correctly predicted\n",
    "- **Limitation**: Doesn't account for false positives (extra predictions)\n",
    "\n",
    "### Precision\n",
    "- **Formula**: `Correct / Total Predicted`\n",
    "- **Meaning**: Of all entities the model predicted, how many were correct?\n",
    "- **High Precision**: Model rarely makes false positive errors (rarely predicts wrong entities)\n",
    "\n",
    "### Recall\n",
    "- **Formula**: `Correct / Total Expected`\n",
    "- **Meaning**: Of all actual entities, how many did the model find?\n",
    "- **High Recall**: Model rarely makes false negative errors (rarely misses entities)\n",
    "\n",
    "### F1 Score\n",
    "- **Formula**: `2 √ó (Precision √ó Recall) / (Precision + Recall)`\n",
    "- **Meaning**: Harmonic mean that balances precision and recall\n",
    "- **Best metric**: When you care equally about false positives and false negatives\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Ground truth: ['aspirin', 'ibuprofen', 'NSAIDs']\n",
    "Prediction:   ['aspirin', 'ibuprofen']\n",
    "\n",
    "Accuracy:  66.7% (2/3 found)\n",
    "Precision: 100% (2/2 predicted were correct)\n",
    "Recall:    66.7% (2/3 actual entities found)\n",
    "F1 Score:  80.0% (balanced metric)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5800935",
   "metadata": {},
   "source": [
    "## 17. Custom Test Cases - Comprehensive NER Evaluation\n",
    "\n",
    "Test the model's ability to:\n",
    "1. **Extract Chemicals** - Identify drug names and chemical compounds\n",
    "2. **Extract Diseases** - Identify medical conditions and diseases\n",
    "3. **Extract Relationships** - Identify which chemicals are related to which diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Chemical Extraction\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: CHEMICAL EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chemical_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "A patient was treated with aspirin and ibuprofen for pain relief. The combination of these NSAIDs proved effective in reducing inflammation. Additionally, metformin was prescribed for glucose control.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{chemical_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(chemical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ef19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Disease Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: DISEASE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "disease_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the diseases mentioned.\n",
    "\n",
    "The patient presented with hypertension, diabetes mellitus, and chronic kidney disease. Laboratory findings revealed proteinuria and elevated creatinine levels, suggesting diabetic nephropathy.\n",
    "\n",
    "List of extracted diseases:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{disease_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(disease_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b12487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Chemical-Disease Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: RELATIONSHIP EXTRACTION - BASIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_1 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract the relationships between chemicals and diseases mentioned in the text.\n",
    "\n",
    "Metformin is commonly prescribed for type 2 diabetes by improving insulin sensitivity and reducing hepatic glucose production. Aspirin is used in cardiovascular disease management in high-risk patients.\n",
    "\n",
    "List the chemical-disease relationships:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_1}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_1, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Multiple Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: RELATIONSHIP EXTRACTION - MULTIPLE PAIRS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_2 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Identify all chemical-disease pairs and their relationships.\n",
    "\n",
    "Long-term use of corticosteroids is associated with osteoporosis and increases the risk of bone fractures. NSAIDs are linked to chronic kidney disease and gastrointestinal bleeding in susceptible patients.\n",
    "\n",
    "List of chemical-disease relationships:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_2}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_2, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Complex Multi-Entity Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 5: COMPREHENSIVE EXTRACTION - ALL ENTITIES & RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_3 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract:\n",
    "1. All chemicals mentioned\n",
    "2. All diseases mentioned\n",
    "3. All relationships between chemicals and diseases\n",
    "\n",
    "The patient with rheumatoid arthritis was started on methotrexate for inflammatory joint disease. However, methotrexate is associated with hepatotoxicity and requires monitoring. The patient also has hypertension managed with lisinopril. Statins were prescribed for cardiovascular disease prevention given elevated cholesterol levels.\n",
    "\n",
    "Extracted information:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_3}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_3, max_new_tokens=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0cd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Chemical Extraction\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: CHEMICAL EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chemical_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "A patient was treated with aspirin and ibuprofen for pain relief. The combination of these NSAIDs proved effective in reducing inflammation. Additionally, metformin was prescribed for glucose control.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{chemical_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(chemical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d60926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Disease Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: DISEASE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "disease_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the diseases mentioned.\n",
    "\n",
    "The patient presented with hypertension, diabetes mellitus, and chronic kidney disease. Laboratory findings revealed proteinuria and elevated creatinine levels, suggesting diabetic nephropathy.\n",
    "\n",
    "List of extracted diseases:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{disease_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(disease_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Relationship Extraction with TYPE explanation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: RELATIONSHIP EXTRACTION - TREATMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_1 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. For each disease-chemical pair, identify the relationship and explain the TYPE of relationship (e.g., treats, prevents, causes, worsens, etc.).\n",
    "\n",
    "Metformin is commonly prescribed to treat type 2 diabetes by improving insulin sensitivity and reducing hepatic glucose production. Aspirin is used to prevent cardiovascular disease in high-risk patients.\n",
    "\n",
    "List the relationships between chemicals and diseases with their types:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_1}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_1, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Relationship Extraction - Adverse Effects\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: RELATIONSHIP EXTRACTION - ADVERSE EFFECTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_2 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. For each disease-chemical pair, identify the relationship and explain the TYPE of relationship (e.g., treats, prevents, causes, worsens, induces, etc.).\n",
    "\n",
    "Long-term use of corticosteroids can cause osteoporosis and increase the risk of bone fractures. NSAIDs may worsen chronic kidney disease and induce gastrointestinal bleeding in susceptible patients.\n",
    "\n",
    "List the relationships between chemicals and diseases with their types:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_2}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_2, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Complex Multi-Relationship Scenario\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 5: COMPLEX MULTI-RELATIONSHIP SCENARIO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_3 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract all chemicals, all diseases, and explain the TYPE of relationship between each chemical-disease pair (treats, prevents, causes, worsens, contraindicates, etc.).\n",
    "\n",
    "The patient with rheumatoid arthritis was started on methotrexate, which effectively treats inflammatory joint disease. However, methotrexate can cause hepatotoxicity and must be monitored carefully. The patient also has hypertension controlled with lisinopril. Statins were prescribed to prevent cardiovascular disease given the patient's elevated cholesterol levels.\n",
    "\n",
    "Provide:\n",
    "1. List of chemicals\n",
    "2. List of diseases  \n",
    "3. Relationships with their types\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_3}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_3, max_new_tokens=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51eef3b",
   "metadata": {},
   "source": [
    "## 18. Model Information and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING COMPLETE! üéâ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nYour fine-tuned model is available at:\")\n",
    "print(f\"  üìÅ Local: ./final_model\")\n",
    "print(f\"  ü§ó Hub: https://huggingface.co/{HF_MODEL_ID}\")\n",
    "print(f\"\\nModel details:\")\n",
    "print(f\"  Base model: {MODEL_NAME}\")\n",
    "print(f\"  Training samples: {len(train_data)}\")\n",
    "print(f\"  Validation samples: {len(val_data)}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Test on more validation examples\")\n",
    "print(f\"  2. Try the model on completely new medical texts\")\n",
    "print(f\"  3. Compare with base model (ablation study)\")\n",
    "print(f\"  4. Deploy via Hugging Face Inference API\")\n",
    "print(f\"  5. Share your model with the community!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebbaa6",
   "metadata": {},
   "source": [
    "## 19. Usage Example\n",
    "\n",
    "How to use the model in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and use the model later\n",
    "usage_code = '''\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter from Hub\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"your-username/llama3-medical-ner-lora\"  # Your model ID\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Use the model\n",
    "prompt = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "Patient was treated with metformin and insulin for diabetes management.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "# ... (use the generate_response function from above)\n",
    "'''\n",
    "\n",
    "print(\"Usage Example:\")\n",
    "print(\"=\"*80)\n",
    "print(usage_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d1ad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "1. ‚úÖ Loaded and analyzed 3,000 medical NER examples\n",
    "2. ‚úÖ Split data into train/validation/test sets (80/10/10)\n",
    "3. ‚úÖ Formatted data in Llama 3 chat format\n",
    "4. ‚úÖ Configured Weights & Biases for tracking\n",
    "5. ‚úÖ Loaded Llama 3.2 3B with 4-bit quantization\n",
    "6. ‚úÖ Applied LoRA for efficient fine-tuning\n",
    "7. ‚úÖ Trained the model with SFT (monitored via W&B)\n",
    "8. ‚úÖ Uploaded checkpoints to Hugging Face Hub\n",
    "9. ‚úÖ Evaluated on completely unseen test set\n",
    "10. ‚úÖ Saved the final model locally and on Hub\n",
    "\n",
    "**Your medical NER model is ready to use! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
