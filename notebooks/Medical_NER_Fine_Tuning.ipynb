{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb06cd0",
   "metadata": {},
   "source": [
    "# Medical NER Fine-Tuning with Llama 3.2 3B + LoRA\n",
    "\n",
    "This notebook implements fine-tuning of Llama 3.2 3B Instruct for medical Named Entity Recognition (NER) using:\n",
    "- **SFT** (Supervised Fine-Tuning)\n",
    "- **LoRA** (Low-Rank Adaptation)\n",
    "- **Hugging Face Hub** integration for checkpoint uploads\n",
    "\n",
    "## Tasks:\n",
    "1. Chemical entity extraction\n",
    "2. Disease entity extraction\n",
    "3. Chemical-Disease relationship extraction\n",
    "\n",
    "## Dataset:\n",
    "- 3,000 medical text examples\n",
    "- 80/10/10 train/validation/test split\n",
    "- **‚ö†Ô∏è CRITICAL**: Data is shuffled before splitting to ensure balanced task distribution\n",
    "- Weights & Biases tracking enabled\n",
    "\n",
    "## Important Note:\n",
    "**Data splitting MUST use `shuffle=True`** to prevent task imbalance. Without shuffling, all relationship extraction examples may cluster in validation/test sets, leading to poor model performance on the most important task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f6f14",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running this notebook!\n",
    "\n",
    "Required:\n",
    "- `HF_TOKEN`: Your Hugging Face token (needed to save models to HF Hub)\n",
    "\n",
    "Optional:\n",
    "- `WANDB_API_KEY`: Your Weights & Biases API key (for training tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your Hugging Face token (required for uploading to HF Hub)\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Set your Weights & Biases API key (optional, for training tracking)\n",
    "os.environ[\"WANDB_API_KEY\"] = \"d88df098d85360ac924ec2bf8dcf5520d745c411\"\n",
    "\n",
    "# Verify environment variables\n",
    "print(\"‚úì Environment variables set\")\n",
    "print(f\"  HF_TOKEN: {'‚úì Set' if os.environ.get('HF_TOKEN') and os.environ['HF_TOKEN'] != 'hf_YOUR_TOKEN_HERE' else '‚úó Not set - UPDATE THIS!'}\")\n",
    "print(f\"  WANDB_API_KEY: {'‚úì Set' if os.environ.get('WANDB_API_KEY') else '‚óã Optional (will use wandb login cache)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82d981",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers trl scikit-learn\n",
    "!pip install -q scipy sentencepiece protobuf wandb\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84c1fe",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ad90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51827af6",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Update `HF_USERNAME` with your Hugging Face username!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Section\n",
    "from datetime import datetime\n",
    "\n",
    "HF_USERNAME = \"albyos\"  # Replace with your HF username\n",
    "\n",
    "# Generate timestamp for checkpoint naming\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "HF_MODEL_ID = f\"{HF_USERNAME}/llama3-medical-ner-lora-{TIMESTAMP}\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MODEL_NAME = BASE_MODEL  # Alias for consistency\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training Configuration\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Data Configuration\n",
    "TRAIN_SPLIT_RATIO = 0.9\n",
    "RANDOM_SEED = 42\n",
    "RESHUFFLE_SPLITS_EACH_RUN = True  # When True, create a fresh validation split every run\n",
    "SPLIT_SEED = random.randint(0, 1_000_000) if RESHUFFLE_SPLITS_EACH_RUN else RANDOM_SEED\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  Training timestamp: {TIMESTAMP}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Data split seed: {SPLIT_SEED} ({'reshuffled' if RESHUFFLE_SPLITS_EACH_RUN else 'fixed'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc274801",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Authentication\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token and hf_token != \"hf_YOUR_TOKEN_HERE\":\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úì Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"‚ö† HF_TOKEN not set. Please update Cell 3 before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb723a0b",
   "metadata": {},
   "source": [
    "## 4b. Weights & Biases Setup\n",
    "\n",
    "Initialize W&B to track training metrics, validation loss, and experiments.\n",
    "Get your API key from: https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740959ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Weights & Biases\n",
    "wandb_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "if wandb_key and wandb_key != 'your_wandb_key_here':\n",
    "    wandb.login(key=wandb_key)\n",
    "    print('‚úì Logged in to Weights & Biases using WANDB_API_KEY')\n",
    "else:\n",
    "    print('‚ö† Warning: WANDB_API_KEY not set. Attempting to use cached login...')\n",
    "    try:\n",
    "        wandb.login()\n",
    "        print('‚úì Logged in to Weights & Biases using cached credentials')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö† Warning: Could not login to W&B: {e}')\n",
    "        print('  Run wandb.login() interactively or set WANDB_API_KEY environment variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51134743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"medical-ner-finetuning\",\n",
    "    name=f\"llama3-medical-ner-{TIMESTAMP}\",\n",
    "    config={\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"lora_rank\": LORA_RANK,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Weights & Biases initialized\")\n",
    "print(f\"  Project: medical-ner-finetuning\")\n",
    "print(f\"  Run name: llama3-medical-ner-{TIMESTAMP}\")\n",
    "print(f\"  Dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7c1b6",
   "metadata": {},
   "source": [
    "## 5. Data Exploration\n",
    "\n",
    "Let's examine the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the dataset\n",
    "# Load data\n",
    "with open('both_rel_instruct_all.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"\\nSample structure:\")\n",
    "print(json.dumps(data[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze task distribution\n",
    "task_counts = {}\n",
    "for sample in data:\n",
    "    if \"chemicals mentioned\" in sample['prompt']:\n",
    "        task = \"Chemical Extraction\"\n",
    "    elif \"diseases mentioned\" in sample['prompt']:\n",
    "        task = \"Disease Extraction\"\n",
    "    elif \"influences between\" in sample['prompt']:\n",
    "        task = \"Relationship Extraction\"\n",
    "    else:\n",
    "        task = \"Other\"\n",
    "    \n",
    "    task_counts[task] = task_counts.get(task, 0) + 1\n",
    "\n",
    "print(\"Task Distribution:\")\n",
    "for task, count in task_counts.items():\n",
    "    print(f\"  {task}: {count} ({count/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example from each task type\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE: Chemical Extraction\")\n",
    "print(\"=\"*80)\n",
    "chem_example = [s for s in data if \"chemicals mentioned\" in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{chem_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{chem_example['completion']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: Disease Extraction\")\n",
    "print(\"=\"*80)\n",
    "disease_example = [s for s in data if \"diseases mentioned\" in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{disease_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{disease_example['completion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e86457",
   "metadata": {},
   "source": [
    "## 6. Dataset Splitting\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL**: Using **stratified splitting** for guaranteed balanced task distribution!\n",
    "\n",
    "**Previous Issue**: Without shuffling, all relationship extraction examples ended up in validation/test sets, causing poor model performance.\n",
    "\n",
    "**New Solution**: Stratified splitting ensures EXACT proportions in all splits (not just probabilistic).\n",
    "\n",
    "Split into:\n",
    "- **80% Training** (2,400 samples) - for fine-tuning\n",
    "- **10% Validation** (300 samples) - for monitoring during training (W&B)\n",
    "- **10% Test** (300 samples) - for final evaluation after training\n",
    "\n",
    "**Guaranteed distribution in each split** (with stratification):\n",
    "- **Exactly 33.3%** Chemical extraction\n",
    "- **Exactly 33.3%** Disease extraction  \n",
    "- **Exactly 33.3%** Relationship extraction\n",
    "\n",
    "**Why stratified?**\n",
    "- `shuffle=True` gives ~33% ¬± 2-3% (probabilistic, good enough)\n",
    "- `stratify=labels` gives **exactly 33.3%** (guaranteed, better!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/val/test (80/10/10) with STRATIFIED sampling\n",
    "random.seed(SPLIT_SEED)\n",
    "\n",
    "# Helper function to classify task type for stratification\n",
    "def get_task_type(prompt):\n",
    "    \"\"\"Classify the task type based on prompt for stratification.\"\"\"\n",
    "    prompt_lower = prompt.lower()\n",
    "    if \"influences between\" in prompt_lower:\n",
    "        return \"relationship\"\n",
    "    elif \"chemicals mentioned\" in prompt_lower:\n",
    "        return \"chemical\"\n",
    "    elif \"diseases mentioned\" in prompt_lower:\n",
    "        return \"disease\"\n",
    "    return \"other\"\n",
    "\n",
    "# Create stratification labels for all data\n",
    "stratify_labels = [get_task_type(sample['prompt']) for sample in data]\n",
    "\n",
    "print(f\"Creating stratified splits to guarantee balanced task distribution...\")\n",
    "print(f\"Original task distribution: {set(stratify_labels)}\")\n",
    "\n",
    "# First split: 80% train, 20% temp (for val + test)\n",
    "# Using stratify= ensures EXACT proportions in both splits\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "    data,\n",
    "    stratify_labels,\n",
    "    test_size=0.2,  # 20% for validation + test\n",
    "    random_state=SPLIT_SEED,\n",
    "    stratify=stratify_labels  # ‚úÖ GUARANTEES exact 33.3% in both train and temp!\n",
    ")\n",
    "\n",
    "# Second split: split the 20% into 10% val, 10% test\n",
    "# Stratify again to ensure exact proportions in val and test\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    temp_data,\n",
    "    temp_labels,\n",
    "    test_size=0.5,  # 50% of 20% = 10% of total\n",
    "    random_state=SPLIT_SEED + 1,\n",
    "    stratify=temp_labels  # ‚úÖ GUARANTEES exact 33.3% in both val and test!\n",
    ")\n",
    "\n",
    "# Save splits\n",
    "with open('train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('validation.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('test.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"‚úì Dataset split complete (seed={SPLIT_SEED}, stratified=True)\")\n",
    "print(f\"  Train samples: {len(train_data)} ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"  Validation samples: {len(val_data)} ({len(val_data)/len(data)*100:.1f}%) - for training monitoring\")\n",
    "print(f\"  Test samples: {len(test_data)} ({len(test_data)/len(data)*100:.1f}%) - for final evaluation\")\n",
    "print(f\"\\nüìä Usage:\")\n",
    "print(f\"  - Train: Used for fine-tuning\")\n",
    "print(f\"  - Validation: Monitored during training (shown in W&B)\")\n",
    "print(f\"  - Test: Used ONLY after training for final evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify task distribution across splits\n",
    "def get_task_type_display(prompt):\n",
    "    \"\"\"Classify the task type based on prompt for display.\"\"\"\n",
    "    prompt_lower = prompt.lower()\n",
    "    if \"influences between\" in prompt_lower:\n",
    "        return \"Relationship Extraction\"\n",
    "    elif \"chemicals mentioned\" in prompt_lower:\n",
    "        return \"Chemical Extraction\"\n",
    "    elif \"diseases mentioned\" in prompt_lower:\n",
    "        return \"Disease Extraction\"\n",
    "    return \"Other\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK DISTRIBUTION VERIFICATION (STRATIFIED SPLITTING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split_name, split_data in [(\"Train\", train_data), (\"Validation\", val_data), (\"Test\", test_data)]:\n",
    "    task_counts = {}\n",
    "    for sample in split_data:\n",
    "        task = get_task_type_display(sample['prompt'])\n",
    "        task_counts[task] = task_counts.get(task, 0) + 1\n",
    "    \n",
    "    print(f\"\\n{split_name} ({len(split_data)} samples):\")\n",
    "    for task, count in sorted(task_counts.items()):\n",
    "        percentage = count / len(split_data) * 100\n",
    "        # Check if exactly balanced (within 0.5% tolerance)\n",
    "        is_perfect = abs(percentage - 33.33) < 0.5\n",
    "        marker = \"‚úÖ\" if is_perfect else \"‚ö†Ô∏è\"\n",
    "        print(f\"  {marker} {task}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Verify no data leakage between splits\n",
    "train_prompts = set(s['prompt'] for s in train_data)\n",
    "val_prompts = set(s['prompt'] for s in val_data)\n",
    "test_prompts = set(s['prompt'] for s in test_data)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATA INTEGRITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "overlap_train_val = len(train_prompts & val_prompts)\n",
    "overlap_train_test = len(train_prompts & test_prompts)\n",
    "overlap_val_test = len(val_prompts & test_prompts)\n",
    "\n",
    "print(f\"Train-Validation overlap: {overlap_train_val} samples {'‚úÖ Perfect!' if overlap_train_val == 0 else '‚ö†Ô∏è  WARNING - Data leakage detected!'}\")\n",
    "print(f\"Train-Test overlap: {overlap_train_test} samples {'‚úÖ Perfect!' if overlap_train_test == 0 else '‚ö†Ô∏è  WARNING - Data leakage detected!'}\")\n",
    "print(f\"Validation-Test overlap: {overlap_val_test} samples {'‚úÖ Perfect!' if overlap_val_test == 0 else '‚ö†Ô∏è  WARNING - Data leakage detected!'}\")\n",
    "\n",
    "if overlap_train_val == 0 and overlap_train_test == 0 and overlap_val_test == 0:\n",
    "    print(\"\\n‚úÖ All splits are properly separated - no data leakage detected!\")\n",
    "    print(\"‚úÖ Stratified splitting guarantees exact task proportions in all splits!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Data leakage detected! Splits contain overlapping samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e0e62",
   "metadata": {},
   "source": [
    "## 7. Data Formatting\n",
    "\n",
    "Format data into Llama 3 chat format with system, user, and assistant roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format data into Llama 3 chat format.\"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{sample['prompt']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{sample['completion']}<|eot_id|>\"\"\"\n",
    "\n",
    "# Test formatting\n",
    "formatted_example = format_instruction(train_data[0])\n",
    "print(\"Formatted Example:\")\n",
    "print(formatted_example[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format all data\n",
    "train_formatted = [{\"text\": format_instruction(sample)} for sample in train_data]\n",
    "val_formatted = [{\"text\": format_instruction(sample)} for sample in val_data]\n",
    "test_formatted = [{\"text\": format_instruction(sample)} for sample in test_data]\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_formatted)\n",
    "val_dataset = Dataset.from_list(val_formatted)\n",
    "test_dataset = Dataset.from_list(test_formatted)\n",
    "\n",
    "print(f\"‚úì Datasets formatted:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7616fb",
   "metadata": {},
   "source": [
    "## 8. Load Model and Tokenizer\n",
    "\n",
    "Load Llama 3.2 3B with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3981a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Quantization config created (4-bit NF4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c092e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd890d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading model... (this may take a few minutes)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ceb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"‚úì Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b2d6c",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA\n",
    "\n",
    "Apply Low-Rank Adaptation for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b69361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,                   # LoRA rank\n",
    "    lora_alpha=LORA_ALPHA,         # LoRA alpha (scaling)\n",
    "    target_modules=[               # Layers to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,             # Dropout for regularization\n",
    "    bias=\"none\",                   # No bias training\n",
    "    task_type=\"CAUSAL_LM\"          # Causal language modeling\n",
    ")\n",
    "\n",
    "print(f\"‚úì LoRA configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {len(lora_config.target_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úì LoRA applied to model\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665501b",
   "metadata": {},
   "source": [
    "## 10. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6201d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the texts.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set tokenized: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úì Validation set tokenized: {len(tokenized_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚úì Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417fc9",
   "metadata": {},
   "source": [
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./llama3-medical-ner-lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluate every 50 steps\n",
    "    \n",
    "    # Checkpointing - Save every 50 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Checkpoint every 50 steps\n",
    "    save_total_limit=None,  # Keep all checkpoints (will push to HF with unique names)\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=True,\n",
    "    \n",
    "    # Hugging Face Hub - Disable default push (we'll use custom callback)\n",
    "    push_to_hub=False,  # Custom callback will handle timestamped uploads\n",
    "    \n",
    "    # Misc\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",  # Enable Weights & Biases logging\n",
    "    run_name=f\"llama3-medical-ner-{TIMESTAMP}\",  # W&B run name\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Checkpoint frequency: Every {training_args.save_steps} steps\")\n",
    "print(f\"  Base HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  ‚ö†Ô∏è Checkpoints will be pushed to HF with timestamp suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3b101",
   "metadata": {},
   "source": [
    "## 11b. Custom Checkpoint Upload Callback\n",
    "\n",
    "This callback will automatically push each checkpoint to Hugging Face Hub with a unique timestamped name every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a78c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from huggingface_hub import HfApi\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "class CheckpointUploadCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to upload checkpoints to Hugging Face Hub with timestamped names.\n",
    "    \n",
    "    Each checkpoint will be saved with format:\n",
    "    {HF_USERNAME}/llama3-medical-ner-lora-checkpoint-{step}-{timestamp}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_id, hf_username):\n",
    "        self.base_model_id = base_model_id\n",
    "        self.hf_username = hf_username\n",
    "        self.api = HfApi()\n",
    "        \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Called when a checkpoint is saved.\n",
    "        Uploads the checkpoint to HF Hub with a timestamped name.\n",
    "        \"\"\"\n",
    "        # Get the checkpoint directory that was just saved\n",
    "        checkpoint_dir = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n",
    "        \n",
    "        if not Path(checkpoint_dir).exists():\n",
    "            print(f\"‚ö†Ô∏è Checkpoint directory not found: {checkpoint_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Create timestamped model ID\n",
    "        checkpoint_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_model_id = f\"{self.hf_username}/llama3-medical-ner-checkpoint-{state.global_step}-{checkpoint_timestamp}\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üì§ Uploading checkpoint to Hugging Face Hub\")\n",
    "        print(f\"   Step: {state.global_step}\")\n",
    "        print(f\"   Model ID: {checkpoint_model_id}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Upload the checkpoint folder to HF Hub\n",
    "            self.api.upload_folder(\n",
    "                folder_path=checkpoint_dir,\n",
    "                repo_id=checkpoint_model_id,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=f\"Checkpoint at step {state.global_step}\",\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Checkpoint uploaded successfully!\")\n",
    "            print(f\"   URL: https://huggingface.co/{checkpoint_model_id}\\n\")\n",
    "            \n",
    "            # Log to wandb if available\n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\n",
    "                    \"checkpoint_step\": state.global_step,\n",
    "                    \"checkpoint_url\": f\"https://huggingface.co/{checkpoint_model_id}\"\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to upload checkpoint: {e}\")\n",
    "            print(f\"   Checkpoint saved locally at: {checkpoint_dir}\\n\")\n",
    "\n",
    "# Initialize the callback\n",
    "checkpoint_upload_callback = CheckpointUploadCallback(\n",
    "    base_model_id=HF_MODEL_ID,\n",
    "    hf_username=HF_USERNAME\n",
    ")\n",
    "\n",
    "print(f\"‚úì Checkpoint upload callback initialized\")\n",
    "print(f\"  Checkpoints will be uploaded to: {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"  Upload frequency: Every {training_args.save_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview expected checkpoint uploads\n",
    "total_steps_estimate = (len(train_data) // (BATCH_SIZE * GRADIENT_ACCUMULATION)) * NUM_EPOCHS\n",
    "checkpoint_count = total_steps_estimate // 50\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHECKPOINT UPLOAD PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Total samples: {len(train_data)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Estimated total steps: ~{total_steps_estimate}\")\n",
    "print(f\"\\nCheckpoint Configuration:\")\n",
    "print(f\"  Frequency: Every 50 steps\")\n",
    "print(f\"  Expected checkpoints: ~{checkpoint_count}\")\n",
    "print(f\"  Local storage: ./llama3-medical-ner-lora/checkpoint-<step>/\")\n",
    "print(f\"\\nHugging Face Upload:\")\n",
    "print(f\"  Format: {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"\\nExample checkpoint names:\")\n",
    "for i, step in enumerate(range(50, min(250, total_steps_estimate), 50), 1):\n",
    "    example_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"  {i}. {HF_USERNAME}/llama3-medical-ner-checkpoint-{step}-{example_time}\")\n",
    "if checkpoint_count > 4:\n",
    "    print(f\"  ... (~{checkpoint_count - 4} more checkpoints)\")\n",
    "    \n",
    "print(f\"\\nFinal model:\")\n",
    "print(f\"  {HF_USERNAME}/llama3-medical-ner-lora-final-<timestamp>\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a993c",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b88564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[checkpoint_upload_callback],  # Add custom checkpoint upload callback\n",
    ")\n",
    "\n",
    "# Configure early stopping to prevent overfitting\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0))\n",
    "\n",
    "# Calculate training steps\n",
    "total_steps = (len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n",
    "\n",
    "print(f\"‚úì Trainer initialized\")\n",
    "print(f\"‚úì Expected training steps: ~{total_steps}\")\n",
    "print(f\"‚úì Expected checkpoints: ~{max(1, total_steps // training_args.save_steps)}\")\n",
    "print(f\"‚úì Checkpoint upload callback enabled\")\n",
    "print(\"‚úì Early stopping enabled (patience = 3 evaluations)\")\n",
    "print(f\"\\nüìã Checkpoint naming format:\")\n",
    "print(f\"   {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"\\n   Example: {HF_USERNAME}/llama3-medical-ner-checkpoint-50-20251104_143022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b853b4",
   "metadata": {},
   "source": [
    "## 13. Start Training\n",
    "\n",
    "‚ö†Ô∏è **This will take 2-3 hours on an A100 GPU**\n",
    "\n",
    "The training will:\n",
    "- **Save checkpoints every 50 steps** to local disk\n",
    "- **Upload each checkpoint to Hugging Face Hub** with timestamped names\n",
    "  - Format: `{username}/llama3-medical-ner-checkpoint-{step}-{timestamp}`\n",
    "  - Example: `albyos/llama3-medical-ner-checkpoint-50-20251104_143022`\n",
    "- Evaluate on validation set every 50 steps\n",
    "- Save the best model based on validation loss\n",
    "- Log all metrics to Weights & Biases\n",
    "\n",
    "**Checkpoint URLs will be printed during training and logged to W&B.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39341273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take 2-3 hours on A100 GPU...\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b3b39",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93373e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "\n",
    "print(f\"‚úì Model saved to: ./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push final model to Hugging Face Hub with timestamped name\n",
    "print(\"Pushing final model to Hugging Face Hub...\")\n",
    "\n",
    "# Create final model ID with timestamp\n",
    "final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_model_id = f\"{HF_USERNAME}/llama3-medical-ner-lora-final-{final_timestamp}\"\n",
    "\n",
    "try:\n",
    "    # Push the final model\n",
    "    model.push_to_hub(\n",
    "        final_model_id,\n",
    "        commit_message=\"Training complete - final model\"\n",
    "    )\n",
    "    tokenizer.push_to_hub(\n",
    "        final_model_id,\n",
    "        commit_message=\"Training complete - final tokenizer\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Final model pushed successfully!\")\n",
    "    print(f\"   Model ID: {final_model_id}\")\n",
    "    print(f\"   URL: https://huggingface.co/{final_model_id}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"final_model_url\": f\"https://huggingface.co/{final_model_id}\",\n",
    "            \"final_model_id\": final_model_id\n",
    "        })\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Failed to push to hub: {e}\")\n",
    "    print(\"  Final model saved locally at: ./final_model\")\n",
    "    print(f\"  You can manually push later using:\")\n",
    "    print(f\"    model.push_to_hub('{final_model_id}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed8e1e",
   "metadata": {},
   "source": [
    "## 15. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract losses\n",
    "train_loss = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "eval_loss = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_loss, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Evaluation Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training metrics plotted and saved to: training_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training steps: {len(train_loss)}\")\n",
    "print(f\"Final training loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {eval_loss[-1]:.4f}\")\n",
    "print(f\"Best validation loss: {min(eval_loss):.4f}\")\n",
    "print(f\"Loss reduction: {((train_loss[0] - train_loss[-1]) / train_loss[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdcba1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Training is complete! Your model has been saved.\n",
    "\n",
    "**To evaluate your model:**\n",
    "1. Open `Medical_NER_Evaluation.ipynb`\n",
    "2. Run the evaluation on the test set\n",
    "3. Test custom examples\n",
    "\n",
    "**Model locations:**\n",
    "- Local: `./final_model`\n",
    "- HuggingFace Hub: Check the output above for your model URL\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
