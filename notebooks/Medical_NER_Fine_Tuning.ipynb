{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb06cd0",
   "metadata": {},
   "source": [
    "# Medical NER Fine-Tuning with Llama 3.2 3B + LoRA\n",
    "\n",
    "This notebook implements fine-tuning of Llama 3.2 3B Instruct for medical Named Entity Recognition (NER) using:\n",
    "- **SFT** (Supervised Fine-Tuning)\n",
    "- **LoRA** (Low-Rank Adaptation)\n",
    "- **Hugging Face Hub** integration for checkpoint uploads\n",
    "\n",
    "## Tasks:\n",
    "1. Chemical entity extraction\n",
    "2. Disease entity extraction\n",
    "3. Chemical-Disease relationship extraction\n",
    "\n",
    "## Dataset:\n",
    "- 3,000 medical text examples\n",
    "- 80/10/10 train/validation/test split\n",
    "- Weights & Biases tracking enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f6f14",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running this notebook!\n",
    "\n",
    "Required:\n",
    "- `HF_TOKEN`: Your Hugging Face token (needed to save models to HF Hub)\n",
    "\n",
    "Optional:\n",
    "- `WANDB_API_KEY`: Your Weights & Biases API key (for training tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your Hugging Face token (required for uploading to HF Hub)\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_YOUR_TOKEN_HERE\"\n",
    "\n",
    "# Set your Weights & Biases API key (optional, for training tracking)\n",
    "os.environ[\"WANDB_API_KEY\"] = \"your_wandb_key_here\"\n",
    "\n",
    "# Verify environment variables\n",
    "print(\"‚úì Environment variables set\")\n",
    "print(f\"  HF_TOKEN: {'‚úì Set' if os.environ.get('HF_TOKEN') and os.environ['HF_TOKEN'] != 'hf_YOUR_TOKEN_HERE' else '‚úó Not set - UPDATE THIS!'}\")\n",
    "print(f\"  WANDB_API_KEY: {'‚úì Set' if os.environ.get('WANDB_API_KEY') else '‚óã Optional (will use wandb login cache)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82d981",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers trl scikit-learn\n",
    "!pip install -q scipy sentencepiece protobuf wandb\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84c1fe",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ad90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51827af6",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Update `HF_USERNAME` with your Hugging Face username!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Section\n",
    "from datetime import datetime\n",
    "\n",
    "HF_USERNAME = \"albyos\"  # Replace with your HF username\n",
    "\n",
    "# Generate timestamp for checkpoint naming\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "HF_MODEL_ID = f\"{HF_USERNAME}/llama3-medical-ner-lora-{TIMESTAMP}\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MODEL_NAME = BASE_MODEL  # Alias for consistency\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training Configuration\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Data Configuration\n",
    "TRAIN_SPLIT_RATIO = 0.9\n",
    "RANDOM_SEED = 42\n",
    "RESHUFFLE_SPLITS_EACH_RUN = True  # When True, create a fresh validation split every run\n",
    "SPLIT_SEED = random.randint(0, 1_000_000) if RESHUFFLE_SPLITS_EACH_RUN else RANDOM_SEED\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  Training timestamp: {TIMESTAMP}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Data split seed: {SPLIT_SEED} ({'reshuffled' if RESHUFFLE_SPLITS_EACH_RUN else 'fixed'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc274801",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Authentication\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token and hf_token != \"hf_YOUR_TOKEN_HERE\":\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úì Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"‚ö† HF_TOKEN not set. Please update Cell 3 before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb723a0b",
   "metadata": {},
   "source": [
    "## 4b. Weights & Biases Setup\n",
    "\n",
    "Initialize W&B to track training metrics, validation loss, and experiments.\n",
    "Get your API key from: https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740959ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Weights & Biases\n",
    "wandb_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "if wandb_key and wandb_key != 'your_wandb_key_here':\n",
    "    wandb.login(key=wandb_key)\n",
    "    print('‚úì Logged in to Weights & Biases using WANDB_API_KEY')\n",
    "else:\n",
    "    print('‚ö† Warning: WANDB_API_KEY not set. Attempting to use cached login...')\n",
    "    try:\n",
    "        wandb.login()\n",
    "        print('‚úì Logged in to Weights & Biases using cached credentials')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö† Warning: Could not login to W&B: {e}')\n",
    "        print('  Run wandb.login() interactively or set WANDB_API_KEY environment variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51134743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"medical-ner-finetuning\",\n",
    "    name=f\"llama3-medical-ner-{TIMESTAMP}\",\n",
    "    config={\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"lora_rank\": LORA_RANK,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Weights & Biases initialized\")\n",
    "print(f\"  Project: medical-ner-finetuning\")\n",
    "print(f\"  Run name: llama3-medical-ner-{TIMESTAMP}\")\n",
    "print(f\"  Dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7c1b6",
   "metadata": {},
   "source": [
    "## 5. Data Exploration\n",
    "\n",
    "Let's examine the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the dataset\n",
    "# Load data\n",
    "with open('both_rel_instruct_all.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"\\nSample structure:\")\n",
    "print(json.dumps(data[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze task distribution\n",
    "task_counts = {}\n",
    "for sample in data:\n",
    "    if \"chemicals mentioned\" in sample['prompt']:\n",
    "        task = \"Chemical Extraction\"\n",
    "    elif \"diseases mentioned\" in sample['prompt']:\n",
    "        task = \"Disease Extraction\"\n",
    "    elif \"influences between\" in sample['prompt']:\n",
    "        task = \"Relationship Extraction\"\n",
    "    else:\n",
    "        task = \"Other\"\n",
    "    \n",
    "    task_counts[task] = task_counts.get(task, 0) + 1\n",
    "\n",
    "print(\"Task Distribution:\")\n",
    "for task, count in task_counts.items():\n",
    "    print(f\"  {task}: {count} ({count/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example from each task type\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE: Chemical Extraction\")\n",
    "print(\"=\"*80)\n",
    "chem_example = [s for s in data if \"chemicals mentioned\" in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{chem_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{chem_example['completion']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: Disease Extraction\")\n",
    "print(\"=\"*80)\n",
    "disease_example = [s for s in data if \"diseases mentioned\" in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{disease_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{disease_example['completion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e86457",
   "metadata": {},
   "source": [
    "## 6. Dataset Splitting\n",
    "\n",
    "Split into:\n",
    "- **80% Training** (2,400 samples) - for fine-tuning\n",
    "- **10% Validation** (300 samples) - for monitoring during training (W&B)\n",
    "- **10% Test** (300 samples) - for final evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/val/test (80/10/10)\n",
    "random.seed(SPLIT_SEED)\n",
    "\n",
    "# First split: 80% train, 20% temp (for val + test)\n",
    "train_data, temp_data = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,  # 20% for validation + test\n",
    "    random_state=SPLIT_SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: split the 20% into 10% val, 10% test\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    test_size=0.5,  # 50% of 20% = 10% of total\n",
    "    random_state=SPLIT_SEED + 1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Save splits\n",
    "with open('train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('validation.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('test.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"‚úì Dataset split complete (seed={SPLIT_SEED})\")\n",
    "print(f\"  Train samples: {len(train_data)} ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"  Validation samples: {len(val_data)} ({len(val_data)/len(data)*100:.1f}%) - for training monitoring\")\n",
    "print(f\"  Test samples: {len(test_data)} ({len(test_data)/len(data)*100:.1f}%) - for final evaluation\")\n",
    "print(f\"\\nüìä Usage:\")\n",
    "print(f\"  - Train: Used for fine-tuning\")\n",
    "print(f\"  - Validation: Monitored during training (shown in W&B)\")\n",
    "print(f\"  - Test: Used ONLY after training for final evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e0e62",
   "metadata": {},
   "source": [
    "## 7. Data Formatting\n",
    "\n",
    "Format data into Llama 3 chat format with system, user, and assistant roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format data into Llama 3 chat format.\"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{sample['prompt']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{sample['completion']}<|eot_id|>\"\"\"\n",
    "\n",
    "# Test formatting\n",
    "formatted_example = format_instruction(train_data[0])\n",
    "print(\"Formatted Example:\")\n",
    "print(formatted_example[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format all data\n",
    "train_formatted = [{\"text\": format_instruction(sample)} for sample in train_data]\n",
    "val_formatted = [{\"text\": format_instruction(sample)} for sample in val_data]\n",
    "test_formatted = [{\"text\": format_instruction(sample)} for sample in test_data]\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_formatted)\n",
    "val_dataset = Dataset.from_list(val_formatted)\n",
    "test_dataset = Dataset.from_list(test_formatted)\n",
    "\n",
    "print(f\"‚úì Datasets formatted:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7616fb",
   "metadata": {},
   "source": [
    "## 8. Load Model and Tokenizer\n",
    "\n",
    "Load Llama 3.2 3B with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3981a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Quantization config created (4-bit NF4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c092e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd890d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading model... (this may take a few minutes)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ceb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"‚úì Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b2d6c",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA\n",
    "\n",
    "Apply Low-Rank Adaptation for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b69361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,                   # LoRA rank\n",
    "    lora_alpha=LORA_ALPHA,         # LoRA alpha (scaling)\n",
    "    target_modules=[               # Layers to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,             # Dropout for regularization\n",
    "    bias=\"none\",                   # No bias training\n",
    "    task_type=\"CAUSAL_LM\"          # Causal language modeling\n",
    ")\n",
    "\n",
    "print(f\"‚úì LoRA configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {len(lora_config.target_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úì LoRA applied to model\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665501b",
   "metadata": {},
   "source": [
    "## 10. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6201d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the texts.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set tokenized: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úì Validation set tokenized: {len(tokenized_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚úì Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417fc9",
   "metadata": {},
   "source": [
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./llama3-medical-ner-lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=True,\n",
    "    \n",
    "    # Hugging Face Hub\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HF_MODEL_ID,\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    hub_private_repo=False,\n",
    "    \n",
    "    # Misc\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",  # Enable Weights & Biases logging\n",
    "    run_name=f\"llama3-medical-ner-{TIMESTAMP}\",  # W&B run name\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Hub model ID: {HF_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a993c",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b88564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Configure early stopping to prevent overfitting\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0))\n",
    "\n",
    "# Calculate training steps\n",
    "total_steps = (len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n",
    "\n",
    "print(f\"‚úì Trainer initialized\")\n",
    "print(f\"‚úì Expected training steps: ~{total_steps}\")\n",
    "print(f\"‚úì Expected checkpoints: ~{max(1, total_steps // training_args.save_steps)}\")\n",
    "print(\"‚úì Early stopping enabled (patience = 3 evaluations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b853b4",
   "metadata": {},
   "source": [
    "## 13. Start Training\n",
    "\n",
    "‚ö†Ô∏è **This will take 2-3 hours on an A100 GPU**\n",
    "\n",
    "The training will:\n",
    "- Save checkpoints every 100 steps\n",
    "- Upload checkpoints to Hugging Face Hub\n",
    "- Evaluate on validation set every 100 steps\n",
    "- Save the best model based on validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39341273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take 2-3 hours on A100 GPU...\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b3b39",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93373e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "\n",
    "print(f\"‚úì Model saved to: ./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "print(\"Pushing to Hugging Face Hub...\")\n",
    "\n",
    "try:\n",
    "    trainer.push_to_hub(commit_message=\"Training complete - final model\")\n",
    "    print(f\"‚úì Model pushed to: https://huggingface.co/{HF_MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Failed to push to hub: {e}\")\n",
    "    print(\"  You can manually push later using: trainer.push_to_hub()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed8e1e",
   "metadata": {},
   "source": [
    "## 15. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract losses\n",
    "train_loss = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "eval_loss = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_loss, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Evaluation Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training metrics plotted and saved to: training_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training steps: {len(train_loss)}\")\n",
    "print(f\"Final training loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {eval_loss[-1]:.4f}\")\n",
    "print(f\"Best validation loss: {min(eval_loss):.4f}\")\n",
    "print(f\"Loss reduction: {((train_loss[0] - train_loss[-1]) / train_loss[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdcba1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Training is complete! Your model has been saved.\n",
    "\n",
    "**To evaluate your model:**\n",
    "1. Open `Medical_NER_Evaluation.ipynb`\n",
    "2. Run the evaluation on the test set\n",
    "3. Test custom examples\n",
    "\n",
    "**Model locations:**\n",
    "- Local: `./final_model`\n",
    "- HuggingFace Hub: Check the output above for your model URL\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
