{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical NER Model Evaluation\n",
    "\n",
    "This notebook evaluates the fine-tuned Llama 3.2 3B medical NER model.\n",
    "\n",
    "## ‚úÖ DATASET VERIFIED & READY FOR EVALUATION\n",
    "\n",
    "**Current Dataset Distribution** (from `both_rel_instruct_all.jsonl`):\n",
    "- **1,000 Chemical extraction** examples (25%)\n",
    "- **2,000 Disease extraction** examples (50%) ‚ö†Ô∏è Intentionally 2x more\n",
    "- **1,000 Relationship extraction** examples (25%)\n",
    "\n",
    "**Data Splits Status**: ‚úÖ Properly stratified using `stratify=` parameter\n",
    "- Training (2,400): 25% chemical, 50% disease, 25% relationship\n",
    "- Validation (300+): 25% chemical, 50% disease, 25% relationship\n",
    "- Test (300+): 25% chemical, 50% disease, 25% relationship\n",
    "\n",
    "**Why Disease is 2x more**:\n",
    "- The original dataset has twice as many disease extraction examples\n",
    "- Stratified splitting preserves this 25/50/25 distribution\n",
    "- This appears intentional for better disease NER performance\n",
    "- All splits are properly balanced relative to the source data\n",
    "\n",
    "**Next Steps**:\n",
    "1. ‚úÖ Training data is properly split with stratification\n",
    "2. ‚úÖ No data leakage between train/val/test\n",
    "3. ‚úÖ Update `HF_MODEL_ID` below with your trained model ID\n",
    "4. ‚úÖ Run this evaluation notebook on the balanced test set\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites:\n",
    "1. Complete training in `Medical_NER_Fine_Tuning.ipynb` (uses stratified splits!)\n",
    "2. Model saved to `./final_model` or uploaded to HuggingFace Hub\n",
    "3. Test data available in `notebooks/test.jsonl` or `../data/test.jsonl`\n",
    "\n",
    "## Evaluation Tasks:\n",
    "1. Load the fine-tuned model\n",
    "2. Evaluate on test set (25% chem, 50% disease, 25% relationship)\n",
    "3. Calculate precision, recall, F1 scores per task type\n",
    "4. Test on custom medical texts\n",
    "5. Analyze errors and false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d2f11",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running the notebook!\n",
    "\n",
    "**Note**: `hf_transfer` is enabled for faster downloads from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Enable hf_transfer for faster downloads from HuggingFace Hub\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# HuggingFace Token (required to download your model from Hub)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"  # ‚Üê UPDATE THIS!\n",
    "\n",
    "# Weights & Biases API Key (optional - only if tracking evaluation metrics)\n",
    "# Get your key from: https://wandb.ai/authorize\n",
    "os.environ[\"WANDB_API_KEY\"] = \"d88df098d85360ac924ec2bf8dcf5520d745c411\"  # Uncomment if needed\n",
    "\n",
    "print(\"‚úì Environment variables set\")\n",
    "print(f\"  HF_HUB_ENABLE_HF_TRANSFER: {os.getenv('HF_HUB_ENABLE_HF_TRANSFER')} (Fast downloads enabled!)\")\n",
    "print(f\"  HF_TOKEN: {'‚úì Set' if os.getenv('HF_TOKEN') and os.getenv('HF_TOKEN') != 'hf_YOUR_TOKEN_HERE' else '‚úó Not set - UPDATE THIS!'}\")\n",
    "if os.getenv(\"WANDB_API_KEY\"):\n",
    "    print(f\"  WANDB_API_KEY: ‚úì Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1d2ca",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Install PyTorch first (for GPU support on remote pod)\n",
    "!pip install torch \n",
    "# Install other required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers hf-transfer\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers peft huggingface-hub\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **Update these paths** to match your model location!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Update with YOUR HuggingFace model ID\n",
    "# Find it at: https://huggingface.co/your-username\n",
    "# Format: \"your-username/llama3-medical-ner-lora-YYYYMMDD_HHMMSS\"\n",
    "HF_MODEL_ID = \"albyos/llama3-medical-ner-lora-20251029_143110\"  # ‚Üê UPDATE THIS!\n",
    "\n",
    "# Alternative: Use local model if you prefer\n",
    "USE_HF_HUB = True  # Set to False to use local ../final_model\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "LOCAL_MODEL_PATH = PROJECT_ROOT / \"final_model\"\n",
    "\n",
    "ADAPTER_PATH = HF_MODEL_ID if USE_HF_HUB else str(LOCAL_MODEL_PATH)\n",
    "\n",
    "# Data configuration - Use the test file in notebooks directory\n",
    "NOTEBOOKS_DIR = Path.cwd()  # Current notebooks directory\n",
    "TEST_DATA_PATH = NOTEBOOKS_DIR / \"test.jsonl\"\n",
    "\n",
    "# Verify test data exists\n",
    "if not TEST_DATA_PATH.exists():\n",
    "    print(f\"‚ùå Test data not found at {TEST_DATA_PATH}\")\n",
    "    print(f\"üí° Expected location: /workspace/ch_10_fine_tuning/notebooks/test.jsonl\")\n",
    "    raise FileNotFoundError(f\"Test data file not found: {TEST_DATA_PATH}\")\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  Adapter source: {'HuggingFace Hub' if USE_HF_HUB else 'Local filesystem'}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Test data: {TEST_DATA_PATH}\")\n",
    "print(f\"  Test data exists: {TEST_DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcae3b",
   "metadata": {},
   "source": [
    "## 4. Authenticate with Hugging Face\n",
    "\n",
    "Log into Hugging Face to download the LoRA adapter when `USE_HF_HUB` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HF_HUB:\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token and hf_token != \"hf_YOUR_TOKEN_HERE\":\n",
    "        login(token=hf_token, add_to_git_credential=True)\n",
    "        print(\"‚úì Logged into Hugging Face Hub\")\n",
    "    else:\n",
    "        raise ValueError(\"HF_TOKEN is not set. Update the Environment Variables cell before continuing.\")\n",
    "else:\n",
    "    print(\"Skipping Hugging Face login because USE_HF_HUB is False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6265a9c",
   "metadata": {},
   "source": [
    "## 5. Load the Fine-Tuned Model\n",
    "\n",
    "Load the base model and attach the LoRA adapter from either Hugging Face Hub or your local filesystem.\n",
    "\n",
    "**Note**: Using `hf_transfer` for faster downloads from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure hf_transfer is enabled for faster downloads\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING FINE-TUNED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nLoading base model: {BASE_MODEL}...\")\n",
    "\n",
    "#Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"‚úì Base model loaded: {BASE_MODEL}\")\n",
    "\n",
    "# Load LoRA adapter from HuggingFace Hub or local path\n",
    "print(f\"\\nLoading LoRA adapter from: {ADAPTER_PATH}...\")\n",
    "print(f\"  Using hf_transfer for faster downloads...\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_PATH,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úì Fine-tuned model loaded successfully!\")\n",
    "print(f\"  Base: {BASE_MODEL}\")\n",
    "print(f\"  LoRA adapter: {ADAPTER_PATH}\")\n",
    "print(f\"  Source: {'HuggingFace Hub' if USE_HF_HUB else 'Local filesystem'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt_text, max_new_tokens=512):\n",
    "    \"\"\"Generate a response for a given prompt.\"\"\"\n",
    "    formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,  # Increased from 0.1 to allow more diversity\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.2,  # ‚úÖ ADDED: Penalize repetition\n",
    "            no_repeat_ngram_size=3,  # ‚úÖ ADDED: Prevent 3-gram repetition\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    if \"assistant\\n\\n\" in response:\n",
    "        response = response.split(\"assistant\\n\\n\")[-1]\n",
    "    elif \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"‚úì Inference function ready\")\n",
    "print(\"  Generation parameters updated:\")\n",
    "print(\"    - temperature: 0.7 (more diverse)\")\n",
    "print(\"    - repetition_penalty: 1.2 (prevents loops)\")\n",
    "print(\"    - no_repeat_ngram_size: 3 (prevents 3-gram repetition)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857449b",
   "metadata": {},
   "source": [
    "## 6. Evaluate on the Held-Out Test Set\n",
    "\n",
    "Run inference on a subset of the unseen test set and compute per-sample precision, recall, and F1 scores.\n",
    "\n",
    "**Note**: The evaluation now uses case-insensitive matching to handle capitalization differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6043fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on COMPLETELY UNSEEN test samples\n",
    "# The test set was not used for training OR validation monitoring\n",
    "# Load test data\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "num_test_samples = len(test_data)\n",
    "print(f\"Testing on {num_test_samples} samples from TEST SET\")\n",
    "print(f\"Total test set size: {len(test_data)}\")\n",
    "print(f\"\\n‚ö†Ô∏è  IMPORTANT:\")\n",
    "print(f\"  - Training set (80%): Used for fine-tuning\")\n",
    "print(f\"  - Validation set (10%): Monitored during training (W&B)\")\n",
    "print(f\"  - Test set (10%): Used ONLY NOW for final evaluation\")\n",
    "\n",
    "# Aggregate metrics\n",
    "total_correct = 0\n",
    "total_predicted = 0\n",
    "total_expected = 0\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for comparison: lowercase and strip whitespace.\"\"\"\n",
    "    return text.lower().strip()\n",
    "\n",
    "for i, sample in enumerate(test_data[:num_test_samples]):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL TEST EXAMPLE {i+1}/{num_test_samples}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show prompt (truncated for readability)\n",
    "    print(f\"\\nüìù PROMPT:\")\n",
    "    prompt_preview = sample['prompt'][:250] + \"...\" if len(sample['prompt']) > 250 else sample['prompt']\n",
    "    print(f\"{prompt_preview}\")\n",
    "    \n",
    "    # Show expected output\n",
    "    print(f\"\\n‚úÖ EXPECTED OUTPUT:\")\n",
    "    print(f\"{sample['completion']}\")\n",
    "    \n",
    "    # Generate prediction\n",
    "    print(f\"\\nü§ñ MODEL PREDICTION:\")\n",
    "    prediction = generate_response(sample['prompt'])\n",
    "    print(f\"{prediction}\")\n",
    "    \n",
    "    # Calculate metrics with normalization for case-insensitive comparison\n",
    "    expected_items = set([normalize_text(item) for item in sample['completion'].split('\\n') if item.strip()])\n",
    "    predicted_items = set([normalize_text(item) for item in prediction.split('\\n') if item.strip()])\n",
    "    \n",
    "    common = expected_items & predicted_items\n",
    "    missing = expected_items - predicted_items\n",
    "    extra = predicted_items - expected_items\n",
    "    \n",
    "    # Update aggregate counts\n",
    "    total_correct += len(common)\n",
    "    total_predicted += len(predicted_items)\n",
    "    total_expected += len(expected_items)\n",
    "    \n",
    "    # Per-sample metrics\n",
    "    accuracy = len(common) / len(expected_items) * 100 if len(expected_items) > 0 else 0\n",
    "    precision = len(common) / len(predicted_items) * 100 if len(predicted_items) > 0 else 0\n",
    "    recall = len(common) / len(expected_items) * 100 if len(expected_items) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä EVALUATION METRICS:\")\n",
    "    print(f\"  ‚úì Correct extractions: {len(common)}/{len(expected_items)}\")\n",
    "    print(f\"  ‚úó Missed extractions: {len(missing)}\")\n",
    "    print(f\"  ‚ö† Extra extractions: {len(extra)}\")\n",
    "    print(f\"\\n  üìà Per-Sample Metrics:\")\n",
    "    print(f\"    Accuracy:  {accuracy:.1f}%\")\n",
    "    print(f\"    Precision: {precision:.1f}%\")\n",
    "    print(f\"    Recall:    {recall:.1f}%\")\n",
    "    print(f\"    F1 Score:  {f1:.1f}%\")\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"\\n  Missed items: {list(missing)[:3]}\")\n",
    "    if extra:\n",
    "        print(f\"  Extra items: {list(extra)[:3]}\")\n",
    "    \n",
    "    # Show matched items for verification\n",
    "    if common:\n",
    "        print(f\"\\n  ‚úì Matched items: {list(common)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319572a",
   "metadata": {},
   "source": [
    "## 7. Aggregate Metrics\n",
    "\n",
    "Summarize performance across the evaluated samples to understand overall precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Metrics across all test samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATE METRICS ACROSS TEST SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "aggregate_precision = total_correct / total_predicted * 100 if total_predicted > 0 else 0\n",
    "aggregate_recall = total_correct / total_expected * 100 if total_expected > 0 else 0\n",
    "aggregate_f1 = 2 * (aggregate_precision * aggregate_recall) / (aggregate_precision + aggregate_recall) if (aggregate_precision + aggregate_recall) > 0 else 0\n",
    "aggregate_accuracy = total_correct / total_expected * 100 if total_expected > 0 else 0\n",
    "\n",
    "print(f\"\\nEvaluated on {num_test_samples} test samples:\")\n",
    "print(f\"\\nüìä Overall Performance:\")\n",
    "print(f\"  Total expected entities:  {total_expected}\")\n",
    "print(f\"  Total predicted entities: {total_predicted}\")\n",
    "print(f\"  Correctly predicted:      {total_correct}\")\n",
    "\n",
    "print(f\"\\nüìà Aggregate Metrics:\")\n",
    "print(f\"  Accuracy:  {aggregate_accuracy:.2f}%\")\n",
    "print(f\"  Precision: {aggregate_precision:.2f}% (fewer false positives)\")\n",
    "print(f\"  Recall:    {aggregate_recall:.2f}% (fewer false negatives)\")\n",
    "print(f\"  F1 Score:  {aggregate_f1:.2f}% (balanced metric)\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"  - Accuracy: {aggregate_accuracy:.1f}% of expected entities were found\")\n",
    "print(f\"  - Precision: Of all entities predicted, {aggregate_precision:.1f}% were correct\")\n",
    "print(f\"  - Recall: Of all actual entities, {aggregate_recall:.1f}% were found\")\n",
    "print(f\"  - F1: Harmonic mean balancing precision and recall\")\n",
    "\n",
    "print(f\"\\nüéØ What these metrics mean:\")\n",
    "print(f\"  - High Precision, Low Recall ‚Üí Model is conservative (misses entities)\")\n",
    "print(f\"  - Low Precision, High Recall ‚Üí Model is aggressive (predicts too many)\")\n",
    "print(f\"  - High F1 Score ‚Üí Good balance between precision and recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3d158",
   "metadata": {},
   "source": [
    "## 7.5 False Positive Analysis\n",
    "\n",
    "Analyze the types of errors the model is making to understand and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed False Positive Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"FALSE POSITIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Re-analyze test data to collect all false positives\n",
    "all_false_positives = []\n",
    "all_false_negatives = []\n",
    "all_true_positives = []\n",
    "\n",
    "for i, sample in enumerate(test_data[:num_test_samples]):\n",
    "    prediction = generate_response(sample['prompt'])\n",
    "    \n",
    "    # Normalize for comparison\n",
    "    expected_items = set([normalize_text(item) for item in sample['completion'].split('\\n') if item.strip()])\n",
    "    predicted_items = set([normalize_text(item) for item in prediction.split('\\n') if item.strip()])\n",
    "    \n",
    "    common = expected_items & predicted_items\n",
    "    false_positives = predicted_items - expected_items  # Model predicted but not in ground truth\n",
    "    false_negatives = expected_items - predicted_items  # In ground truth but model missed\n",
    "    \n",
    "    all_false_positives.extend(false_positives)\n",
    "    all_false_negatives.extend(false_negatives)\n",
    "    all_true_positives.extend(common)\n",
    "\n",
    "print(f\"\\nüìä Error Distribution:\")\n",
    "print(f\"  True Positives:   {len(all_true_positives)} (Correct predictions)\")\n",
    "print(f\"  False Positives:  {len(all_false_positives)} (Extra/wrong predictions)\")\n",
    "print(f\"  False Negatives:  {len(all_false_negatives)} (Missed entities)\")\n",
    "\n",
    "# Calculate error rates\n",
    "total_predictions = len(all_true_positives) + len(all_false_positives)\n",
    "total_ground_truth = len(all_true_positives) + len(all_false_negatives)\n",
    "\n",
    "false_positive_rate = len(all_false_positives) / total_predictions * 100 if total_predictions > 0 else 0\n",
    "false_negative_rate = len(all_false_negatives) / total_ground_truth * 100 if total_ground_truth > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà Error Rates:\")\n",
    "print(f\"  False Positive Rate: {false_positive_rate:.1f}% (of all predictions)\")\n",
    "print(f\"  False Negative Rate: {false_negative_rate:.1f}% (of all expected)\")\n",
    "\n",
    "# Show example false positives\n",
    "print(f\"\\n‚ö†Ô∏è  Example False Positives (Extra predictions):\")\n",
    "for i, fp in enumerate(all_false_positives[:10], 1):\n",
    "    print(f\"  {i}. {fp}\")\n",
    "\n",
    "# Show example false negatives\n",
    "print(f\"\\n‚ùå Example False Negatives (Missed entities):\")\n",
    "for i, fn in enumerate(all_false_negatives[:10], 1):\n",
    "    print(f\"  {i}. {fn}\")\n",
    "\n",
    "# Analysis insights\n",
    "print(f\"\\nüí° Insights:\")\n",
    "if false_positive_rate > 20:\n",
    "    print(f\"  ‚ö†Ô∏è  High false positive rate ({false_positive_rate:.1f}%)\")\n",
    "    print(f\"     ‚Üí Model is too aggressive, predicting entities that aren't in ground truth\")\n",
    "    print(f\"     ‚Üí Consider: More conservative prompting, post-processing filters, or additional training\")\n",
    "elif false_positive_rate < 10:\n",
    "    print(f\"  ‚úì Low false positive rate ({false_positive_rate:.1f}%)\")\n",
    "    print(f\"     ‚Üí Model is conservative and precise\")\n",
    "\n",
    "if false_negative_rate > 20:\n",
    "    print(f\"  ‚ö†Ô∏è  High false negative rate ({false_negative_rate:.1f}%)\")\n",
    "    print(f\"     ‚Üí Model is missing many expected entities\")\n",
    "    print(f\"     ‚Üí Consider: More training data, longer context, or prompt engineering\")\n",
    "elif false_negative_rate < 10:\n",
    "    print(f\"  ‚úì Low false negative rate ({false_negative_rate:.1f}%)\")\n",
    "    print(f\"     ‚Üí Model has good recall\")\n",
    "\n",
    "print(f\"\\nüéØ Recommendations:\")\n",
    "if false_positive_rate > false_negative_rate:\n",
    "    print(f\"  Primary issue: TOO MANY FALSE POSITIVES\")\n",
    "    print(f\"  Solutions:\")\n",
    "    print(f\"    1. Add post-processing to filter common false positives\")\n",
    "    print(f\"    2. Adjust generation parameters (lower temperature, higher top_p)\")\n",
    "    print(f\"    3. Fine-tune with more negative examples\")\n",
    "    print(f\"    4. Use stricter prompt instructions\")\n",
    "else:\n",
    "    print(f\"  Primary issue: TOO MANY FALSE NEGATIVES\")\n",
    "    print(f\"  Solutions:\")\n",
    "    print(f\"    1. Increase training data quantity\")\n",
    "    print(f\"    2. Improve prompt clarity\")\n",
    "    print(f\"    3. Check if test data format matches training data\")\n",
    "    print(f\"    4. Consider ensemble methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8545782",
   "metadata": {},
   "source": [
    "## 8. Interpret the Metrics\n",
    "\n",
    "### Accuracy\n",
    "- **Formula**: `Correct / Total Expected`\n",
    "- **Meaning**: Percentage of expected entities that were correctly predicted\n",
    "- **Limitation**: Doesn't account for false positives (extra predictions)\n",
    "\n",
    "### Precision\n",
    "- **Formula**: `Correct / Total Predicted`\n",
    "- **Meaning**: Of all entities the model predicted, how many were correct?\n",
    "- **High Precision**: Model rarely makes false positive errors (rarely predicts wrong entities)\n",
    "\n",
    "### Recall\n",
    "- **Formula**: `Correct / Total Expected`\n",
    "- **Meaning**: Of all actual entities, how many did the model find?\n",
    "- **High Recall**: Model rarely makes false negative errors (rarely misses entities)\n",
    "\n",
    "### F1 Score\n",
    "- **Formula**: `2 √ó (Precision √ó Recall) / (Precision + Recall)`\n",
    "- **Meaning**: Harmonic mean that balances precision and recall\n",
    "- **Best metric**: When you care equally about false positives and false negatives\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Ground truth: ['aspirin', 'ibuprofen', 'NSAIDs']\n",
    "Prediction:   ['aspirin', 'ibuprofen']\n",
    "\n",
    "Accuracy:  66.7% (2/3 found)\n",
    "Precision: 100% (2/2 predicted were correct)\n",
    "Recall:    66.7% (2/3 actual entities found)\n",
    "F1 Score:  80.0% (balanced metric)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5800935",
   "metadata": {},
   "source": [
    "## 9. Custom Test Cases ‚Äî Comprehensive NER Evaluation\n",
    "\n",
    "Test the model's ability to:\n",
    "1. **Extract Chemicals** - Identify drug names and chemical compounds\n",
    "2. **Extract Diseases** - Identify medical conditions and diseases\n",
    "3. **Extract Relationships** - Identify which chemicals are related to which diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Chemical Extraction\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: CHEMICAL EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chemical_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "A patient was treated with aspirin and ibuprofen for pain relief. The combination of these NSAIDs proved effective in reducing inflammation. Additionally, metformin was prescribed for glucose control.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{chemical_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(chemical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ef19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Disease Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: DISEASE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "disease_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the diseases mentioned.\n",
    "\n",
    "The patient presented with hypertension, diabetes mellitus, and chronic kidney disease. Laboratory findings revealed proteinuria and elevated creatinine levels, suggesting diabetic nephropathy.\n",
    "\n",
    "List of extracted diseases:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{disease_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(disease_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b12487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Chemical-Disease Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: RELATIONSHIP EXTRACTION - BASIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_1 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract the relationships between chemicals and diseases mentioned in the text.\n",
    "\n",
    "Metformin is commonly prescribed for type 2 diabetes by improving insulin sensitivity and reducing hepatic glucose production. Aspirin is used in cardiovascular disease management in high-risk patients.\n",
    "\n",
    "List the chemical-disease relationships:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_1}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_1, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Multiple Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: RELATIONSHIP EXTRACTION - MULTIPLE PAIRS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_2 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Identify all chemical-disease pairs and their relationships.\n",
    "\n",
    "Long-term use of corticosteroids is associated with osteoporosis and increases the risk of bone fractures. NSAIDs are linked to chronic kidney disease and gastrointestinal bleeding in susceptible patients.\n",
    "\n",
    "List of chemical-disease relationships:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_2}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_2, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Complex Multi-Entity Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 5: COMPREHENSIVE EXTRACTION - ALL ENTITIES & RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_3 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract:\n",
    "1. All chemicals mentioned\n",
    "2. All diseases mentioned\n",
    "3. All relationships between chemicals and diseases\n",
    "\n",
    "The patient with rheumatoid arthritis was started on methotrexate for inflammatory joint disease. However, methotrexate is associated with hepatotoxicity and requires monitoring. The patient also has hypertension managed with lisinopril. Statins were prescribed for cardiovascular disease prevention given elevated cholesterol levels.\n",
    "\n",
    "Extracted information:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_3}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_3, max_new_tokens=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51eef3b",
   "metadata": {},
   "source": [
    "## 10. Suggested Next Steps\n",
    "\n",
    "- Evaluate the full test set (set `num_test_samples = len(test_data)`) to capture complete performance.\n",
    "- Compare with the base model to quantify the lift from fine-tuning.\n",
    "- Log metrics to Weights & Biases or another tracker for experiment history.\n",
    "- Export predictions for manual spot checks with subject-matter experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebbaa6",
   "metadata": {},
   "source": [
    "## 11. Usage Example (Optional)\n",
    "\n",
    "How to load the model in a production script or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and use the model later\n",
    "usage_code = '''\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter from Hub\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"your-username/llama3-medical-ner-lora\"  # Your model ID\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Use the model\n",
    "prompt = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "Patient was treated with metformin and insulin for diabetes management.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "# ... (use the generate_response function from above)\n",
    "'''\n",
    "\n",
    "print(\"Usage Example:\")\n",
    "print(\"=\"*80)\n",
    "print(usage_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d1ad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ‚úÖ Configured environment variables and authentication for Hugging Face and W&B.\n",
    "2. ‚úÖ Installed required evaluation dependencies.\n",
    "3. ‚úÖ Loaded the fine-tuned medical NER model (base + LoRA adapter).\n",
    "4. ‚úÖ Evaluated performance on unseen test samples with detailed metrics.\n",
    "5. ‚úÖ Aggregated precision, recall, and F1 across all evaluated examples.\n",
    "6. ‚úÖ Validated behaviour on curated chemical, disease, and relationship prompts.\n",
    "7. ‚úÖ Outlined next steps and provided a ready-to-use inference snippet.\n",
    "\n",
    "**Your medical NER evaluation workflow is ready! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
