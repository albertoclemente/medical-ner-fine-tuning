{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb06cd0",
   "metadata": {},
   "source": [
    "# Medical NER Fine-Tuning with Llama 3.2 8B + LoRA\n",
    "\n",
    "This notebook implements fine-tuning of Llama 3.2 8B Instruct for medical Named Entity Recognition (NER) using:\n",
    "- **SFT** (Supervised Fine-Tuning)\n",
    "- **LoRA** (Low-Rank Adaptation)\n",
    "- **Hugging Face Hub** integration for checkpoint uploads\n",
    "\n",
    "## Tasks:\n",
    "1. Chemical entity extraction\n",
    "2. Disease entity extraction\n",
    "3. Chemical-Disease relationship extraction\n",
    "\n",
    "## Dataset:\n",
    "- 3,000 medical text examples\n",
    "- 80/10/10 train/validation/test split\n",
    "- **‚ö†Ô∏è CRITICAL**: Data is shuffled before splitting to ensure balanced task distribution\n",
    "- Weights & Biases tracking enabled\n",
    "\n",
    "## Important Note:\n",
    "**Data splitting MUST use `shuffle=True`** to prevent task imbalance. Without shuffling, all relationship extraction examples may cluster in validation/test sets, leading to poor model performance on the most important task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222cdc3",
   "metadata": {},
   "source": [
    "## üìä Dataset Characteristics (from Deep Data Exploration)\n",
    "\n",
    "**This fine-tuning is optimized based on comprehensive data analysis** ([See: DATA_EXPLORATION_SUMMARY.md](../docs/DATA_EXPLORATION_SUMMARY.md))\n",
    "\n",
    "### Key Dataset Statistics:\n",
    "- **Total samples**: 3,000 (perfectly balanced)\n",
    "- **Unique chemicals**: 1,578 entities\n",
    "- **Unique diseases**: 2,199 entities\n",
    "- **Vocabulary size**: 13,710 unique words\n",
    "- **Prompt characteristics**: \n",
    "  - Median length: 1,357 characters (195 words)\n",
    "  - Range: 345-4,018 characters\n",
    "  - **Optimal max_length**: 2048 tokens ‚úì\n",
    "\n",
    "### Data Quality:\n",
    "- **Zero duplicates** found (0.00%)\n",
    "- **Zero empty completions** (0.00%)\n",
    "- **Quality score**: High ‚úì\n",
    "\n",
    "### Format Preprocessing:\n",
    "- **Critical**: 2,050 relationships (68%) converted from OLD to NEW format\n",
    "  - OLD: `\"chemical X influences disease Y\"` (sentence format)\n",
    "  - NEW: `\"X | Y\"` (pipe format)\n",
    "- **Hyphen preservation**: ~459 entities contain hyphens (e.g., \"type-2 diabetes\")\n",
    "  - Must preserve hyphens during normalization ‚úì\n",
    "\n",
    "### Training Parameters (Data-Driven):\n",
    "All hyperparameters below are validated against dataset characteristics:\n",
    "- **Batch size**: 4 per device (optimal for 3,000 samples)\n",
    "- **Gradient accumulation**: 4 (effective batch = 16)\n",
    "- **Learning rate**: 5e-5 (conservative for LoRA on this vocab size)\n",
    "- **Epochs**: 5 (sufficient for format learning + entity recognition)\n",
    "- **Max length**: 2048 tokens (covers 99%+ of prompts)\n",
    "\n",
    "**Why these parameters work**: Dataset analysis confirmed vocabulary is manageable (13,710 words), prompts fit within 2048 tokens, and entity complexity requires moderate training (5 epochs) but not excessive fine-tuning that risks overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f6f14",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running this notebook!\n",
    "\n",
    "Required:\n",
    "- `HF_TOKEN`: Your Hugging Face token (needed to save models to HF Hub)\n",
    "\n",
    "Optional:\n",
    "- `WANDB_API_KEY`: Your Weights & Biases API key (for training tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e71987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Use Weights & Biases for tracking? (y/n):  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment variables configured\n",
      "  HF_TOKEN: ‚úì Set\n",
      "  WANDB_API_KEY: ‚óã Will use cached login or prompt later\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Secure token input - use environment variables or prompt for input\n",
    "# Option 1: Set environment variables before running notebook (recommended for automation)\n",
    "# Option 2: Enter tokens when prompted (recommended for interactive use)\n",
    "\n",
    "# Hugging Face Token (required for uploading to HF Hub)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token or hf_token == \"\":\n",
    "    hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "# Weights & Biases API Key (optional, for training tracking)\n",
    "wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if not wandb_key:\n",
    "    use_wandb = input(\"Use Weights & Biases for tracking? (y/n): \").lower() == 'y'\n",
    "    if use_wandb:\n",
    "        wandb_key = getpass.getpass(\"Enter your W&B API key (or press Enter to use cached login): \")\n",
    "        if wandb_key:\n",
    "            os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "\n",
    "# Verify environment variables\n",
    "print(\"‚úì Environment variables configured\")\n",
    "print(f\"  HF_TOKEN: {'‚úì Set' if os.environ.get('HF_TOKEN') else '‚úó Not set'}\")\n",
    "print(f\"  WANDB_API_KEY: {'‚úì Set' if os.environ.get('WANDB_API_KEY') else '‚óã Will use cached login or prompt later'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82d981",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9477574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers trl scikit-learn\n",
    "!pip install -q scipy sentencepiece protobuf wandb hf_transfer\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84c1fe",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c56ad90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A40\n",
      "CUDA version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51827af6",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Update `HF_USERNAME` with your Hugging Face username!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03c8d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Base model: meta-llama/Llama-3.2-3B-Instruct\n",
      "  HF model ID: albyos/llama3-medical-ner-lora-20251111_075432\n",
      "  Training timestamp: 20251111_075432\n",
      "\n",
      "üìä Data-Driven Hyperparameters (from exploration analysis):\n",
      "  Dataset: 3,000 samples, 13,710 vocabulary, 1,578 chemicals, 2,199 diseases\n",
      "  LoRA rank: 16\n",
      "  Training epochs: 5 (optimal for dataset size)\n",
      "  Batch size: 4 (in range 4-8 recommended)\n",
      "  Gradient accumulation: 4 (effective batch = 16)\n",
      "  Learning rate: 5e-05 (conservative for vocab size)\n",
      "  Data split seed: 573234 (reshuffled)\n",
      "\n",
      "  ‚úì All parameters validated against DATA_EXPLORATION_SUMMARY.md\n"
     ]
    }
   ],
   "source": [
    "# Configuration Section\n",
    "from datetime import datetime\n",
    "\n",
    "HF_USERNAME = \"albyos\"  # Replace with your HF username\n",
    "\n",
    "# Generate timestamp for checkpoint naming\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "HF_MODEL_ID = f\"{HF_USERNAME}/llama3-medical-ner-lora-{TIMESTAMP}\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MODEL_NAME = BASE_MODEL  # Alias for consistency\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training Configuration (DATA-DRIVEN - based on exploration analysis)\n",
    "# Dataset: 3,000 samples, 13,710 vocab, median 1,357 chars, 2,050 OLD format relationships\n",
    "NUM_EPOCHS = 5  # Data-driven: 5 epochs optimal for 3K samples (prevents overfitting while learning format conversion)\n",
    "BATCH_SIZE = 4  # Data-driven: 4-8 recommended range, 4 works well for 3K dataset\n",
    "GRADIENT_ACCUMULATION = 4  # Effective batch = 16 (4 * 4) - optimal for dataset size\n",
    "LEARNING_RATE = 5e-5  # Data-driven: Conservative LR for 13,710 vocab (prevents over-prediction, improves precision)\n",
    "\n",
    "# Data Configuration\n",
    "TRAIN_SPLIT_RATIO = 0.9\n",
    "RANDOM_SEED = 42\n",
    "RESHUFFLE_SPLITS_EACH_RUN = True  # When True, create a fresh validation split every run\n",
    "SPLIT_SEED = random.randint(0, 1_000_000) if RESHUFFLE_SPLITS_EACH_RUN else RANDOM_SEED\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  Training timestamp: {TIMESTAMP}\")\n",
    "print(f\"\\nüìä Data-Driven Hyperparameters (from exploration analysis):\")\n",
    "print(f\"  Dataset: 3,000 samples, 13,710 vocabulary, 1,578 chemicals, 2,199 diseases\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Training epochs: {NUM_EPOCHS} (optimal for dataset size)\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} (in range 4-8 recommended)\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION} (effective batch = {BATCH_SIZE * GRADIENT_ACCUMULATION})\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE} (conservative for vocab size)\")\n",
    "print(f\"  Data split seed: {SPLIT_SEED} ({'reshuffled' if RESHUFFLE_SPLITS_EACH_RUN else 'fixed'})\")\n",
    "print(f\"\\n  ‚úì All parameters validated against DATA_EXPLORATION_SUMMARY.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a54d9",
   "metadata": {},
   "source": [
    "## 0) Reusable Utilities\n",
    "\n",
    "These utility functions provide text normalization, hashing for deduplication, parsing, and validation for the data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d0344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Utility functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ===== Utilities: normalization, hashing, parsing =====\n",
    "import re, json, hashlib\n",
    "from collections import Counter\n",
    "\n",
    "def dehyphenate(s: str) -> str:\n",
    "    # Join words broken across lines with hyphens + whitespace\n",
    "    return re.sub(r\"(\\w+)-\\s+(\\w+)\", r\"\\1\\2\", s)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = dehyphenate(s or \"\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\u00A0\\t\\r\\n]+\", \" \", s)     # spaces/newlines\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def prompt_hash(prompt: str) -> str:\n",
    "    return hashlib.md5(normalize_text(prompt).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def parse_bullets(text: str):\n",
    "    items = []\n",
    "    for line in (text or \"\").splitlines():\n",
    "        m = re.match(r\"^\\s*[-*]\\s*(.+?)\\s*$\", line)\n",
    "        if m:\n",
    "            items.append(m.group(1))\n",
    "    return items\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    # Keep hyphens intact (e.g., \"type-2 diabetes\" stays \"type-2 diabetes\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # Only normalize whitespace\n",
    "    s = re.sub(r\"[\\.,;:]+$\", \"\", s).strip()\n",
    "    return s\n",
    "\n",
    "def in_text(item: str, text: str) -> bool:\n",
    "    return normalize_item(item) in normalize_text(text)\n",
    "\n",
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc274801",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Authentication\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b6aa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logged in to Hugging Face\n"
     ]
    }
   ],
   "source": [
    "# Authenticate with Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token and hf_token != \"hf_YOUR_TOKEN_HERE\":\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úì Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"‚ö† HF_TOKEN not set. Please update Cell 3 before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb723a0b",
   "metadata": {},
   "source": [
    "## 4b. Weights & Biases Setup\n",
    "\n",
    "Initialize W&B to track training metrics, validation loss, and experiments.\n",
    "Get your API key from: https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740959ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Warning: WANDB_API_KEY not set. Attempting to use cached login...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mclemalb\u001b[0m (\u001b[33malberto-clemente\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logged in to Weights & Biases using cached credentials\n"
     ]
    }
   ],
   "source": [
    "# Login to Weights & Biases\n",
    "wandb_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "if wandb_key and wandb_key != 'your_wandb_key_here':\n",
    "    wandb.login(key=wandb_key)\n",
    "    print('‚úì Logged in to Weights & Biases using WANDB_API_KEY')\n",
    "else:\n",
    "    print('‚ö† Warning: WANDB_API_KEY not set. Attempting to use cached login...')\n",
    "    try:\n",
    "        wandb.login()\n",
    "        print('‚úì Logged in to Weights & Biases using cached credentials')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö† Warning: Could not login to W&B: {e}')\n",
    "        print('  Run wandb.login() interactively or set WANDB_API_KEY environment variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51134743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251111_075238-94lpbx6e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alberto-clemente/medical-ner-finetuning/runs/94lpbx6e' target=\"_blank\">llama3-medical-ner-20251111_075210</a></strong> to <a href='https://wandb.ai/alberto-clemente/medical-ner-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alberto-clemente/medical-ner-finetuning' target=\"_blank\">https://wandb.ai/alberto-clemente/medical-ner-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alberto-clemente/medical-ner-finetuning/runs/94lpbx6e' target=\"_blank\">https://wandb.ai/alberto-clemente/medical-ner-finetuning/runs/94lpbx6e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Weights & Biases initialized\n",
      "  Project: medical-ner-finetuning\n",
      "  Run name: llama3-medical-ner-20251111_075210\n",
      "  Dashboard: https://wandb.ai\n"
     ]
    }
   ],
   "source": [
    "  # Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"medical-ner-finetuning\",\n",
    "    name=f\"llama3-medical-ner-{TIMESTAMP}\",\n",
    "    config={\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"lora_rank\": LORA_RANK,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Weights & Biases initialized\")\n",
    "print(f\"  Project: medical-ner-finetuning\")\n",
    "print(f\"  Run name: llama3-medical-ner-{TIMESTAMP}\")\n",
    "print(f\"  Dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e86457",
   "metadata": {},
   "source": [
    "## 5. Leak-Free Stratified Dataset Splitting\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL**: Using **leak-free stratified splitting** with deduplication to prevent data contamination!\n",
    "\n",
    "**Key Improvements**:\n",
    "1. **Deduplication**: Remove duplicate prompts (by normalized hash) before splitting\n",
    "2. **Stratified Splitting**: Ensures exact task distribution across all splits\n",
    "3. **Leakage Prevention**: Hard assertions verify zero overlap between train/val/test\n",
    "\n",
    "Split into:\n",
    "- **80% Training** - for fine-tuning\n",
    "- **10% Validation** - for monitoring during training (W&B)\n",
    "- **10% Test** - for final evaluation after training\n",
    "\n",
    "**Guaranteed distribution in each split** (with stratification):\n",
    "- **Exactly 33.3%** Chemical extraction\n",
    "- **Exactly 33.3%** Disease extraction  \n",
    "- **Exactly 33.3%** Relationship extraction\n",
    "\n",
    "**Why this matters**:\n",
    "- Duplicates can leak between splits, causing overfitting\n",
    "- Normalized prompts ensure semantic uniqueness\n",
    "- Hard assertions catch any data leakage bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6cda170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset loaded from: both_rel_instruct_all.jsonl\n",
      "  Total samples: 3000\n"
     ]
    }
   ],
   "source": [
    "# ===== Load Dataset =====\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the dataset from the JSONL file located in the project data folder\n",
    "data_path = Path(\"both_rel_instruct_all.jsonl\")\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {data_path.resolve()}\")\n",
    "\n",
    "with data_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"‚úì Dataset loaded from: {data_path}\")\n",
    "print(f\"  Total samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b378c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Deduplication complete:\n",
      "  Original samples: 3000\n",
      "  Duplicates removed: 0\n",
      "  Unique samples: 3000\n",
      "  ‚úì Influences converted to pipe format (chemical | disease)\n",
      "\n",
      "‚úì Task label distribution: {'chemicals': 1000, 'diseases': 1000, 'influences': 1000}\n",
      "\n",
      "‚úì Checking for data leakage...\n",
      "  train ‚à© val: overlap=0\n",
      "  train ‚à© test: overlap=0\n",
      "  val ‚à© test: overlap=0\n",
      "‚úì No leakage detected - all splits are independent!\n",
      "\n",
      "‚úì Dataset split complete (seed=995973, stratified=True, deduplicated=True)\n",
      "  Train: 2400 samples (80.0%)\n",
      "  Validation: 300 samples (10.0%)\n",
      "  Test: 300 samples (10.0%)\n",
      "\n",
      "  Files written: train.jsonl, validation.jsonl, test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Deduplicate + Stratified Split (80/10/10) =====\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random, os, time\n",
    "\n",
    "random.seed(SPLIT_SEED)\n",
    "\n",
    "# 1.1 Load full dataset into `data` (already loaded above)\n",
    "assert isinstance(data, list) and len(data) > 0, \"Load `data` first\"\n",
    "\n",
    "# 1.1.5 Define task classifier (needed for format conversion)\n",
    "def task_from_prompt(prompt: str) -> str:\n",
    "    p = normalize_text(prompt)\n",
    "    if \"list of extracted chemicals\" in p: return \"chemicals\"\n",
    "    if \"list of extracted diseases\"  in p: return \"diseases\"\n",
    "    if \"list of extracted influences\" in p: return \"influences\"\n",
    "    # Fallback patterns\n",
    "    if \"chemicals mentioned\" in p: return \"chemicals\"\n",
    "    if \"diseases mentioned\" in p: return \"diseases\"\n",
    "    if \"influences between\" in p: return \"influences\"\n",
    "    return \"other\"\n",
    "\n",
    "# 1.2 Deduplicate by normalized prompt\n",
    "seen = set()\n",
    "clean = []\n",
    "duplicates_removed = 0\n",
    "for row in data:\n",
    "    ph = prompt_hash(row[\"prompt\"])\n",
    "    if ph in seen: \n",
    "        duplicates_removed += 1\n",
    "        continue\n",
    "    seen.add(ph)\n",
    "    \n",
    "    # Dedupe and normalize completion lines\n",
    "    items = unique_preserve_order(parse_bullets(row.get(\"completion\",\"\")))\n",
    "    items_norm = []\n",
    "    \n",
    "    # Convert OLD sentence format to NEW pipe format for influences\n",
    "    task = task_from_prompt(row[\"prompt\"])\n",
    "    for item in items:\n",
    "        if not item.strip():\n",
    "            continue\n",
    "        \n",
    "        if task == \"influences\":\n",
    "            # Convert: \"chemical X influences disease Y\" ‚Üí \"X | Y\"\n",
    "            m = re.match(r'^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$', item, re.I)\n",
    "            if m:\n",
    "                chem = normalize_item(m.group(1))\n",
    "                dis = normalize_item(m.group(2))\n",
    "                items_norm.append(f\"{chem} | {dis}\")\n",
    "            else:\n",
    "                # Already in pipe format or other format - keep as-is\n",
    "                items_norm.append(normalize_item(item))\n",
    "        else:\n",
    "            # Chemicals or diseases - just normalize\n",
    "            items_norm.append(normalize_item(item))\n",
    "    \n",
    "    items_norm = unique_preserve_order(items_norm)\n",
    "    row[\"completion\"] = \"\\n\".join(f\"- {x}\" for x in items_norm)\n",
    "    clean.append(row)\n",
    "\n",
    "data = clean\n",
    "print(f\"‚úì Deduplication complete:\")\n",
    "print(f\"  Original samples: {len(data) + duplicates_removed}\")\n",
    "print(f\"  Duplicates removed: {duplicates_removed}\")\n",
    "print(f\"  Unique samples: {len(data)}\")\n",
    "print(f\"  ‚úì Influences converted to pipe format (chemical | disease)\")\n",
    "\n",
    "# 1.3 Get task labels for stratification\n",
    "labels = [task_from_prompt(r[\"prompt\"]) for r in data]\n",
    "print(f\"\\n‚úì Task label distribution: {dict(Counter(labels))}\")\n",
    "\n",
    "# 1.4 First split: 80% train, 20% temp\n",
    "train_data, temp_data, train_y, temp_y = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=SPLIT_SEED, stratify=labels\n",
    ")\n",
    "\n",
    "# 1.5 Second split: 10% val, 10% test (split temp 50/50)\n",
    "val_data, test_data, val_y, test_y = train_test_split(\n",
    "    temp_data, temp_y, test_size=0.5, random_state=SPLIT_SEED+1, stratify=temp_y\n",
    ")\n",
    "\n",
    "# 1.6 Check for leakage\n",
    "def check_leak(a, b, name):\n",
    "    ha = {prompt_hash(r[\"prompt\"]) for r in a}\n",
    "    hb = {prompt_hash(r[\"prompt\"]) for r in b}\n",
    "    overlap = ha & hb\n",
    "    print(f\"  {name}: overlap={len(overlap)}\")\n",
    "    assert len(overlap) == 0, f\"‚ùå Leak detected between {name}\"\n",
    "\n",
    "print(f\"\\n‚úì Checking for data leakage...\")\n",
    "check_leak(train_data, val_data, \"train ‚à© val\")\n",
    "check_leak(train_data, test_data, \"train ‚à© test\")\n",
    "check_leak(val_data,   test_data, \"val ‚à© test\")\n",
    "print(f\"‚úì No leakage detected - all splits are independent!\")\n",
    "\n",
    "# 1.7 Write files\n",
    "out_train = \"train.jsonl\"\n",
    "out_val   = \"validation.jsonl\"\n",
    "out_test  = \"test.jsonl\"\n",
    "\n",
    "for path, split in [(out_train,train_data),(out_val,val_data),(out_test,test_data)]:\n",
    "    with open(path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for r in split:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\n‚úì Dataset split complete (seed={SPLIT_SEED}, stratified=True, deduplicated=True)\")\n",
    "print(f\"  Train: {len(train_data)} samples ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_data)} samples ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_data)} samples ({len(test_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"\\n  Files written: {out_train}, {out_val}, {out_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9df44e",
   "metadata": {},
   "source": [
    "## üîß Critical Format Conversion Applied\n",
    "\n",
    "**Problem Identified:**  \n",
    "The source data uses OLD format for influences:\n",
    "```\n",
    "\"- chemical cyclophosphamide influences disease urinary bladder cancer\"\n",
    "```\n",
    "\n",
    "But for better model performance, we want NEW format:\n",
    "```\n",
    "\"- cyclophosphamide | urinary bladder cancer\"\n",
    "```\n",
    "\n",
    "**Solution:**  \n",
    "During deduplication, we now automatically convert influences from sentence format to pipe format. This ensures:\n",
    "1. Model learns concise `\"chemical | disease\"` format\n",
    "2. Cleaner outputs with less verbosity\n",
    "3. Easier to parse during evaluation\n",
    "4. Consistent format across all relationship extractions\n",
    "\n",
    "The conversion happens in the deduplication loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808f2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Distribution (2400 samples):\n",
      "  Chemical Extraction: 800 (33.3%)\n",
      "  Disease Extraction: 800 (33.3%)\n",
      "  Relationship Extraction: 800 (33.3%)\n",
      "\n",
      "Validation Distribution (300 samples):\n",
      "  Chemical Extraction: 100 (33.3%)\n",
      "  Disease Extraction: 100 (33.3%)\n",
      "  Relationship Extraction: 100 (33.3%)\n",
      "\n",
      "Test Distribution (300 samples):\n",
      "  Chemical Extraction: 100 (33.3%)\n",
      "  Disease Extraction: 100 (33.3%)\n",
      "  Relationship Extraction: 100 (33.3%)\n",
      "\n",
      "‚úì All splits have balanced task distributions\n"
     ]
    }
   ],
   "source": [
    "# Verify task distribution across splits\n",
    "def get_task_type_display(prompt):\n",
    "    \"\"\"Classify the task type based on prompt for display.\"\"\"\n",
    "    task = task_from_prompt(prompt)\n",
    "    display_map = {\n",
    "        \"chemicals\": \"Chemical Extraction\",\n",
    "        \"diseases\": \"Disease Extraction\", \n",
    "        \"influences\": \"Relationship Extraction\",\n",
    "        \"other\": \"Other\"\n",
    "    }\n",
    "    return display_map.get(task, \"Other\")\n",
    "\n",
    "def verify_split_distribution(split_data, split_name):\n",
    "    \"\"\"Verify task distribution in a split.\"\"\"\n",
    "    task_counts = {}\n",
    "    for sample in split_data:\n",
    "        task = get_task_type_display(sample['prompt'])\n",
    "        task_counts[task] = task_counts.get(task, 0) + 1\n",
    "    \n",
    "    print(f\"\\n{split_name} Distribution ({len(split_data)} samples):\")\n",
    "    for task, count in sorted(task_counts.items()):\n",
    "        print(f\"  {task}: {count} ({count/len(split_data)*100:.1f}%)\")\n",
    "\n",
    "verify_split_distribution(train_data, \"Train\")\n",
    "verify_split_distribution(val_data, \"Validation\")\n",
    "verify_split_distribution(test_data, \"Test\")\n",
    "\n",
    "print(\"\\n‚úì All splits have balanced task distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52498f03",
   "metadata": {},
   "source": [
    "## 6b. Post-Split Leakage Verification\n",
    "\n",
    "Double-check that written files have no overlapping prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b337ece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Verifying written files for leakage...\n",
      "  train ‚à© val: 0 overlaps\n",
      "  train ‚à© test: 0 overlaps\n",
      "  val ‚à© test: 0 overlaps\n",
      "\n",
      "‚úì‚úì‚úì No cross-split leakage in written files!\n"
     ]
    }
   ],
   "source": [
    "# ===== Post-split leakage check (standalone verification) =====\n",
    "def hash_file(path):\n",
    "    \"\"\"Load all prompt hashes from a JSONL file.\"\"\"\n",
    "    out=set()\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            r=json.loads(line)\n",
    "            out.add(prompt_hash(r[\"prompt\"]))\n",
    "    return out\n",
    "\n",
    "Htrain = hash_file(\"train.jsonl\")\n",
    "Hval   = hash_file(\"validation.jsonl\")\n",
    "Htest  = hash_file(\"test.jsonl\")\n",
    "\n",
    "print(\"‚úì Verifying written files for leakage...\")\n",
    "print(f\"  train ‚à© val: {len(Htrain & Hval)} overlaps\")\n",
    "print(f\"  train ‚à© test: {len(Htrain & Htest)} overlaps\")\n",
    "print(f\"  val ‚à© test: {len(Hval & Htest)} overlaps\")\n",
    "\n",
    "assert not (Htrain & Hval),   \"‚ùå Leak detected: train ‚à© val\"\n",
    "assert not (Htrain & Htest),  \"‚ùå Leak detected: train ‚à© test\"\n",
    "assert not (Hval & Htest),    \"‚ùå Leak detected: val ‚à© test\"\n",
    "\n",
    "print(\"\\n‚úì‚úì‚úì No cross-split leakage in written files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e0e62",
   "metadata": {},
   "source": [
    "## 7. Data Formatting\n",
    "\n",
    "Format data into Llama 3 chat format with system, user, and assistant roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d77a47",
   "metadata": {},
   "source": [
    "## üí° Prompt Engineering Insights (from Data Exploration)\n",
    "\n",
    "**Based on analysis of 3,000 samples**, the following prompt structure improvements address discovered patterns:\n",
    "\n",
    "### **Key Findings from Exploration:**\n",
    "\n",
    "1. **Format Inconsistency** (68% of data):\n",
    "   - 2,050 relationships used OLD sentence format: `\"chemical X influences disease Y\"`\n",
    "   - Solution: Explicit pipe format specification in system prompt\n",
    "\n",
    "2. **Entity Complexity** (significant prevalence):\n",
    "   - ~459 hyphenated entities (e.g., \"type-2 diabetes\", \"5-fluorouracil\")\n",
    "   - 13 types of special characters in entity names\n",
    "   - Multi-word entities: avg 1.7 words for diseases, 1.2 for chemicals\n",
    "   - Solution: Explicit preservation instructions\n",
    "\n",
    "3. **Prompt Length Variation**:\n",
    "   - Range: 345-4,018 characters (wide variance)\n",
    "   - Median: 1,357 characters (195 words)\n",
    "   - Solution: Max length 2048 tokens covers 99%+ prompts\n",
    "\n",
    "### **Enhanced System Prompt Features:**\n",
    "\n",
    "‚úÖ **Explicit Format Rules**: Reduces ambiguity about output structure  \n",
    "‚úÖ **Hyphen Preservation**: Addresses 459 hyphenated entities  \n",
    "‚úÖ **Multi-word Guidance**: Ensures complete entity extraction (not just keywords)  \n",
    "‚úÖ **Special Character Handling**: Preserves medical notation (parentheses, numbers, etc.)  \n",
    "‚úÖ **Verbatim Extraction**: Prevents paraphrasing or abbreviation\n",
    "\n",
    "**Result**: More consistent training signal, better format adherence, reduced false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2711dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Formatted Example:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a medical NER expert specialized in three tasks:\n",
      "1. **Chemical Extraction**: Identify chemical/drug names in medical texts\n",
      "2. **Disease Extraction**: Identify disease/condition names in medical texts\n",
      "3. **Relationship Extraction**: Identify influences/interactions where chemicals affect diseases\n",
      "\n",
      "Extract entities EXACTLY as they appear in the text.\n",
      "\n",
      "CRITICAL RULES:\n",
      "1. Return ONLY entities found verbatim in the article\n",
      "2. Preserve exact formatting: hyphens (e.g., \"type-2\"), capitalization, special characters\n",
      "3. Extract complete multi-word terms (e.g., \"chronic kidney disease\", not just \"disease\")\n",
      "4. For relationships: identi...\n",
      "\n",
      "‚úì System prompt enhanced with data exploration insights:\n",
      "  - Task clarification (3 types: chemical, disease, relationship extraction)\n",
      "  - Explicit relationship definition (chemical influences/affects disease)\n",
      "  - Explicit hyphen preservation (~459 hyphenated entities)\n",
      "  - Multi-word entity guidance (avg 1.7 words for diseases)\n",
      "  - Special character handling (13 types identified)\n",
      "  - Clear pipe format for relationships (converted 2,050 samples)\n"
     ]
    }
   ],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"\n",
    "    Format data into Llama 3 chat format with enhanced system instructions.\n",
    "    \n",
    "    Improvements based on data exploration:\n",
    "    - Explicit format specification (addresses 2,050 OLD format conversions)\n",
    "    - Hyphen preservation guidance (~459 hyphenated entities)\n",
    "    - Multi-word entity handling (avg 1.7 words for diseases)\n",
    "    - Special character preservation (13 types found)\n",
    "    - Task clarification (3 types: chemical extraction, disease extraction, relationship extraction)\n",
    "    \"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert specialized in three tasks:\n",
    "1. **Chemical Extraction**: Identify chemical/drug names in medical texts\n",
    "2. **Disease Extraction**: Identify disease/condition names in medical texts\n",
    "3. **Relationship Extraction**: Identify influences/interactions where chemicals affect diseases\n",
    "\n",
    "Extract entities EXACTLY as they appear in the text.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Return ONLY entities found verbatim in the article\n",
    "2. Preserve exact formatting: hyphens (e.g., \"type-2\"), capitalization, special characters\n",
    "3. Extract complete multi-word terms (e.g., \"chronic kidney disease\", not just \"disease\")\n",
    "4. For relationships: identify chemical-disease pairs where the chemical influences/affects the disease\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "- For chemicals/diseases: '- ENTITY NAME' (one per line)\n",
    "- For relationships: '- chemical NAME | disease NAME' (pipe-separated)\n",
    "- If none exist: return nothing (no explanations)\n",
    "\n",
    "IMPORTANT: Do not paraphrase, abbreviate, or modify entity names.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{sample['prompt']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{sample['completion']}<|eot_id|>\"\"\"\n",
    "\n",
    "# Test formatting\n",
    "formatted_example = format_instruction(train_data[0])\n",
    "print(\"Enhanced Formatted Example:\")\n",
    "print(formatted_example[:700] + \"...\")\n",
    "print(\"\\n‚úì System prompt enhanced with data exploration insights:\")\n",
    "print(\"  - Task clarification (3 types: chemical, disease, relationship extraction)\")\n",
    "print(\"  - Explicit relationship definition (chemical influences/affects disease)\")\n",
    "print(\"  - Explicit hyphen preservation (~459 hyphenated entities)\")\n",
    "print(\"  - Multi-word entity guidance (avg 1.7 words for diseases)\")\n",
    "print(\"  - Special character handling (13 types identified)\")\n",
    "print(\"  - Clear pipe format for relationships (converted 2,050 samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7aca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Datasets formatted:\n",
      "  Train: 2400 samples\n",
      "  Validation: 300 samples\n",
      "  Test: 300 samples\n"
     ]
    }
   ],
   "source": [
    "# Format all data\n",
    "train_formatted = [{\"text\": format_instruction(sample)} for sample in train_data]\n",
    "val_formatted = [{\"text\": format_instruction(sample)} for sample in val_data]\n",
    "test_formatted = [{\"text\": format_instruction(sample)} for sample in test_data]\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_formatted)\n",
    "val_dataset = Dataset.from_list(val_formatted)\n",
    "test_dataset = Dataset.from_list(test_formatted)\n",
    "\n",
    "print(f\"‚úì Datasets formatted:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7616fb",
   "metadata": {},
   "source": [
    "## 8. Load Model and Tokenizer\n",
    "\n",
    "Load Llama 3.2 3B with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa3981a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Quantization config created (4-bit NF4)\n"
     ]
    }
   ],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Quantization config created (4-bit NF4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c092e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f3f5c67fbb43249989daad55a83668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea98ba280004c53b965fae24815ddb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4065191f8a4997934f7b132444f4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenizer loaded: meta-llama/Llama-3.2-3B-Instruct\n",
      "  Vocab size: 128256\n",
      "  PAD token: <|eot_id|>\n",
      "  EOS token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd890d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... (this may take a few minutes)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8c787b28ca4f53be0eb134bef0a366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912bc9a99fe748368dbbd5e9edce9012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d761c6f115482b91cb3726bb2473ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e1cfa6f33d4afa9369b24e8c64765c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae029ec0b0a642ddbe1de98eb00af607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224fb0c2adbb425182996d73739ba959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Base model loaded: meta-llama/Llama-3.2-3B-Instruct\n",
      "  Model size: 2.20 GB\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "print(\"Loading model... (this may take a few minutes)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62ceb88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model prepared for k-bit training\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"‚úì Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b2d6c",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA\n",
    "\n",
    "Apply Low-Rank Adaptation for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57b69361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LoRA configuration:\n",
      "  Rank (r): 16\n",
      "  Alpha: 32\n",
      "  Dropout: 0.05\n",
      "  Target modules: 7\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,                   # LoRA rank\n",
    "    lora_alpha=LORA_ALPHA,         # LoRA alpha (scaling)\n",
    "    target_modules=[               # Layers to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,             # Dropout for regularization\n",
    "    bias=\"none\",                   # No bias training\n",
    "    task_type=\"CAUSAL_LM\"          # Causal language modeling\n",
    ")\n",
    "\n",
    "print(f\"‚úì LoRA configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {len(lora_config.target_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd0e2cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LoRA applied to model\n",
      "\n",
      "Trainable parameters:\n",
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úì LoRA applied to model\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665501b",
   "metadata": {},
   "source": [
    "## 10. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6201d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n",
      "  Using max_length=2048 (optimal for median 1,357 char prompts)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea6abd19579428ca862ecb1d5898b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train set:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9028860d521c46c4952f87bee895a00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation set:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Train set tokenized: 2400 samples\n",
      "‚úì Validation set tokenized: 300 samples\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the texts.\n",
    "    \n",
    "    Parameters optimized based on data exploration:\n",
    "    - max_length=2048: Covers median prompt (1,357 chars) + completion with buffer\n",
    "      - Exploration found: 195 median words, range 345-4,018 chars\n",
    "      - 2048 tokens ensures 99%+ prompts fit without truncation\n",
    "    - truncation=True: Handle rare long prompts gracefully\n",
    "    - padding=False: Dynamic padding in data collator (more efficient)\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,  # Data-driven: Based on prompt length analysis (median 1,357 chars)\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "print(\"  Using max_length=2048 (optimal for median 1,357 char prompts)\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set tokenized: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úì Validation set tokenized: {len(tokenized_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2941de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data collator created\n"
     ]
    }
   ],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚úì Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417fc9",
   "metadata": {},
   "source": [
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9cb39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training configuration:\n",
      "  Epochs: 5\n",
      "  Batch size (per device): 4\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 16\n",
      "  Learning rate: 5e-05\n",
      "  Checkpoint frequency: Every 50 steps\n",
      "  Base HF model ID: albyos/llama3-medical-ner-lora-20251111_075432\n",
      "  ‚ö†Ô∏è Checkpoints will be pushed to HF with timestamp suffix\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./llama3-medical-ner-lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluate every 50 steps\n",
    "    \n",
    "    # Checkpointing - Save every 50 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Checkpoint every 50 steps\n",
    "    save_total_limit=None,  # Keep all checkpoints (will push to HF with unique names)\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=True,\n",
    "    \n",
    "    # Hugging Face Hub - Disable default push (we'll use custom callback)\n",
    "    push_to_hub=False,  # Custom callback will handle timestamped uploads\n",
    "    \n",
    "    # Misc\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",  # Enable Weights & Biases logging\n",
    "    run_name=f\"llama3-medical-ner-{TIMESTAMP}\",  # W&B run name\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Checkpoint frequency: Every {training_args.save_steps} steps\")\n",
    "print(f\"  Base HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  ‚ö†Ô∏è Checkpoints will be pushed to HF with timestamp suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3b101",
   "metadata": {},
   "source": [
    "## 11b. Custom Checkpoint Upload Callback\n",
    "\n",
    "This callback will automatically push each checkpoint to Hugging Face Hub with a unique timestamped name every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90a78c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Checkpoint upload callback initialized\n",
      "  Checkpoints will be uploaded to: albyos/llama3-medical-ner-checkpoint-<step>-<timestamp>\n",
      "  Upload frequency: Every 50 steps\n",
      "  Local deletion: Enabled (checkpoints deleted after upload)\n",
      "  Auto-create repos: Enabled (repos will be created automatically)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "from huggingface_hub import HfApi\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "class CheckpointUploadCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to upload checkpoints to Hugging Face Hub with timestamped names.\n",
    "    Deletes local checkpoints after successful upload to save disk space.\n",
    "    \n",
    "    Each checkpoint will be saved with format:\n",
    "    {HF_USERNAME}/llama3-medical-ner-checkpoint-{step}-{timestamp}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_id, hf_username, delete_after_upload=True):\n",
    "        self.base_model_id = base_model_id\n",
    "        self.hf_username = hf_username\n",
    "        self.delete_after_upload = delete_after_upload\n",
    "        self.api = HfApi()\n",
    "        \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Called when a checkpoint is saved.\n",
    "        Uploads the checkpoint to HF Hub with a timestamped name, then deletes local copy.\n",
    "        \"\"\"\n",
    "        # Get the checkpoint directory that was just saved\n",
    "        checkpoint_dir = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n",
    "        \n",
    "        if not Path(checkpoint_dir).exists():\n",
    "            print(f\"‚ö†Ô∏è Checkpoint directory not found: {checkpoint_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Create timestamped model ID\n",
    "        checkpoint_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_model_id = f\"{self.hf_username}/llama3-medical-ner-checkpoint-{state.global_step}-{checkpoint_timestamp}\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üì§ Uploading checkpoint to Hugging Face Hub\")\n",
    "        print(f\"   Step: {state.global_step}\")\n",
    "        print(f\"   Model ID: {checkpoint_model_id}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Create repository if it doesn't exist\n",
    "            try:\n",
    "                self.api.create_repo(\n",
    "                    repo_id=checkpoint_model_id,\n",
    "                    repo_type=\"model\",\n",
    "                    exist_ok=True,\n",
    "                    private=False\n",
    "                )\n",
    "                print(f\"‚úì Repository created/verified: {checkpoint_model_id}\")\n",
    "            except Exception as create_error:\n",
    "                print(f\"‚ö†Ô∏è  Repository creation note: {create_error}\")\n",
    "            \n",
    "            # Upload the checkpoint folder to HF Hub\n",
    "            self.api.upload_folder(\n",
    "                folder_path=checkpoint_dir,\n",
    "                repo_id=checkpoint_model_id,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=f\"Checkpoint at step {state.global_step}\",\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Checkpoint uploaded successfully!\")\n",
    "            print(f\"   URL: https://huggingface.co/{checkpoint_model_id}\")\n",
    "            \n",
    "            # Delete local checkpoint after successful upload\n",
    "            if self.delete_after_upload:\n",
    "                try:\n",
    "                    shutil.rmtree(checkpoint_dir)\n",
    "                    print(f\"üóëÔ∏è  Local checkpoint deleted to save disk space\")\n",
    "                except Exception as delete_error:\n",
    "                    print(f\"‚ö†Ô∏è  Could not delete local checkpoint: {delete_error}\")\n",
    "            \n",
    "            print()  # Empty line for readability\n",
    "            \n",
    "            # Log to wandb if available\n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\n",
    "                    \"checkpoint_step\": state.global_step,\n",
    "                    \"checkpoint_url\": f\"https://huggingface.co/{checkpoint_model_id}\"\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to upload checkpoint: {e}\")\n",
    "            print(f\"   Checkpoint saved locally at: {checkpoint_dir}\\n\")\n",
    "\n",
    "# Initialize the callback with automatic deletion after upload\n",
    "checkpoint_upload_callback = CheckpointUploadCallback(\n",
    "    base_model_id=HF_MODEL_ID,\n",
    "    hf_username=HF_USERNAME,\n",
    "    delete_after_upload=True  # Delete local checkpoints after upload\n",
    ")\n",
    "\n",
    "print(f\"‚úì Checkpoint upload callback initialized\")\n",
    "print(f\"  Checkpoints will be uploaded to: {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"  Upload frequency: Every {training_args.save_steps} steps\")\n",
    "print(f\"  Local deletion: Enabled (checkpoints deleted after upload)\")\n",
    "print(f\"  Auto-create repos: Enabled (repos will be created automatically)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cadccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHECKPOINT UPLOAD PREVIEW\n",
      "================================================================================\n",
      "\n",
      "Training Configuration:\n",
      "  Total samples: 2400\n",
      "  Batch size: 4\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 16\n",
      "  Epochs: 5\n",
      "  Estimated total steps: ~750\n",
      "\n",
      "Checkpoint Configuration:\n",
      "  Frequency: Every 50 steps\n",
      "  Expected checkpoints: ~15\n",
      "  Local storage: ./llama3-medical-ner-lora/checkpoint-<step>/\n",
      "\n",
      "Hugging Face Upload:\n",
      "  Format: albyos/llama3-medical-ner-checkpoint-<step>-<timestamp>\n",
      "\n",
      "Example checkpoint names:\n",
      "  1. albyos/llama3-medical-ner-checkpoint-50-20251111_075702\n",
      "  2. albyos/llama3-medical-ner-checkpoint-100-20251111_075702\n",
      "  3. albyos/llama3-medical-ner-checkpoint-150-20251111_075702\n",
      "  4. albyos/llama3-medical-ner-checkpoint-200-20251111_075702\n",
      "  ... (~11 more checkpoints)\n",
      "\n",
      "Final model:\n",
      "  albyos/llama3-medical-ner-lora-final-<timestamp>\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Preview expected checkpoint uploads\n",
    "total_steps_estimate = (len(train_data) // (BATCH_SIZE * GRADIENT_ACCUMULATION)) * NUM_EPOCHS\n",
    "checkpoint_count = total_steps_estimate // 50\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHECKPOINT UPLOAD PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Total samples: {len(train_data)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Estimated total steps: ~{total_steps_estimate}\")\n",
    "print(f\"\\nCheckpoint Configuration:\")\n",
    "print(f\"  Frequency: Every 50 steps\")\n",
    "print(f\"  Expected checkpoints: ~{checkpoint_count}\")\n",
    "print(f\"  Local storage: ./llama3-medical-ner-lora/checkpoint-<step>/\")\n",
    "print(f\"\\nHugging Face Upload:\")\n",
    "print(f\"  Format: {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"\\nExample checkpoint names:\")\n",
    "for i, step in enumerate(range(50, min(250, total_steps_estimate), 50), 1):\n",
    "    example_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"  {i}. {HF_USERNAME}/llama3-medical-ner-checkpoint-{step}-{example_time}\")\n",
    "if checkpoint_count > 4:\n",
    "    print(f\"  ... (~{checkpoint_count - 4} more checkpoints)\")\n",
    "    \n",
    "print(f\"\\nFinal model:\")\n",
    "print(f\"  {HF_USERNAME}/llama3-medical-ner-lora-final-<timestamp>\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a993c",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9b88564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Trainer initialized\n",
      "‚úì Expected training steps: ~750\n",
      "‚úì Expected checkpoints: ~15\n",
      "‚úì Checkpoint upload callback enabled\n",
      "‚úì Early stopping enabled (patience = 3 evaluations)\n",
      "\n",
      "üìã Checkpoint naming format:\n",
      "   albyos/llama3-medical-ner-checkpoint-<step>-<timestamp>\n",
      "\n",
      "   Example: albyos/llama3-medical-ner-checkpoint-50-20251104_143022\n"
     ]
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[checkpoint_upload_callback],  # Add custom checkpoint upload callback\n",
    ")\n",
    "\n",
    "# Configure early stopping to prevent overfitting\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0))\n",
    "\n",
    "# Calculate training steps\n",
    "total_steps = (len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n",
    "\n",
    "print(f\"‚úì Trainer initialized\")\n",
    "print(f\"‚úì Expected training steps: ~{total_steps}\")\n",
    "print(f\"‚úì Expected checkpoints: ~{max(1, total_steps // training_args.save_steps)}\")\n",
    "print(f\"‚úì Checkpoint upload callback enabled\")\n",
    "print(\"‚úì Early stopping enabled (patience = 3 evaluations)\")\n",
    "print(f\"\\nüìã Checkpoint naming format:\")\n",
    "print(f\"   {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"\\n   Example: {HF_USERNAME}/llama3-medical-ner-checkpoint-50-20251104_143022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b853b4",
   "metadata": {},
   "source": [
    "## 13. Start Training\n",
    "\n",
    "‚ö†Ô∏è **This will take 2-3 hours on an A100 GPU**\n",
    "\n",
    "The training will:\n",
    "- **Save checkpoints every 50 steps** to local disk\n",
    "- **Upload each checkpoint to Hugging Face Hub** with timestamped names\n",
    "  - Format: `{username}/llama3-medical-ner-checkpoint-{step}-{timestamp}`\n",
    "  - Example: `albyos/llama3-medical-ner-checkpoint-50-20251104_143022`\n",
    "- Evaluate on validation set every 50 steps\n",
    "- Save the best model based on validation loss\n",
    "- Log all metrics to Weights & Biases\n",
    "\n",
    "**Checkpoint URLs will be printed during training and logged to W&B.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39341273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "This may take 2-3 hours on A100 GPU...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 1:41:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.050200</td>\n",
       "      <td>0.980517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.968100</td>\n",
       "      <td>0.943931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.958300</td>\n",
       "      <td>0.911151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>0.871128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.852970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>0.833104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.804500</td>\n",
       "      <td>0.816857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>0.796536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.777900</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.709300</td>\n",
       "      <td>0.770044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.715100</td>\n",
       "      <td>0.756149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.727700</td>\n",
       "      <td>0.748497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.747142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.745828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.745545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 50\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-50-20251111_080408\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-50-20251111_080408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cfec8bdd344da6bccfe79e7ad0acc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ed87c6f7cd4f5ab55813152437a9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-50-20251111_080408\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 100\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-100-20251111_081049\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-100-20251111_081049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d04dd32176e450d8eec201c6f5c272e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2d4f93577140b08ed4c10094ad2da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-100-20251111_081049\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 150\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-150-20251111_081731\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-150-20251111_081731\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4b83ccfd6b43359ddff504c8e1c77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc92a191a0214ae385de4cec9d0ea021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-150-20251111_081731\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 200\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-200-20251111_082424\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-200-20251111_082424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76611db826d54dd6a6912b36edce6f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729718122c964552b57e384fc8c2bf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-200-20251111_082424\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 250\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-250-20251111_083109\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-250-20251111_083109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c58aaf1495e42bf88e8d6adb2c61e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2ec7685b764de7949ec232ae44b542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-250-20251111_083109\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 300\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-300-20251111_083749\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-300-20251111_083749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b365ec7593394a5ab90c89216eec3daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bb0ac3bc6547d4b6c508749d4baf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-300-20251111_083749\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 350\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-350-20251111_084433\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-350-20251111_084433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a29dd4ceec7494086d4f878d8f7d397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a08c75b20e4b119cbeb3d3a59bc5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-350-20251111_084433\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 400\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-400-20251111_085113\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-400-20251111_085113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20304c3b13444174a9e5a863043b0dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c15640e1321432c8b5b9eeb5a3b2511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-400-20251111_085113\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 450\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-450-20251111_085805\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-450-20251111_085805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8649cf73400646b79c1ccd9ba2e5e748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032827f09631485586d232e5292e1767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-450-20251111_085805\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 500\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-500-20251111_090449\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-500-20251111_090449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef7d215aa4344a0b502432088472b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9697cef50e947f7ad04bf058140e5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-500-20251111_090449\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 550\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-550-20251111_091135\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-550-20251111_091135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62249254d82744a4b019322a86a7f459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e563fac5c1f64d4e9804c68c8173f419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-550-20251111_091135\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 600\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-600-20251111_091823\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-600-20251111_091823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e96f5fce614ad08aa2700df2915fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33eadf8466b4fdca6d6c0923ff00f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-600-20251111_091823\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 650\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-650-20251111_092514\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-650-20251111_092514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8fb46caae944259380c823f8383ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1aeb7753d40474a81a2f0507f3d9f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-650-20251111_092514\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 700\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-700-20251111_093156\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-700-20251111_093156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5079b57605d1429c9f7a299fea62bf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f336b22d044ad9a66d474de6440f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-700-20251111_093156\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 750\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-750-20251111_093839\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-750-20251111_093839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de49b40c669c4d60be65557fa5d6d3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9164ee86116f48e99207aae9d998dfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./llama3-medical-ner-lora/checkpoint-750/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-750-20251111_093839\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.8618368581136068, metrics={'train_runtime': 6083.1931, 'train_samples_per_second': 1.973, 'train_steps_per_second': 0.123, 'total_flos': 1.4317469940110131e+17, 'train_loss': 0.8618368581136068, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take 2-3 hours on A100 GPU...\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed8e1e",
   "metadata": {},
   "source": [
    "## 14. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract losses\n",
    "train_loss = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "eval_loss = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_loss, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Evaluation Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training metrics plotted and saved to: training_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training steps: {len(train_loss)}\")\n",
    "print(f\"Final training loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {eval_loss[-1]:.4f}\")\n",
    "print(f\"Best validation loss: {min(eval_loss):.4f}\")\n",
    "print(f\"Loss reduction: {((train_loss[0] - train_loss[-1]) / train_loss[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdcba1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Training is complete! Your model has been saved.\n",
    "\n",
    "**To evaluate your model:**\n",
    "1. Open `Medical_NER_Evaluation.ipynb`\n",
    "2. Run the evaluation on the test set\n",
    "3. Test custom examples\n",
    "\n",
    "**Model locations:**\n",
    "- HuggingFace Hub: Check the output above for your model URL\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
