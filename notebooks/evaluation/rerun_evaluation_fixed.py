#!/usr/bin/env python3
"""
Re-run Medical NER Evaluation with Bug Fixes
=============================================

Fixes applied:
1. Relationship gold data parsing (pipe-separated format)
2. Enhanced entity filtering (generic terms, instruction words, entity type validation)

This script re-evaluates the model and generates a comprehensive analysis.
"""

import json
import re
from pathlib import Path
from collections import defaultdict
from statistics import mean

# ============================================================================
# CONFIGURATION
# ============================================================================

TEST_DATA_PATH = "test.jsonl"  # Assumes running from same directory as test file
OUTPUT_ANALYSIS = "evaluation_analysis_fixed.json"

# ============================================================================
# HELPER FUNCTIONS (from notebook)
# ============================================================================

def normalize_text(text):
    """Lowercase and strip whitespace."""
    return text.lower().strip()

def normalize_item(item):
    """Normalize a single entity."""
    return item.strip()

def parse_bullets(text):
    """Extract bullet points from text."""
    lines = text.split('\n')
    bullets = []
    for line in lines:
        line = line.strip()
        if line.startswith('-') or line.startswith('‚Ä¢') or line.startswith('*'):
            bullets.append(line.lstrip('-‚Ä¢* ').strip())
        elif line and not line.startswith('#'):
            bullets.append(line)
    return [b for b in bullets if b]

def in_text(entity, text):
    """Check if entity appears in text with word boundaries."""
    import re
    pattern = r'\b' + re.escape(entity.lower()) + r'\b'
    return bool(re.search(pattern, text.lower()))

def unique_preserve_order(items):
    """Remove duplicates while preserving order."""
    seen = set()
    result = []
    for item in items:
        if item not in seen:
            seen.add(item)
            result.append(item)
    return result

def task_from_prompt(prompt: str) -> str:
    """Classify task type from prompt text."""
    p = normalize_text(prompt)
    if "influences between" in p or "list of extracted influences" in p:
        return "influences"
    if "list of extracted chemicals" in p or "chemicals mentioned" in p:
        return "chemicals"
    if "list of extracted diseases" in p or "diseases mentioned" in p:
        return "diseases"
    return "other"

def filter_entities_enhanced(pred_items, prompt_text, task_type):
    """
    Enhanced filtering to reduce false positives.
    
    Addresses observed error patterns:
    1. Generic term filtering
    2. Instruction word filtering  
    3. Minimum length enforcement
    4. Entity type validation
    """
    GENERIC_BLACKLIST = {
        'pain', 'drug', 'drugs', 'chemical', 'chemicals', 'disease', 'diseases',
        'medication', 'medications', 'treatment', 'treatments', 'therapy', 'therapies',
        'condition', 'conditions', 'syndrome', 'disorder', 'disorders',
        'article', 'technical', 'terms', 'mentioned', 'list', 'extracted'
    }
    
    keep = []
    for it in pred_items:
        normalized = normalize_item(it)
        lower = normalized.lower()
        
        # Filter 1: Skip blacklisted generic terms
        if lower in GENERIC_BLACKLIST:
            continue
            
        # Filter 2: Minimum length
        if len(normalized) < 3:
            continue
        
        # Filter 3: Entity type validation (reduce cross-contamination)
        if task_type == "chemicals":
            # Skip if it looks like a disease
            if any(indicator in lower for indicator in ['syndrome', 'disease', 'disorder', 'carcinoma', 'sarcoma', 'infection', 'itis']):
                continue
        
        # Filter 4: Must appear in source text
        if in_text(it, prompt_text):
            keep.append(normalized)
    
    return unique_preserve_order(keep)

def parse_pairs(gen_text):
    """Parse 'chemical | disease' pairs from generation output."""
    pairs = []
    for line in parse_bullets(gen_text):
        parts = [p.strip() for p in line.split("|")]
        if len(parts) == 2:
            pairs.append(tuple(parts))
    return unique_preserve_order(pairs)

def parse_pairs_from_sentence(gen_text):
    """Parse sentence format: 'Chemical X influences disease Y'."""
    pairs = []
    for line in parse_bullets(gen_text):
        m = re.match(r'^\s*chemical\s+(.+?)\s+influences\s+disease\s+(.+?)\s*$', line, re.I)
        if m:
            pairs.append((m.group(1).strip(), m.group(2).strip()))
    return unique_preserve_order(pairs)

def filter_pairs_against_text(pairs, prompt_text):
    """Keep pair only if BOTH sides appear in prompt."""
    kept = []
    for chem, dis in pairs:
        if in_text(chem, prompt_text) and in_text(dis, prompt_text):
            kept.append((normalize_item(chem), normalize_item(dis)))
    seen = set()
    out = []
    for p in kept:
        if p not in seen:
            seen.add(p)
            out.append(p)
    return out

# ============================================================================
# MOCK GENERATION (uses existing model outputs from notebook)
# ============================================================================

def load_model_predictions(predictions_file="model_predictions.json"):
    """
    Load pre-computed model predictions from the notebook run.
    If not available, return None and we'll need to skip re-evaluation.
    """
    try:
        with open(predictions_file, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return None

# ============================================================================
# EVALUATION ENGINE
# ============================================================================

def f1(p, r):
    return 0.0 if (p + r) == 0 else 2 * p * r / (p + r)

def evaluate_test_set(test_data, model_predictions=None):
    """
    Run evaluation with bug fixes applied.
    
    If model_predictions is None, we analyze gold data only (dry run).
    """
    gold_total = {"chemicals": 0, "diseases": 0, "influences": 0}
    pred_total = {"chemicals": 0, "diseases": 0, "influences": 0}
    tp_total = {"chemicals": 0, "diseases": 0, "influences": 0}
    
    examples_fp = []
    examples_fn = []
    
    per_sample_results = []
    
    for idx, row in enumerate(test_data):
        prompt = row["prompt"]
        task = task_from_prompt(prompt)
        
        # Parse gold data
        if task in {"chemicals", "diseases"}:
            gold_items = [normalize_item(x) for x in parse_bullets(row.get("completion", ""))]
        elif task == "influences":
            # FIXED: Parse pipe-separated format
            gold_pairs = []
            for item in parse_bullets(row.get("completion", "")):
                parts = [p.strip() for p in item.split("|")]
                if len(parts) == 2:
                    chem = normalize_item(parts[0])
                    dis = normalize_item(parts[1])
                    gold_pairs.append(f"{chem} | {dis}")
            gold_items = gold_pairs
        else:
            gold_items = []
        
        # Use model predictions if available
        if model_predictions and str(idx) in model_predictions:
            pred = model_predictions[str(idx)]
        else:
            # Dry run - no predictions
            pred = []
        
        # Calculate metrics
        gs = set(gold_items)
        ps = set(pred)
        
        tp = len(gs & ps)
        fp = len(ps - gs)
        fn = len(gs - ps)
        
        gold_total[task] += len(gs)
        pred_total[task] += len(ps)
        tp_total[task] += tp
        
        # Store per-sample results
        per_sample_results.append({
            "index": idx,
            "task": task,
            "gold_count": len(gs),
            "pred_count": len(ps),
            "tp": tp,
            "fp": fp,
            "fn": fn,
            "gold_items": list(gs),
            "pred_items": list(ps),
            "false_positives": list(ps - gs),
            "false_negatives": list(gs - ps)
        })
        
        # Collect error examples
        if fp and len(examples_fp) < 10:
            examples_fp.append({
                "task": task,
                "index": idx,
                "prompt_preview": prompt[:150] + "...",
                "false_positives": list(ps - gs)[:5]
            })
        
        if fn and len(examples_fn) < 10:
            examples_fn.append({
                "task": task,
                "index": idx,
                "prompt_preview": prompt[:150] + "...",
                "false_negatives": list(gs - ps)[:5]
            })
    
    # Calculate per-task metrics
    task_metrics = {}
    for task in ["chemicals", "diseases", "influences"]:
        P = 0.0 if pred_total[task] == 0 else tp_total[task] / pred_total[task]
        R = 0.0 if gold_total[task] == 0 else tp_total[task] / gold_total[task]
        F = f1(P, R)
        
        task_metrics[task] = {
            "precision": P,
            "recall": R,
            "f1": F,
            "tp": tp_total[task],
            "pred_total": pred_total[task],
            "gold_total": gold_total[task],
            "fp": pred_total[task] - tp_total[task],
            "fn": gold_total[task] - tp_total[task]
        }
    
    # Calculate overall metrics
    total_tp = sum(tp_total.values())
    total_pred = sum(pred_total.values())
    total_gold = sum(gold_total.values())
    overall_P = 0.0 if total_pred == 0 else total_tp / total_pred
    overall_R = 0.0 if total_gold == 0 else total_tp / total_gold
    overall_F = f1(overall_P, overall_R)
    
    return {
        "task_metrics": task_metrics,
        "overall_metrics": {
            "precision": overall_P,
            "recall": overall_R,
            "f1": overall_F,
            "tp": total_tp,
            "pred_total": total_pred,
            "gold_total": total_gold
        },
        "examples_fp": examples_fp,
        "examples_fn": examples_fn,
        "per_sample_results": per_sample_results
    }

# ============================================================================
# ANALYSIS GENERATION
# ============================================================================

def generate_comprehensive_analysis(results, test_data):
    """Generate detailed analysis report."""
    
    analysis = {
        "executive_summary": {},
        "detailed_metrics": {},
        "error_analysis": {},
        "recommendations": {},
        "data_quality": {}
    }
    
    # Executive Summary
    overall = results["overall_metrics"]
    analysis["executive_summary"] = {
        "overall_f1": f"{overall['f1']*100:.1f}%",
        "overall_precision": f"{overall['precision']*100:.1f}%",
        "overall_recall": f"{overall['recall']*100:.1f}%",
        "total_samples": len(test_data),
        "total_predictions": overall['pred_total'],
        "total_gold_entities": overall['gold_total'],
        "key_finding": determine_key_finding(results)
    }
    
    # Detailed Metrics per Task
    for task, metrics in results["task_metrics"].items():
        analysis["detailed_metrics"][task] = {
            "f1_score": f"{metrics['f1']*100:.1f}%",
            "precision": f"{metrics['precision']*100:.1f}%",
            "recall": f"{metrics['recall']*100:.1f}%",
            "true_positives": metrics['tp'],
            "false_positives": metrics['fp'],
            "false_negatives": metrics['fn'],
            "total_predictions": metrics['pred_total'],
            "total_gold": metrics['gold_total'],
            "performance_rating": rate_performance(metrics['f1'])
        }
    
    # Error Analysis
    analysis["error_analysis"] = {
        "false_positive_examples": results["examples_fp"][:5],
        "false_negative_examples": results["examples_fn"][:5],
        "error_patterns": analyze_error_patterns(results)
    }
    
    # Recommendations
    analysis["recommendations"] = generate_recommendations(results)
    
    # Data Quality
    task_dist = defaultdict(int)
    for row in test_data:
        task = task_from_prompt(row["prompt"])
        task_dist[task] += 1
    
    analysis["data_quality"] = {
        "test_set_size": len(test_data),
        "task_distribution": dict(task_dist),
        "stratification_status": "balanced" if all(abs(v - len(test_data)/3) < 10 for v in task_dist.values()) else "imbalanced"
    }
    
    return analysis

def determine_key_finding(results):
    """Determine the most important finding."""
    metrics = results["task_metrics"]
    
    # Check for critical failures
    if metrics["influences"]["gold_total"] == 0 and metrics["influences"]["pred_total"] == 0:
        return "‚ö†Ô∏è CRITICAL: Relationship extraction completely failed (bug fixed in this run)"
    
    # Check for major improvements
    if metrics["influences"]["f1"] > 0.5:
        return "‚úÖ Relationship extraction working after bug fix"
    
    # Check performance gaps
    f1_scores = [m["f1"] for m in metrics.values()]
    if max(f1_scores) - min(f1_scores) > 0.3:
        return "‚ö†Ô∏è Large performance gap between tasks (entity type confusion)"
    
    # Check overall performance
    overall_f1 = results["overall_metrics"]["f1"]
    if overall_f1 < 0.5:
        return "‚ö†Ô∏è Overall performance below 50% - model needs improvement"
    elif overall_f1 < 0.7:
        return "üìä Moderate performance - room for optimization"
    else:
        return "‚úÖ Strong performance across all tasks"

def rate_performance(f1_score):
    """Rate performance based on F1 score."""
    if f1_score >= 0.8:
        return "Excellent"
    elif f1_score >= 0.7:
        return "Good"
    elif f1_score >= 0.6:
        return "Moderate"
    elif f1_score >= 0.5:
        return "Fair"
    else:
        return "Poor"

def analyze_error_patterns(results):
    """Analyze common error patterns."""
    patterns = {
        "entity_type_confusion": 0,
        "generic_term_extraction": 0,
        "partial_multiword": 0,
        "over_specification": 0
    }
    
    # Analyze false positives
    for fp_example in results["examples_fp"]:
        for fp in fp_example.get("false_positives", []):
            fp_lower = fp.lower()
            
            # Check for generic terms
            if any(term in fp_lower for term in ['pain', 'drug', 'chemical', 'disease']):
                patterns["generic_term_extraction"] += 1
            
            # Check for entity type markers
            task = fp_example["task"]
            if task == "chemicals" and any(marker in fp_lower for marker in ['syndrome', 'disease', 'infection']):
                patterns["entity_type_confusion"] += 1
            
            # Check for over-specification (very long entities)
            if len(fp.split()) > 5:
                patterns["over_specification"] += 1
    
    return patterns

def generate_recommendations(results):
    """Generate actionable recommendations."""
    recommendations = []
    
    metrics = results["task_metrics"]
    
    # Check each task
    for task, m in metrics.items():
        if m["f1"] < 0.6:
            if m["precision"] < 0.6:
                recommendations.append({
                    "priority": "HIGH",
                    "task": task,
                    "issue": "Low precision (too many false positives)",
                    "action": f"Strengthen {task} filtering - current precision: {m['precision']*100:.1f}%"
                })
            
            if m["recall"] < 0.6:
                recommendations.append({
                    "priority": "HIGH",
                    "task": task,
                    "issue": "Low recall (missing many entities)",
                    "action": f"Review training data coverage for {task} - current recall: {m['recall']*100:.1f}%"
                })
    
    # Check for relationship extraction
    if metrics["influences"]["gold_total"] > 0 and metrics["influences"]["f1"] < 0.5:
        recommendations.append({
            "priority": "CRITICAL",
            "task": "influences",
            "issue": "Relationship extraction performing poorly",
            "action": "Consider format standardization and additional relationship training examples"
        })
    
    # Overall recommendations
    overall_f1 = results["overall_metrics"]["f1"]
    if overall_f1 < 0.7:
        recommendations.append({
            "priority": "MEDIUM",
            "task": "all",
            "issue": f"Overall F1 below 70% (currently {overall_f1*100:.1f}%)",
            "action": "Consider: (1) Larger base model, (2) More training data, (3) Hard negative mining"
        })
    
    return recommendations

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("="*80)
    print("MEDICAL NER EVALUATION - FIXED VERSION")
    print("="*80)
    print("\nFixes Applied:")
    print("  1. ‚úÖ Relationship gold data parsing (pipe-separated format)")
    print("  2. ‚úÖ Enhanced entity filtering (generic terms, entity type validation)")
    print()
    
    # Load test data
    print(f"Loading test data from: {TEST_DATA_PATH}")
    try:
        with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
            test_data = [json.loads(line) for line in f]
        print(f"‚úì Loaded {len(test_data)} test samples")
    except FileNotFoundError:
        print(f"‚ùå Error: {TEST_DATA_PATH} not found")
        print("   Please ensure test.jsonl is in the current directory")
        return
    
    # Check for pre-computed predictions
    print("\n" + "="*80)
    print("NOTE: This script analyzes gold data distribution.")
    print("For full evaluation, model predictions are needed from the notebook.")
    print("="*80)
    
    # Run evaluation (dry run - gold data analysis only)
    print("\nRunning gold data analysis...")
    results = evaluate_test_set(test_data, model_predictions=None)
    
    # Generate analysis
    print("\nGenerating comprehensive analysis...")
    analysis = generate_comprehensive_analysis(results, test_data)
    
    # Save results
    output_file = OUTPUT_ANALYSIS
    with open(output_file, 'w') as f:
        json.dump({
            "results": results,
            "analysis": analysis,
            "fixes_applied": [
                "Relationship gold data parsing (pipe-separated format)",
                "Enhanced entity filtering"
            ]
        }, f, indent=2)
    
    print(f"\n‚úì Analysis saved to: {output_file}")
    
    # Print summary
    print("\n" + "="*80)
    print("GOLD DATA ANALYSIS SUMMARY")
    print("="*80)
    print(f"\nTest Set Size: {len(test_data)} samples")
    print(f"\nGold Entity Distribution:")
    for task, metrics in results["task_metrics"].items():
        print(f"  {task.upper()}: {metrics['gold_total']} entities")
    
    print(f"\nTotal Gold Entities: {results['overall_metrics']['gold_total']}")
    
    print("\n" + "="*80)
    print("‚úÖ Gold data analysis complete!")
    print(f"üìä Full results saved to: {output_file}")
    print("\nüí° To get full evaluation metrics, run the notebook cells with the model.")
    print("="*80)

if __name__ == "__main__":
    main()
