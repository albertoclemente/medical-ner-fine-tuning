{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4913c85c",
   "metadata": {},
   "source": [
    "## üöÄ RunPod Quick Start\n",
    "\n",
    "### Prerequisites:\n",
    "1. **GPU Pod**: RTX 3090/4090, A5000, or A6000 recommended\n",
    "2. **VRAM**: Minimum 12GB (24GB recommended for optimal performance)\n",
    "3. **Test Data**: Upload to `/workspace/data/splits_cleaned_20251113/test.jsonl`\n",
    "\n",
    "### Execution Steps:\n",
    "1. Upload this notebook to RunPod pod\n",
    "2. Upload test data file\n",
    "3. Set HuggingFace token (Cell 1)\n",
    "4. Run all cells sequentially\n",
    "5. Review comprehensive analysis at the end\n",
    "\n",
    "### Expected Runtime:\n",
    "- **GPU**: RTX 4090 ‚Üí ~3-5 minutes (299 samples)\n",
    "- **GPU**: RTX 3090 ‚Üí ~5-8 minutes\n",
    "- **GPU**: A5000 ‚Üí ~8-12 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f49c48",
   "metadata": {},
   "source": [
    "## üìä Dataset Information\n",
    "\n",
    "**Using Cleaned Dataset**: `splits_cleaned_20251113/`\n",
    "\n",
    "### Quality Improvements:\n",
    "- ‚úÖ **0 empty completions** (removed 6 invalid samples)\n",
    "- ‚úÖ **0 prompts >2048 chars** (intelligently truncated 307)\n",
    "- ‚úÖ **99.8% retention** (2,994 of 3,000 samples)\n",
    "- ‚úÖ **Perfect stratification** (33.3% per task)\n",
    "\n",
    "### Test Set Details:\n",
    "- **Total samples**: 299\n",
    "- **Chemicals**: 100 samples (33.4%)\n",
    "- **Diseases**: 99 samples (33.1%)\n",
    "- **Relationships**: 100 samples (33.4%)\n",
    "\n",
    "### Expected Performance:\n",
    "- **Target F1**: 70-80% (medical domain model on clean data)\n",
    "- **Previous baseline** (Llama-3.2-3B): ~54% F1\n",
    "- **Key improvement**: Relationship extraction (was 0%, now measurable)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80fe598",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Environment Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your HuggingFace token before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# HuggingFace Token (required for model downloads)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"HF_TOKEN not found in environment variables\")\n",
    "    hf_token = getpass(\"Enter your HuggingFace token: \")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "else:\n",
    "    print(\"‚úì HF_TOKEN loaded from environment\")\n",
    "\n",
    "# Enable fast transfers\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "print(\"\\n‚úì Environment configured for RunPod\")\n",
    "print(f\"  HF_HUB_ENABLE_HF_TRANSFER: {os.getenv('HF_HUB_ENABLE_HF_TRANSFER')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dfe2e",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies\n",
    "\n",
    "Install required packages for AWQ quantized model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24af24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AutoAWQ for quantized model support\n",
    "!pip install -q autoawq transformers accelerate\n",
    "!pip install -q huggingface-hub hf-transfer\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")\n",
    "print(\"  - autoawq (AWQ quantization support)\")\n",
    "print(\"  - transformers (HuggingFace models)\")\n",
    "print(\"  - accelerate (device management)\")\n",
    "print(\"  - hf-transfer (fast downloads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095f6b8",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import Libraries & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1506083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from pathlib import Path\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüöÄ GPU Detected:\")\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No GPU detected! This will be very slow.\")\n",
    "    print(\"   Please ensure you're running on a RunPod GPU pod.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f2cfc",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configuration\n",
    "\n",
    "Set model and data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "MODEL_ID = \"BioMistral/BioMistral-7B-SLERP-AWQ-QGS128-W4-GEMM\"\n",
    "\n",
    "# Data Configuration (RunPod paths)\n",
    "TEST_DATA_PATH = \"/workspace/data/splits_cleaned_20251113/test.jsonl\"\n",
    "\n",
    "# Alternative local path (if running locally)\n",
    "# TEST_DATA_PATH = \"../../data/splits_cleaned_20251113/test.jsonl\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {MODEL_ID}\")\n",
    "print(f\"  Type: AWQ Quantized (4-bit)\")\n",
    "print(f\"  Base: BioMistral-7B-SLERP\")\n",
    "print(f\"  Optimization: QGS128-W4-GEMM (NVIDIA GPU optimized)\")\n",
    "print(f\"\\nTest Data: {TEST_DATA_PATH}\")\n",
    "print(f\"  Dataset: Cleaned (splits_cleaned_20251113)\")\n",
    "print(f\"  Quality: 99.8% retention, 0 issues\")\n",
    "print(f\"  Expected samples: 299 (100 chemicals, 99 diseases, 100 relationships)\")\n",
    "\n",
    "# Verify test data exists\n",
    "if Path(TEST_DATA_PATH).exists():\n",
    "    print(f\"\\n‚úì Test data file found\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Test data file not found at {TEST_DATA_PATH}\")\n",
    "    print(\"   Please upload test.jsonl to the correct location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867591f",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Utility Functions\n",
    "\n",
    "Reusable functions for text processing, parsing, and filtering (from previous evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a11a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Text Normalization =====\n",
    "def dehyphenate(s: str) -> str:\n",
    "    \"\"\"Join words broken across lines with hyphens.\"\"\"\n",
    "    return re.sub(r\"(\\w+)-\\s+(\\w+)\", r\"\\1\\2\", s)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize text for consistent comparison.\"\"\"\n",
    "    s = dehyphenate(s or \"\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\u00A0\\t\\r\\n]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_item(item: str) -> str:\n",
    "    \"\"\"Normalize entity: lowercase, strip quotes/whitespace.\"\"\"\n",
    "    item = item.strip().lower()\n",
    "    item = re.sub(r'^[\"\\']|[\"\\']$', '', item)\n",
    "    item = re.sub(r'\\s+', ' ', item).strip()\n",
    "    return item\n",
    "\n",
    "# ===== Parsing Functions =====\n",
    "def parse_bullets(text: str):\n",
    "    \"\"\"Extract items from bullet list (- or *).\"\"\"\n",
    "    items = []\n",
    "    for line in (text or \"\").splitlines():\n",
    "        m = re.match(r\"^\\s*[-*]\\s*(.+?)\\s*$\", line)\n",
    "        if m:\n",
    "            items.append(m.group(1))\n",
    "    return items\n",
    "\n",
    "def extract_list_from_generation(gen: str):\n",
    "    \"\"\"Extract list items from model generation.\"\"\"\n",
    "    items = []\n",
    "    for line in gen.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        m = re.match(r\"^[-*]\\s*(.+)$\", line)\n",
    "        if m:\n",
    "            items.append(m.group(1).strip())\n",
    "    return items\n",
    "\n",
    "def task_from_prompt(prompt: str) -> str:\n",
    "    \"\"\"Determine task type from prompt text.\"\"\"\n",
    "    prompt_lower = prompt.lower()\n",
    "    if \"list only of the chemicals\" in prompt_lower:\n",
    "        return \"chemicals\"\n",
    "    elif \"list only of the diseases\" in prompt_lower:\n",
    "        return \"diseases\"\n",
    "    elif \"list only of the influences\" in prompt_lower:\n",
    "        return \"influences\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# ===== Relationship Parsing =====\n",
    "def parse_pairs(text: str):\n",
    "    \"\"\"Parse pipe-separated relationships: chemical | disease.\"\"\"\n",
    "    pairs = []\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if '|' in line:\n",
    "            line = re.sub(r'^[-*]\\s*', '', line)\n",
    "            parts = [p.strip() for p in line.split('|')]\n",
    "            if len(parts) == 2:\n",
    "                pairs.append((parts[0], parts[1]))\n",
    "    return pairs\n",
    "\n",
    "def parse_pairs_from_sentence(text: str):\n",
    "    \"\"\"Parse sentence format: chemical X influences disease Y.\"\"\"\n",
    "    pairs = []\n",
    "    for line in text.splitlines():\n",
    "        m = re.search(r'chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+)', line, re.I)\n",
    "        if m:\n",
    "            pairs.append((m.group(1).strip(), m.group(2).strip()))\n",
    "    return pairs\n",
    "\n",
    "# ===== Enhanced Filtering (Reduces False Positives) =====\n",
    "def filter_entities_enhanced(pred_items, prompt_text, task_type):\n",
    "    \"\"\"\n",
    "    Enhanced filtering to reduce false positives.\n",
    "    Filters out: generic terms, instruction words, entity confusion, short fragments.\n",
    "    \"\"\"\n",
    "    GENERIC_BLACKLIST = {\n",
    "        'pain', 'drug', 'drugs', 'chemical', 'chemicals',\n",
    "        'disease', 'diseases', 'medication', 'medications',\n",
    "        'treatment', 'treatments', 'therapy', 'article',\n",
    "        'mentioned', 'list', 'extracted', 'following'\n",
    "    }\n",
    "    \n",
    "    # Disease markers (shouldn't be in chemicals)\n",
    "    DISEASE_MARKERS = {'syndrome', 'disease', 'disorder', 'infection', 'itis', 'osis'}\n",
    "    \n",
    "    prompt_lower = prompt_text.lower()\n",
    "    filtered = []\n",
    "    \n",
    "    for item in pred_items:\n",
    "        item_norm = normalize_item(item)\n",
    "        \n",
    "        # Skip empty or very short\n",
    "        if len(item_norm) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Skip generic terms\n",
    "        if item_norm in GENERIC_BLACKLIST:\n",
    "            continue\n",
    "        \n",
    "        # Skip if not in prompt text\n",
    "        if item_norm not in prompt_lower:\n",
    "            continue\n",
    "        \n",
    "        # Entity type validation for chemicals\n",
    "        if task_type == \"chemicals\":\n",
    "            # Skip if it has disease markers\n",
    "            if any(marker in item_norm for marker in DISEASE_MARKERS):\n",
    "                continue\n",
    "        \n",
    "        filtered.append(item_norm)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def filter_pairs_against_text(pairs, prompt_text):\n",
    "    \"\"\"Filter relationship pairs to only those mentioned in text.\"\"\"\n",
    "    prompt_lower = prompt_text.lower()\n",
    "    filtered = []\n",
    "    for (c, d) in pairs:\n",
    "        c_norm = normalize_item(c)\n",
    "        d_norm = normalize_item(d)\n",
    "        if c_norm in prompt_lower and d_norm in prompt_lower:\n",
    "            filtered.append((c_norm, d_norm))\n",
    "    return filtered\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")\n",
    "print(\"  - Text normalization\")\n",
    "print(\"  - Bullet list parsing\")\n",
    "print(\"  - Task identification\")\n",
    "print(\"  - Relationship parsing (pipe + sentence formats)\")\n",
    "print(\"  - Enhanced filtering (reduces false positives)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabefc75",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load Model & Tokenizer\n",
    "\n",
    "Load the AWQ quantized BioMistral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token, add_to_git_credential=True)\n",
    "    print(\"‚úì Logged into HuggingFace Hub\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING AWQ QUANTIZED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_ID}\")\n",
    "print(\"Loading tokenizer...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "print(\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Load AWQ quantized model\n",
    "print(\"\\nLoading AWQ quantized model (4-bit)...\")\n",
    "print(\"  This may take 1-2 minutes on first run (caching model)\")\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    MODEL_ID,\n",
    "    fuse_layers=True,  # Enable layer fusion for better performance\n",
    "    device_map=\"auto\",  # Automatically distribute across GPUs\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Model loaded successfully!\")\n",
    "print(f\"  Model: {MODEL_ID}\")\n",
    "print(f\"  Quantization: AWQ 4-bit\")\n",
    "print(f\"  Device: {model.device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Memory Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  GPU Memory Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72adc7",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Generation Function\n",
    "\n",
    "Deterministic generation optimized for BioMistral (Mistral chat format).\n",
    "\n",
    "**Note**: Uses the exact prompt format from the training data without adding system instructions. This ensures consistency between baseline evaluation and future fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7276d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt_text, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generate response using BioMistral with Mistral chat format.\n",
    "    Uses deterministic generation (greedy decoding) for reproducibility.\n",
    "    \n",
    "    IMPORTANT: Uses EXACT prompt format from training data (no system instruction added)\n",
    "    to ensure consistency between baseline evaluation and fine-tuning.\n",
    "    \"\"\"\n",
    "    # Mistral chat format - use raw prompt from dataset without modification\n",
    "    formatted_prompt = f\"\"\"<s>[INST] {prompt_text} [/INST]\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy decoding (deterministic)\n",
    "            temperature=1.0,  # Not used with do_sample=False\n",
    "            top_p=1.0,  # Not used with do_sample=False\n",
    "            num_beams=1,  # No beam search\n",
    "            repetition_penalty=1.15,  # Slight penalty\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,  # Enable KV cache\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant's response (after [/INST])\n",
    "    if \"[/INST]\" in response:\n",
    "        response = response.split(\"[/INST]\")[-1]\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"‚úì Generation function ready\")\n",
    "print(\"  Format: Mistral chat format (<s>[INST] ... [/INST])\")\n",
    "print(\"  Prompt: Uses exact dataset format (no added system instruction)\")\n",
    "print(\"  Mode: Deterministic (greedy decoding)\")\n",
    "print(\"  Max tokens: 128 (optimal for NER)\")\n",
    "print(\"  KV cache: Enabled (faster inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca949a1",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Run Evaluation\n",
    "\n",
    "Evaluate model on test set with per-task metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d78ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def f1(p, r):\n",
    "    \"\"\"Calculate F1 score from precision and recall.\"\"\"\n",
    "    return 0.0 if (p + r) == 0 else 2 * p * r / (p + r)\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"‚úì Loaded test set: {len(test_data)} samples\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  - Deterministic generation (greedy decoding)\")\n",
    "print(\"  - Enhanced filtering (reduces false positives)\")\n",
    "print(\"  - Task-specific processing\")\n",
    "print(\"\\nProcessing samples...\\n\")\n",
    "\n",
    "# Initialize per-task counters\n",
    "gold_total = {\"chemicals\": 0, \"diseases\": 0, \"influences\": 0}\n",
    "pred_total = {\"chemicals\": 0, \"diseases\": 0, \"influences\": 0}\n",
    "tp_total = {\"chemicals\": 0, \"diseases\": 0, \"influences\": 0}\n",
    "\n",
    "examples_fp = []  # False positives\n",
    "examples_fn = []  # False negatives\n",
    "\n",
    "# Process each test sample\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in enumerate(test_data):\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = elapsed / (idx + 1)\n",
    "        remaining = rate * (len(test_data) - idx - 1)\n",
    "        print(f\"  Progress: {idx + 1}/{len(test_data)} samples ({elapsed:.1f}s elapsed, ~{remaining:.1f}s remaining)\")\n",
    "    \n",
    "    prompt = row[\"prompt\"]\n",
    "    gold_items = [normalize_item(x) for x in parse_bullets(row.get(\"completion\", \"\"))]\n",
    "    task = task_from_prompt(prompt)\n",
    "    \n",
    "    # Generate prediction\n",
    "    gen = generate_response(prompt, max_new_tokens=128)\n",
    "    pred_raw = extract_list_from_generation(gen)\n",
    "    \n",
    "    # Apply task-specific processing\n",
    "    if task in {\"chemicals\", \"diseases\"}:\n",
    "        # Use enhanced filtering\n",
    "        pred = filter_entities_enhanced(pred_raw, prompt, task)\n",
    "    elif task == \"influences\":\n",
    "        # Parse gold data (pipe-separated format)\n",
    "        gold_pairs = []\n",
    "        for item in parse_bullets(row.get(\"completion\", \"\")):\n",
    "            parts = [p.strip() for p in item.split(\"|\")]\n",
    "            if len(parts) == 2:\n",
    "                chem = normalize_item(parts[0])\n",
    "                dis = normalize_item(parts[1])\n",
    "                gold_pairs.append(f\"{chem} | {dis}\")\n",
    "        gold_items = gold_pairs\n",
    "        \n",
    "        # Parse model output (try both formats)\n",
    "        pairs_sentence = parse_pairs_from_sentence(gen)\n",
    "        pairs_pipe = parse_pairs(gen)\n",
    "        all_pairs = pairs_sentence if pairs_sentence else pairs_pipe\n",
    "        \n",
    "        # Normalize and filter\n",
    "        pred = [f\"{normalize_item(c)} | {normalize_item(d)}\"\n",
    "                for (c, d) in filter_pairs_against_text(all_pairs, prompt)]\n",
    "    else:\n",
    "        pred = []\n",
    "    \n",
    "    # Calculate metrics\n",
    "    gs = set(gold_items)\n",
    "    ps = set(pred)\n",
    "    \n",
    "    tp = len(gs & ps)\n",
    "    fp = len(ps - gs)\n",
    "    fn = len(gs - ps)\n",
    "    \n",
    "    gold_total[task] += len(gs)\n",
    "    pred_total[task] += len(ps)\n",
    "    tp_total[task] += tp\n",
    "    \n",
    "    # Collect examples\n",
    "    if fp and len(examples_fp) < 8:\n",
    "        examples_fp.append({\n",
    "            \"task\": task,\n",
    "            \"prompt_preview\": prompt[:120] + \"...\",\n",
    "            \"pred_extras\": list(ps - gs)[:5]\n",
    "        })\n",
    "    if fn and len(examples_fn) < 8:\n",
    "        examples_fn.append({\n",
    "            \"task\": task,\n",
    "            \"prompt_preview\": prompt[:120] + \"...\",\n",
    "            \"missed\": list(gs - ps)[:5]\n",
    "        })\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Evaluation complete!\")\n",
    "print(f\"  Total time: {total_time:.1f}s\")\n",
    "print(f\"  Average: {total_time / len(test_data):.2f}s per sample\")\n",
    "print(f\"  Throughput: {len(test_data) / total_time:.1f} samples/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb26ae",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Results & Analysis\n",
    "\n",
    "Display comprehensive per-task metrics and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS - BioMistral-7B-SLERP-AWQ\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Calculate and display per-task metrics\n",
    "all_metrics = []\n",
    "\n",
    "for task in [\"chemicals\", \"diseases\", \"influences\"]:\n",
    "    P = 0.0 if pred_total[task] == 0 else tp_total[task] / pred_total[task]\n",
    "    R = 0.0 if gold_total[task] == 0 else tp_total[task] / gold_total[task]\n",
    "    F1 = f1(P, R)\n",
    "    \n",
    "    all_metrics.append({\"task\": task, \"P\": P, \"R\": R, \"F1\": F1})\n",
    "    \n",
    "    print(f\"üìä {task.upper()}\")\n",
    "    print(f\"   Precision: {P*100:.1f}%\")\n",
    "    print(f\"   Recall:    {R*100:.1f}%\")\n",
    "    print(f\"   F1 Score:  {F1*100:.1f}%\")\n",
    "    print(f\"   True Positives:  {tp_total[task]}\")\n",
    "    print(f\"   Gold Standard:   {gold_total[task]}\")\n",
    "    print(f\"   Predictions:     {pred_total[task]}\")\n",
    "    print()\n",
    "\n",
    "# Overall metrics (macro-average)\n",
    "overall_P = mean([m[\"P\"] for m in all_metrics])\n",
    "overall_R = mean([m[\"R\"] for m in all_metrics])\n",
    "overall_F1 = mean([m[\"F1\"] for m in all_metrics])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìà OVERALL PERFORMANCE (Macro-Average)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Precision: {overall_P*100:.1f}%\")\n",
    "print(f\"Recall:    {overall_R*100:.1f}%\")\n",
    "print(f\"F1 Score:  {overall_F1*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Comparison with baseline\n",
    "print(\"=\"*80)\n",
    "print(\"üìä COMPARISON WITH BASELINE\")\n",
    "print(\"=\"*80)\n",
    "baseline_f1 = 53.8  # From previous Llama-3.2-3B evaluation\n",
    "improvement = overall_F1 * 100 - baseline_f1\n",
    "\n",
    "print(f\"Llama-3.2-3B Baseline: {baseline_f1:.1f}% F1\")\n",
    "print(f\"BioMistral-7B-AWQ:     {overall_F1*100:.1f}% F1\")\n",
    "print(f\"Improvement:           {improvement:+.1f} points\")\n",
    "print()\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"‚úÖ BioMistral shows {improvement:.1f} point improvement!\")\n",
    "elif improvement > -5:\n",
    "    print(f\"‚ö†Ô∏è  Performance similar to baseline ({improvement:+.1f} points)\")\n",
    "else:\n",
    "    print(f\"‚ùå Performance below baseline ({improvement:+.1f} points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8febde1",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Error Analysis\n",
    "\n",
    "Examine false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç ERROR ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# False Positives\n",
    "if examples_fp:\n",
    "    print(\"‚ùå FALSE POSITIVES (Predicted but not in gold standard)\\n\")\n",
    "    for i, ex in enumerate(examples_fp[:5], 1):\n",
    "        print(f\"{i}. Task: {ex['task']}\")\n",
    "        print(f\"   Text: {ex['prompt_preview']}\")\n",
    "        print(f\"   Extra predictions: {ex['pred_extras']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚úì No false positives collected\\n\")\n",
    "\n",
    "# False Negatives\n",
    "if examples_fn:\n",
    "    print(\"‚ö†Ô∏è  FALSE NEGATIVES (In gold standard but missed by model)\\n\")\n",
    "    for i, ex in enumerate(examples_fn[:5], 1):\n",
    "        print(f\"{i}. Task: {ex['task']}\")\n",
    "        print(f\"   Text: {ex['prompt_preview']}\")\n",
    "        print(f\"   Missed entities: {ex['missed']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚úì No false negatives collected\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ccead",
   "metadata": {},
   "source": [
    "## üîü Test Sample Examples\n",
    "\n",
    "Run model on a few test samples to see actual outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Show 3 examples (one per task)\n",
    "task_examples = {}\n",
    "for row in test_data:\n",
    "    task = task_from_prompt(row[\"prompt\"])\n",
    "    if task not in task_examples:\n",
    "        task_examples[task] = row\n",
    "    if len(task_examples) == 3:\n",
    "        break\n",
    "\n",
    "for task in [\"chemicals\", \"diseases\", \"influences\"]:\n",
    "    if task in task_examples:\n",
    "        row = task_examples[task]\n",
    "        prompt = row[\"prompt\"]\n",
    "        gold = row[\"completion\"]\n",
    "        \n",
    "        # Generate prediction\n",
    "        pred = generate_response(prompt, max_new_tokens=128)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TASK: {task.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nPROMPT:\\n{prompt[:300]}...\")\n",
    "        print(f\"\\nGOLD STANDARD:\\n{gold}\")\n",
    "        print(f\"\\nMODEL PREDICTION:\\n{pred}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124cc25",
   "metadata": {},
   "source": [
    "## üìã Summary & Conclusions\n",
    "\n",
    "**Model**: BioMistral-7B-SLERP-AWQ-QGS128-W4-GEMM  \n",
    "**Dataset**: Cleaned splits_cleaned_20251113 (299 test samples)  \n",
    "**Date**: November 15, 2025\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Medical Domain Advantage**: Evaluate whether BioMistral's medical pretraining improves performance\n",
    "2. **Quantization Impact**: AWQ 4-bit quantization maintains accuracy while reducing memory\n",
    "3. **Clean Data Effect**: Using optimized dataset (99.8% retention, 0 issues)\n",
    "4. **Inference Speed**: AWQ quantization provides faster inference on RunPod GPUs\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- [ ] Compare with Llama-3.2-3B baseline\n",
    "- [ ] Fine-tune BioMistral on medical NER data\n",
    "- [ ] Test with different quantization levels\n",
    "- [ ] Optimize prompt format for BioMistral\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook**: `BioMistral_7B_AWQ_Evaluation_20251115.ipynb`  \n",
    "**Related Issue**: #2 - Retrain with BioMistral-7B-SLERP"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
