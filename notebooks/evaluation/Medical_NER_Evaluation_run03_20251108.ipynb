{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15813e67",
   "metadata": {},
   "source": [
    "# üöÄ RunPod GPU Setup\n",
    "\n",
    "**This notebook is optimized for RunPod GPU pods with NVIDIA GPUs**\n",
    "\n",
    "## Quick Start on RunPod:\n",
    "\n",
    "1. **Launch a GPU Pod** (RTX 3090, 4090, or A5000 recommended)\n",
    "2. **Upload this notebook** to the pod\n",
    "3. **Upload test data** (`test_run_20251106.jsonl`) to `/workspace/data/`\n",
    "4. **Run cells in order** - evaluation should complete in ~5-10 minutes\n",
    "\n",
    "## Expected Performance:\n",
    "- **GPU**: RTX 3090/4090 ‚Üí ~0.5-1 sec/sample (~5 min total)\n",
    "- **GPU**: RTX A5000 ‚Üí ~1-2 sec/sample (~10 min total)\n",
    "- **Full evaluation**: 300 samples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4828c",
   "metadata": {},
   "source": [
    "# Medical NER Model Evaluation\n",
    "\n",
    "This notebook evaluates the fine-tuned Llama 3.2 3B medical NER model.\n",
    "\n",
    "## ‚úÖ DATASET VERIFIED & READY FOR EVALUATION\n",
    "\n",
    "**Current Dataset Distribution** (from `both_rel_instruct_all.jsonl`):\n",
    "- **1,000 Chemical extraction** examples (33.3%)\n",
    "- **1,000 Disease extraction** examples (33.3%)\n",
    "- **1,000 Relationship extraction** examples (33.3%)\n",
    "\n",
    "**Data Splits Status**: ‚úÖ Properly stratified using `stratify=` parameter\n",
    "- Training (2,400): 33.3% chemical, 33.3% disease, 33.3% relationship\n",
    "- Validation (300): 33.3% chemical, 33.3% disease, 33.3% relationship\n",
    "- Test (300): 33.3% chemical, 33.3% disease, 33.3% relationship\n",
    "\n",
    "**Balanced Distribution**:\n",
    "- All three tasks equally represented\n",
    "- Stratified splitting ensures exact proportions in all splits\n",
    "- All splits are properly balanced with perfect 33.3% per task\n",
    "\n",
    "**Next Steps**:\n",
    "1. ‚úÖ Training data is properly split with stratification\n",
    "2. ‚úÖ No data leakage between train/val/test\n",
    "3. ‚úÖ Update `HF_MODEL_ID` below with your trained model ID\n",
    "4. ‚úÖ Run this evaluation notebook on the balanced test set\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites:\n",
    "1. Complete training in `Medical_NER_Fine_Tuning.ipynb` (uses stratified splits!)\n",
    "2. Model saved to `./final_model` or uploaded to HuggingFace Hub\n",
    "3. Test data available in `../data/splits_20251111/test.jsonl`\n",
    "\n",
    "## Evaluation Tasks:\n",
    "1. Load the fine-tuned model\n",
    "2. Evaluate on test set (33.3% each: chemicals, diseases, relationships)\n",
    "3. Calculate precision, recall, F1 scores per task type\n",
    "4. Test on custom medical texts\n",
    "5. Analyze errors and false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09513f9",
   "metadata": {},
   "source": [
    "## üìä Expected Dataset Characteristics (from Deep Data Exploration)\n",
    "\n",
    "**This evaluation is calibrated against known dataset statistics**\n",
    "\n",
    "### Test Set Expected Properties:\n",
    "- **Total samples**: 300 (10% of 3,000)\n",
    "- **Task distribution**: Perfectly balanced (stratified split)\n",
    "  - Chemical extraction: 100 samples (33.3%)\n",
    "  - Disease extraction: 100 samples (33.3%)\n",
    "  - Relationship extraction: 100 samples (33.3%)\n",
    "\n",
    "### Entity Universe (Full Dataset):\n",
    "- **Unique chemicals**: ~1,578 total entities\n",
    "- **Unique diseases**: ~2,199 total entities  \n",
    "- **Vocabulary**: ~13,710 unique words\n",
    "\n",
    "### Entity Complexity:\n",
    "- **Chemical names**: Avg 11.1 chars, 1.2 words\n",
    "- **Disease names**: Avg 14.9 chars, 1.7 words\n",
    "- **Hyphenated entities**: ~459 (e.g., \"type-2 diabetes\")\n",
    "  - Model should preserve hyphens during extraction\n",
    "- **Special characters**: 13 types found in entities\n",
    "\n",
    "### Expected Model Behavior:\n",
    "1. **Format Handling**: Model trained with simple system prompt\n",
    "   - Training format: Llama 3 chat with basic NER instructions\n",
    "   - Evaluation uses matching prompt format for consistency\n",
    "2. **Entity Recognition**: Should extract only entities appearing verbatim in text\n",
    "   - Post-filters verify all predictions exist in source text\n",
    "3. **Precision Focus**: Conservative training to minimize false positives\n",
    "\n",
    "### Quality Benchmarks:\n",
    "- **Training data quality**: Zero duplicates, zero empty completions\n",
    "- **Vocabulary coverage**: Complete (13,710 words trained)\n",
    "- **Task distribution**: Perfect 33.3% per task maintained in all splits\n",
    "\n",
    "**Use these statistics to validate evaluation results and detect anomalies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d2f11",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running the notebook!\n",
    "\n",
    "**Note**: `hf_transfer` is enabled for faster downloads from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enable hf_transfer for faster downloads from HuggingFace Hub\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# HuggingFace Token (required to download your model from Hub)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"HF_TOKEN not found in environment variables\")\n",
    "    hf_token = getpass(\"Enter your HuggingFace token: \")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "else:\n",
    "    print(\"‚úì HF_TOKEN loaded from environment\")\n",
    "\n",
    "# Weights & Biases API Key (optional - only if tracking evaluation metrics)\n",
    "# Get your key from: https://wandb.ai/authorize\n",
    "wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if wandb_key:\n",
    "    print(\"‚úì WANDB_API_KEY loaded from environment\")\n",
    "else:\n",
    "    print(\"‚Ñπ WANDB_API_KEY not set (optional)\")\n",
    "\n",
    "print(\"\\n‚úì Environment variables configured\")\n",
    "print(f\"  HF_HUB_ENABLE_HF_TRANSFER: {os.getenv('HF_HUB_ENABLE_HF_TRANSFER')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1d2ca",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch and other required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers hf-transfer\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")\n",
    "print(\"  - transformers (HuggingFace models)\")\n",
    "print(\"  - peft (LoRA adapters)\")\n",
    "print(\"  - accelerate (device management)\")\n",
    "print(\"  - bitsandbytes (quantization)\")\n",
    "print(\"  - hf-transfer (fast downloads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bce21",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e80ce",
   "metadata": {},
   "source": [
    "## 0) Reusable Utilities\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Run this cell BEFORE running evaluation cells below!\n",
    "\n",
    "These utility functions provide text normalization, hashing, parsing, and validation for the evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904898f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Utilities: normalization, hashing, parsing =====\n",
    "import re, json, hashlib\n",
    "from collections import Counter\n",
    "\n",
    "def dehyphenate(s: str) -> str:\n",
    "    # Join words broken across lines with hyphens + whitespace\n",
    "    return re.sub(r\"(\\w+)-\\s+(\\w+)\", r\"\\1\\2\", s)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = dehyphenate(s or \"\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\u00A0\\t\\r\\n]+\", \" \", s)     # spaces/newlines\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def prompt_hash(prompt: str) -> str:\n",
    "    return hashlib.md5(normalize_text(prompt).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def parse_bullets(text: str):\n",
    "    items = []\n",
    "    for line in (text or \"\").splitlines():\n",
    "        m = re.match(r\"^\\s*[-*]\\s*(.+?)\\s*$\", line)\n",
    "        if m:\n",
    "            items.append(m.group(1))\n",
    "    return items\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    # Keep hyphens intact (e.g., \"type-2 diabetes\" stays \"type-2 diabetes\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # Only normalize whitespace\n",
    "    s = re.sub(r\"[\\.,;:]+$\", \"\", s).strip()\n",
    "    return s\n",
    "\n",
    "def in_text(item: str, text: str) -> bool:\n",
    "    \"\"\"Check if item appears in text using word boundaries to avoid partial matches.\"\"\"\n",
    "    item_norm = normalize_item(item)\n",
    "    text_norm = normalize_text(text)\n",
    "    # Use word boundaries to avoid matching \"aspirin\" in \"aspirinate\"\n",
    "    pattern = r'\\b' + re.escape(item_norm) + r'\\b'\n",
    "    return bool(re.search(pattern, text_norm))\n",
    "\n",
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40a094",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **Update these paths** to match your model location!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec819c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Update with YOUR HuggingFace model ID\n",
    "# Find it at: https://huggingface.co/your-username\n",
    "# Format: \"your-username/llama3-medical-ner-lora-YYYYMMDD_HHMMSS\"\n",
    "HF_MODEL_ID = \"albyos/llama3-medical-ner-checkpoint-450-20251108_114135\"  # ‚Üê UPDATE THIS!\n",
    "\n",
    "# Alternative: Use local model if you prefer\n",
    "USE_HF_HUB = True  # Set to False to use local ../final_model\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "LOCAL_MODEL_PATH = PROJECT_ROOT / \"final_model\"\n",
    "\n",
    "ADAPTER_PATH = HF_MODEL_ID if USE_HF_HUB else str(LOCAL_MODEL_PATH)\n",
    "\n",
    "# Data configuration\n",
    "# For RunPod: Upload test data to /workspace/data/test.jsonl\n",
    "# For local: Use your local path\n",
    "try:\n",
    "    # Try actual split location first\n",
    "    TEST_DATA_PATH = Path.cwd().parent.parent / \"data\" / \"splits_20251111\" / \"test.jsonl\"\n",
    "    if not TEST_DATA_PATH.exists():\n",
    "        # Fallback to parent data directory (for local)\n",
    "        TEST_DATA_PATH = Path.cwd().parent / \"data\" / \"splits_20251111\" / \"test.jsonl\"\n",
    "        if not TEST_DATA_PATH.exists():\n",
    "            # Another fallback - current directory\n",
    "            TEST_DATA_PATH = Path(\"test.jsonl\")\n",
    "except Exception:\n",
    "    TEST_DATA_PATH = Path(\"test.jsonl\")\n",
    "\n",
    "# Verify test data exists\n",
    "if not TEST_DATA_PATH.exists():\n",
    "    print(f\"‚ùå Test data not found at {TEST_DATA_PATH}\")\n",
    "    print(f\"üí° RunPod: Upload to /workspace/data/test.jsonl\")\n",
    "    print(f\"üí° Local: Place in ../data/test.jsonl or notebooks/test.jsonl\")\n",
    "    raise FileNotFoundError(f\"Test data file not found: {TEST_DATA_PATH}\")\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  Adapter source: {'HuggingFace Hub' if USE_HF_HUB else 'Local filesystem'}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Test data: {TEST_DATA_PATH}\")\n",
    "print(f\"  Test data exists: {TEST_DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcae3b",
   "metadata": {},
   "source": [
    "## 4. Authenticate with Hugging Face\n",
    "\n",
    "Log into Hugging Face to download the LoRA adapter when `USE_HF_HUB` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace Hub to access your model\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"‚ùå HF_TOKEN not found in environment\")\n",
    "    print(\"   Please run cell #3 first to set your HF token\")\n",
    "    raise ValueError(\"HF_TOKEN is required to download model from HuggingFace Hub\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=hf_token, add_to_git_credential=True)\n",
    "\n",
    "print(\"‚úì Logged into Hugging Face Hub\")\n",
    "print(f\"  Will load model from: {HF_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6265a9c",
   "metadata": {},
   "source": [
    "## 5. Load the Fine-Tuned Model\n",
    "\n",
    "Load the base model and attach the LoRA adapter from either Hugging Face Hub or your local filesystem.\n",
    "\n",
    "**Note**: Using `hf_transfer` for faster downloads from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure hf_transfer is enabled for faster downloads\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING FINE-TUNED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nLoading base model: {BASE_MODEL}...\")\n",
    "\n",
    "#Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Check for GPU support (optimized for RunPod/CUDA)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"üöÄ NVIDIA GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"üöÄ Apple Silicon GPU (MPS) detected\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"‚ö†Ô∏è  No GPU detected, using CPU (very slow)\")\n",
    "\n",
    "# Load base model with GPU acceleration\n",
    "# On RunPod: Uses CUDA with float16 for optimal performance\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Automatically distribute model across available GPUs\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Base model loaded: {BASE_MODEL}\")\n",
    "print(f\"  Device: {device.upper()}\")\n",
    "print(f\"  Precision: {base_model.dtype}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"  GPU Memory Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Load LoRA adapter from HuggingFace Hub or local path\n",
    "print(f\"\\nLoading LoRA adapter from: {ADAPTER_PATH}...\")\n",
    "print(f\"  Using hf_transfer for faster downloads...\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_PATH,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úì Fine-tuned model loaded successfully!\")\n",
    "print(f\"  Base: {BASE_MODEL}\")\n",
    "print(f\"  LoRA adapter: {ADAPTER_PATH}\")\n",
    "print(f\"  Source: {'HuggingFace Hub' if USE_HF_HUB else 'Local filesystem'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Deterministic generation for evaluation =====\n",
    "def generate_response(prompt_text, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generate a response for a given prompt - DETERMINISTIC for precision.\n",
    "    \n",
    "    CRITICAL: Uses SIMPLE system prompt that matches training format exactly.\n",
    "    This ensures training-inference consistency for optimal performance.\n",
    "    \n",
    "    Generation parameters:\n",
    "    - do_sample=False: Greedy decoding prevents hallucinations\n",
    "    - temperature=0.0: No randomness for reproducible results\n",
    "    - Removes sampling parameters (top_k, top_p)\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy decoding (deterministic)\n",
    "            temperature=0.0,  # No randomness\n",
    "            top_p=1.0,  # Not used with do_sample=False, but set for clarity\n",
    "            num_beams=1,  # No beam search (faster)\n",
    "            repetition_penalty=1.15,  # Slight penalty to avoid repetition\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,  # Enable KV cache for faster generation\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    if \"assistant\\n\\n\" in response:\n",
    "        response = response.split(\"assistant\\n\\n\")[-1]\n",
    "    elif \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"‚úì Deterministic inference function ready\")\n",
    "print(\"  System prompt: MATCHES training format exactly (simple NER instructions)\")\n",
    "print(\"\\n  Generation parameters:\")\n",
    "print(\"    - do_sample: False (greedy decoding)\")\n",
    "print(\"    - temperature: 0.0 (no randomness)\")\n",
    "print(\"    - max_new_tokens: 128 (optimal for NER tasks)\")\n",
    "print(\"    - use_cache: True (KV cache for speed)\")\n",
    "print(\"\\n  Benefits:\")\n",
    "print(\"    - Training-inference consistency (same prompt structure)\")\n",
    "print(\"    - Reproducible results (same input ‚Üí same output)\")\n",
    "print(\"    - Reduced hallucinations and false positives\")\n",
    "print(\"    - Faster inference (no sampling overhead)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad4aaa",
   "metadata": {},
   "source": [
    "## üîç System Prompt - Training-Inference Consistency\n",
    "\n",
    "**CRITICAL**: The evaluation prompt must match the training prompt for optimal performance.\n",
    "\n",
    "### Training Prompt (from Medical_NER_Fine_Tuning.ipynb):\n",
    "```\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.\n",
    "```\n",
    "\n",
    "### Why Simple Prompt Works:\n",
    "- **Training used this exact format** ‚Üí Model learned these specific instructions\n",
    "- **No complex rules** ‚Üí Model relies on training data patterns, not verbose instructions\n",
    "- **Clean output** ‚Üí Trained to produce bullet lists without explanations\n",
    "\n",
    "### Training-Inference Mismatch Problems:\n",
    "- ‚ùå Enhanced prompt during eval ‚â† simple prompt during training = confusion\n",
    "- ‚ùå Model hasn't seen complex instructions ‚Üí may ignore or misinterpret them\n",
    "- ‚ùå Different prompt structure ‚Üí activates different model behaviors\n",
    "\n",
    "### Our Solution:\n",
    "- ‚úÖ Use identical system prompt in training AND evaluation\n",
    "- ‚úÖ Model sees familiar instruction pattern it was trained on\n",
    "- ‚úÖ Activates learned behaviors consistently\n",
    "\n",
    "**Result**: More accurate evaluation that reflects true model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f4971",
   "metadata": {},
   "source": [
    "## 6. Task Classification and Post-Filters\n",
    "\n",
    "These functions classify tasks from prompts and filter predictions to ensure they appear in the source text, reducing false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad71a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Task classification and post-filters =====\n",
    "\n",
    "# Task classifier\n",
    "def task_from_prompt(prompt: str) -> str:\n",
    "    \"\"\"Classify task type from prompt text.\"\"\"\n",
    "    p = normalize_text(prompt)\n",
    "    # Check for influences FIRST because those prompts also contain \"chemicals\" and \"diseases\"\n",
    "    if \"influences between\" in p or \"list of extracted influences\" in p:\n",
    "        return \"influences\"\n",
    "    if \"list of extracted chemicals\" in p or \"chemicals mentioned\" in p:\n",
    "        return \"chemicals\"\n",
    "    if \"list of extracted diseases\" in p or \"diseases mentioned\" in p:\n",
    "        return \"diseases\"\n",
    "    return \"other\"\n",
    "\n",
    "# Entity extraction and filtering\n",
    "def extract_list_from_generation(gen_text):\n",
    "    \"\"\"Parse bullets from the model output.\"\"\"\n",
    "    return parse_bullets(gen_text)\n",
    "\n",
    "def filter_items_against_text(pred_items, prompt_text):\n",
    "    \"\"\"\n",
    "    Keep only items that appear in the source text (after normalization). Deduplicate.\n",
    "    \n",
    "    Enhanced with data exploration insights:\n",
    "    - Strict word boundary matching (~459 hyphenated entities)\n",
    "    - Preserves multi-word entities (avg 1.7 words for diseases)\n",
    "    - Handles special characters (13 types found)\n",
    "    \"\"\"\n",
    "    keep = []\n",
    "    for it in pred_items:\n",
    "        if in_text(it, prompt_text):\n",
    "            keep.append(normalize_item(it))\n",
    "    return unique_preserve_order(keep)\n",
    "\n",
    "# ENHANCED: Strict filtering with confidence scoring\n",
    "def strict_filter_items_against_text(pred_items, prompt_text, min_length=2):\n",
    "    \"\"\"\n",
    "    Stricter filtering to reduce false positives.\n",
    "    \n",
    "    Based on data exploration insights:\n",
    "    - Filters very short entities (likely fragments)\n",
    "    - Requires strict word boundaries\n",
    "    - Validates against known entity complexity (avg 11.1 chars for chemicals, 14.9 for diseases)\n",
    "    \n",
    "    Args:\n",
    "        pred_items: Predicted entity list\n",
    "        prompt_text: Source text to verify against\n",
    "        min_length: Minimum entity length (default 2 to avoid single chars)\n",
    "    \"\"\"\n",
    "    keep = []\n",
    "    for it in pred_items:\n",
    "        normalized = normalize_item(it)\n",
    "        # Skip very short entities (likely noise or fragments)\n",
    "        if len(normalized) < min_length:\n",
    "            continue\n",
    "        # Strict word boundary check\n",
    "        if in_text(it, prompt_text):\n",
    "            keep.append(normalized)\n",
    "    return unique_preserve_order(keep)\n",
    "\n",
    "# Influences/Relationships - parse as pairs or sentences\n",
    "def parse_pairs(gen_text):\n",
    "    \"\"\"Parse 'chemical | disease' pairs from generation output.\"\"\"\n",
    "    pairs = []\n",
    "    for line in parse_bullets(gen_text):\n",
    "        parts = [p.strip() for p in line.split(\"|\")]\n",
    "        if len(parts)==2:\n",
    "            pairs.append(tuple(parts))\n",
    "    return unique_preserve_order(pairs)\n",
    "\n",
    "def parse_pairs_from_sentence(gen_text):\n",
    "    \"\"\"Parse sentence format: 'Chemical X influences disease Y' from generation.\"\"\"\n",
    "    pairs = []\n",
    "    for line in parse_bullets(gen_text):\n",
    "        # Match pattern: \"Chemical NAME influences disease NAME\"\n",
    "        m = re.match(r'^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$', line, re.I)\n",
    "        if m:\n",
    "            pairs.append((m.group(1).strip(), m.group(2).strip()))\n",
    "    return unique_preserve_order(pairs)\n",
    "\n",
    "def filter_pairs_against_text(pairs, prompt_text):\n",
    "    \"\"\"Keep the pair only if BOTH sides appear in the prompt.\"\"\"\n",
    "    kept = []\n",
    "    for chem, dis in pairs:\n",
    "        if in_text(chem, prompt_text) and in_text(dis, prompt_text):\n",
    "            kept.append((normalize_item(chem), normalize_item(dis)))\n",
    "    # Deduplicate normalized pairs\n",
    "    seen=set(); out=[]\n",
    "    for p in kept:\n",
    "        if p not in seen:\n",
    "            seen.add(p); out.append(p)\n",
    "    return out\n",
    "\n",
    "# ENHANCED: Fuzzy matching for minor variations\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def fuzzy_match(pred, gold, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Allow minor typos or formatting differences.\n",
    "    \n",
    "    Based on data exploration:\n",
    "    - 13 types of special characters may cause exact match failures\n",
    "    - Hyphen variations (~459 entities)\n",
    "    - Capitalization differences\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted entity\n",
    "        gold: Gold standard entity\n",
    "        threshold: Similarity threshold (0.9 = 90% match)\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, pred.lower(), gold.lower()).ratio() > threshold\n",
    "\n",
    "def enhanced_match_with_fuzzy(pred_set, gold_set, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Match predictions with gold using both exact and fuzzy matching.\n",
    "    Returns true positives using flexible matching.\n",
    "    \"\"\"\n",
    "    tp_exact = pred_set & gold_set\n",
    "    \n",
    "    # For remaining predictions, try fuzzy matching\n",
    "    remaining_pred = pred_set - gold_set\n",
    "    remaining_gold = gold_set - tp_exact\n",
    "    \n",
    "    tp_fuzzy = set()\n",
    "    for pred in remaining_pred:\n",
    "        for gold in remaining_gold:\n",
    "            if fuzzy_match(pred, gold, threshold):\n",
    "                tp_fuzzy.add(pred)\n",
    "                remaining_gold.discard(gold)\n",
    "                break\n",
    "    \n",
    "    return tp_exact | tp_fuzzy\n",
    "\n",
    "print(\"‚úì Task classification and filter functions loaded\")\n",
    "print(\"\\n  Active Functions (for 3-task dataset):\")\n",
    "print(\"    - task_from_prompt(): Classify chemicals, diseases, or influences\")\n",
    "print(\"    - filter_items_against_text(): Keep only entities in source text\")\n",
    "print(\"    - strict_filter_items_against_text(): Stricter filtering for FP reduction\")\n",
    "print(\"    - parse_pairs() / parse_pairs_from_sentence(): Parse relationship pairs\")\n",
    "print(\"    - filter_pairs_against_text(): Keep pairs where both sides exist\")\n",
    "print(\"\\n  ENHANCED Functions (based on data exploration):\")\n",
    "print(\"    - fuzzy_match(): Handle minor variations (hyphens, special chars)\")\n",
    "print(\"    - enhanced_match_with_fuzzy(): Flexible matching for evaluation\")\n",
    "print(\"\\n  Addresses data exploration findings:\")\n",
    "print(\"    - ~459 hyphenated entities (strict preservation)\")\n",
    "print(\"    - 13 special character types (flexible matching)\")\n",
    "print(\"    - Avg 1.7 words for diseases (multi-word validation)\")\n",
    "print(\"    - Relationship parsing for 'Chemical X influences disease Y' format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857449b",
   "metadata": {},
   "source": [
    "## 7. Evaluate on the Held-Out Test Set\n",
    "\n",
    "Run inference on the test set with deterministic generation and post-filters.\n",
    "\n",
    "**Key Features**:\n",
    "- **Deterministic generation**: No sampling (do_sample=False)\n",
    "- **Post-filters**: Keep only entities that appear in source text\n",
    "- **Per-task metrics**: Separate P/R/F1 for chemicals, diseases, influences\n",
    "- **Sanity checks**: Show examples of false positives and false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9bf27",
   "metadata": {},
   "source": [
    "## üîß Critical Fixes Applied\n",
    "\n",
    "**Format Mismatch Issue Resolved:**\n",
    "\n",
    "The test data uses OLD format for influences:\n",
    "```\n",
    "\"- chemical cyclophosphamide influences disease urinary bladder cancer\"\n",
    "```\n",
    "\n",
    "But the model may output NEW format:\n",
    "```\n",
    "\"- cyclophosphamide | urinary bladder cancer\"\n",
    "```\n",
    "\n",
    "**Solution:** The evaluation now handles BOTH formats automatically by:\n",
    "1. Parsing gold data from OLD sentence format\n",
    "2. Trying to parse model output from NEW format first, then OLD format as fallback\n",
    "3. Normalizing both to `\"chemical | disease\"` format for comparison\n",
    "\n",
    "This ensures accurate metrics regardless of which format the model learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6043fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Evaluation with per-task metrics and filters =====\n",
    "from statistics import mean\n",
    "\n",
    "def f1(p, r): \n",
    "    return 0.0 if (p+r)==0 else 2*p*r/(p+r)\n",
    "\n",
    "# Load test data\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"‚úì Loaded test set: {len(test_data)} samples\")\n",
    "print(f\"\\n‚ö†Ô∏è  IMPORTANT:\")\n",
    "print(f\"  - Training set (80%): Used for fine-tuning\")\n",
    "print(f\"  - Validation set (10%): Monitored during training (W&B)\")\n",
    "print(f\"  - Test set (10%): Used ONLY NOW for final evaluation\")\n",
    "print(f\"\\nRunning evaluation with deterministic generation + post-filters...\")\n",
    "\n",
    "# Initialize per-task counters\n",
    "gold_total = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "pred_total = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "tp_total   = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "\n",
    "examples_fp = []  # False positives\n",
    "examples_fn = []  # False negatives\n",
    "\n",
    "# Process each test sample\n",
    "for idx, row in enumerate(test_data):\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"  Progress: {idx + 1}/{len(test_data)} samples...\")\n",
    "    \n",
    "    prompt = row[\"prompt\"]\n",
    "    gold_items = [normalize_item(x) for x in parse_bullets(row.get(\"completion\",\"\"))]\n",
    "    task = task_from_prompt(prompt)\n",
    "    \n",
    "    # Generate prediction\n",
    "    gen = generate_response(prompt, max_new_tokens=128)\n",
    "    pred_raw = extract_list_from_generation(gen)\n",
    "    \n",
    "    # Apply filters based on task type\n",
    "    if task in {\"chemicals\", \"diseases\"}:\n",
    "        pred = filter_items_against_text(pred_raw, prompt)\n",
    "    elif task == \"influences\":\n",
    "        # Parse gold data (format: \"Chemical X influences disease Y\")\n",
    "        gold_pairs = []\n",
    "        for item in parse_bullets(row.get(\"completion\",\"\")):\n",
    "            # Parse sentence format\n",
    "            m = re.match(r'^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$', item, re.I)\n",
    "            if m:\n",
    "                chem = normalize_item(m.group(1))\n",
    "                dis = normalize_item(m.group(2))\n",
    "                gold_pairs.append(f\"{chem} | {dis}\")\n",
    "        gold_items = gold_pairs\n",
    "        \n",
    "        # Parse model output (could be sentence format OR pipe-separated)\n",
    "        pairs_sentence = parse_pairs_from_sentence(gen)  # Try sentence format first\n",
    "        pairs_pipe = parse_pairs(gen)  # Try pipe format as fallback\n",
    "        all_pairs = pairs_sentence if pairs_sentence else pairs_pipe\n",
    "        \n",
    "        # Normalize both sides of the pair for consistent comparison\n",
    "        pred = [f\"{normalize_item(c)} | {normalize_item(d)}\" \n",
    "                for (c,d) in filter_pairs_against_text(all_pairs, prompt)]\n",
    "    else:\n",
    "        pred = []\n",
    "    \n",
    "    # Convert to sets for metrics\n",
    "    gs = set(gold_items)\n",
    "    ps = set(pred)\n",
    "    \n",
    "    tp = len(gs & ps)\n",
    "    fp = len(ps - gs)\n",
    "    fn = len(gs - ps)\n",
    "    \n",
    "    gold_total[task] += len(gs)\n",
    "    pred_total[task] += len(ps)\n",
    "    tp_total[task]   += tp\n",
    "    \n",
    "    # Collect examples for analysis\n",
    "    if fp and len(examples_fp) < 8:\n",
    "        examples_fp.append({\n",
    "            \"task\": task,\n",
    "            \"prompt_preview\": prompt[:120]+\"...\",\n",
    "            \"pred_extras\": list(ps-gs)[:5]\n",
    "        })\n",
    "    if fn and len(examples_fn) < 8:\n",
    "        examples_fn.append({\n",
    "            \"task\": task,\n",
    "            \"prompt_preview\": prompt[:120]+\"...\",\n",
    "            \"missed\": list(gs-ps)[:5]\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úì Evaluation complete!\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PER-TASK METRICS (with post-filters)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Calculate and display metrics for each task\n",
    "for t in [\"chemicals\", \"diseases\", \"influences\"]:\n",
    "    P = 0.0 if pred_total[t]==0 else tp_total[t]/pred_total[t]\n",
    "    R = 0.0 if gold_total[t]==0 else tp_total[t]/gold_total[t]\n",
    "    F = f1(P,R)\n",
    "    print(f\"{t.upper()}\")\n",
    "    print(f\"  Precision: {P*100:5.1f}%  (TP={tp_total[t]}, Pred={pred_total[t]})\")\n",
    "    print(f\"  Recall:    {R*100:5.1f}%  (TP={tp_total[t]}, Gold={gold_total[t]})\")\n",
    "    print(f\"  F1 Score:  {F*100:5.1f}%\")\n",
    "    print()\n",
    "\n",
    "# Overall metrics\n",
    "total_tp = sum(tp_total.values())\n",
    "total_pred = sum(pred_total.values())\n",
    "total_gold = sum(gold_total.values())\n",
    "overall_P = 0.0 if total_pred==0 else total_tp/total_pred\n",
    "overall_R = 0.0 if total_gold==0 else total_tp/total_gold\n",
    "overall_F = f1(overall_P, overall_R)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"OVERALL METRICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Precision: {overall_P*100:5.1f}%\")\n",
    "print(f\"  Recall:    {overall_R*100:5.1f}%\")\n",
    "print(f\"  F1 Score:  {overall_F*100:5.1f}%\")\n",
    "print(f\"\\n  Total TP: {total_tp}, Total Pred: {total_pred}, Total Gold: {total_gold}\")\n",
    "\n",
    "# Show example errors\n",
    "if examples_fp:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXAMPLE FALSE POSITIVES (model predicted, but not in gold)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for e in examples_fp[:5]:\n",
    "        print(f\"\\nTask: {e['task']}\")\n",
    "        print(f\"Prompt: {e['prompt_preview']}\")\n",
    "        print(f\"Extra predictions: {e['pred_extras']}\")\n",
    "\n",
    "if examples_fn:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXAMPLE FALSE NEGATIVES (in gold, but model missed)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for e in examples_fn[:5]:\n",
    "        print(f\"\\nTask: {e['task']}\")\n",
    "        print(f\"Prompt: {e['prompt_preview']}\")\n",
    "        print(f\"Missed items: {e['missed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e12b8",
   "metadata": {},
   "source": [
    "## üìä Test Set Distribution Validation\n",
    "\n",
    "Verify test set matches expected characteristics from data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Validate test set distribution against exploration findings =====\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST SET VALIDATION (vs. Exploration Expectations)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get task distribution\n",
    "test_tasks = [task_from_prompt(row[\"prompt\"]) for row in test_data]\n",
    "task_dist = Counter(test_tasks)\n",
    "\n",
    "print(f\"\\nüìä Test Set Size: {len(test_data)} samples\")\n",
    "print(f\"   Expected: ~300 samples (10% of 3,000) ‚úì\" if 250 <= len(test_data) <= 350 else f\"   ‚ö†Ô∏è Size anomaly detected!\")\n",
    "\n",
    "print(f\"\\nüìä Task Distribution:\")\n",
    "for task in [\"chemicals\", \"diseases\", \"influences\"]:\n",
    "    count = task_dist.get(task, 0)\n",
    "    pct = count / len(test_data) * 100 if len(test_data) > 0 else 0\n",
    "    expected_pct = 33.3\n",
    "    status = \"‚úì\" if abs(pct - expected_pct) < 5 else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {task.capitalize()}: {count} samples ({pct:.1f}%) {status}\")\n",
    "    print(f\"      Expected: ~33.3% (stratified split)\")\n",
    "\n",
    "# Count unique entities in test completions\n",
    "print(f\"\\nüìä Entity Statistics (Test Set):\")\n",
    "all_chemicals = set()\n",
    "all_diseases = set()\n",
    "all_relationships = 0\n",
    "\n",
    "for row in test_data:\n",
    "    task = task_from_prompt(row[\"prompt\"])\n",
    "    items = [normalize_item(x) for x in parse_bullets(row.get(\"completion\",\"\"))]\n",
    "    \n",
    "    if task == \"chemicals\":\n",
    "        all_chemicals.update(items)\n",
    "    elif task == \"diseases\":\n",
    "        all_diseases.update(items)\n",
    "    elif task == \"influences\":\n",
    "        all_relationships += len(items)\n",
    "\n",
    "print(f\"   Unique chemicals in test: {len(all_chemicals)}\")\n",
    "print(f\"      (Full dataset has ~1,578 unique chemicals)\")\n",
    "print(f\"   Unique diseases in test: {len(all_diseases)}\")\n",
    "print(f\"      (Full dataset has ~2,199 unique diseases)\")\n",
    "print(f\"   Total relationships in test: {all_relationships}\")\n",
    "\n",
    "print(f\"\\n‚úì Test set validation complete!\")\n",
    "print(f\"  Ready for evaluation with calibrated expectations.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5800935",
   "metadata": {},
   "source": [
    "## 8. Custom Test Cases ‚Äî Comprehensive NER Evaluation\n",
    "\n",
    "Test the model's ability to:\n",
    "1. **Extract Chemicals** - Identify drug names and chemical compounds\n",
    "2. **Extract Diseases** - Identify medical conditions and diseases\n",
    "3. **Extract Relationships** - Identify which chemicals are related to which diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Chemical Extraction\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: CHEMICAL EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chemical_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "A patient was treated with aspirin and ibuprofen for pain relief. The combination of these NSAIDs proved effective in reducing inflammation. Additionally, metformin was prescribed for glucose control.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{chemical_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(chemical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ef19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Disease Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: DISEASE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "disease_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the diseases mentioned.\n",
    "\n",
    "The patient presented with hypertension, diabetes mellitus, and chronic kidney disease. Laboratory findings revealed proteinuria and elevated creatinine levels, suggesting diabetic nephropathy.\n",
    "\n",
    "List of extracted diseases:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{disease_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(disease_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3ac1f",
   "metadata": {},
   "source": [
    "# Test 3: Chemical-Disease Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: RELATIONSHIP EXTRACTION - BASIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_1 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the influences between the chemicals and diseases mentioned.\n",
    "\n",
    "Metformin is commonly prescribed for type 2 diabetes by improving insulin sensitivity and reducing hepatic glucose production. Aspirin is used in cardiovascular disease management in high-risk patients.\n",
    "\n",
    "List of extracted influences:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_1}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_1, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49acce4f",
   "metadata": {},
   "source": [
    "# Test 4: Multiple Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: RELATIONSHIP EXTRACTION - MULTIPLE PAIRS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_2 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the influences between the chemicals and diseases mentioned.\n",
    "\n",
    "Long-term use of corticosteroids is associated with osteoporosis and increases the risk of bone fractures. NSAIDs are linked to chronic kidney disease and gastrointestinal bleeding in susceptible patients.\n",
    "\n",
    "List of extracted influences:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_2}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_2, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539f09d",
   "metadata": {},
   "source": [
    "# Test 5: Complex Multi-Entity Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 5: COMPREHENSIVE EXTRACTION - ALL ENTITIES & RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_3 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the influences between the chemicals and diseases mentioned.\n",
    "\n",
    "The patient with rheumatoid arthritis was started on methotrexate for inflammatory joint disease. However, methotrexate is associated with hepatotoxicity and requires monitoring. The patient also has hypertension managed with lisinopril. Statins were prescribed for cardiovascular disease prevention given elevated cholesterol levels.\n",
    "\n",
    "List of extracted influences:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_3}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_3, max_new_tokens=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe24c4f",
   "metadata": {},
   "source": [
    "## üîç Enhanced False Positive Analysis\n",
    "\n",
    "Based on data exploration insights, this section provides diagnostic tools to categorize and analyze false positives:\n",
    "\n",
    "### Known Risk Factors from Data Exploration:\n",
    "1. **Hyphen Variations** (~459 hyphenated entities)\n",
    "   - Examples: \"type-2\" vs \"type 2\", \"5-fluorouracil\" vs \"5 fluorouracil\"\n",
    "   \n",
    "2. **Multi-word Partial Extraction** (avg 1.7 words for diseases)\n",
    "   - Examples: Predicting \"disease\" instead of \"chronic kidney disease\"\n",
    "   \n",
    "3. **Special Character Mismatches** (13 types found)\n",
    "   - Examples: \"COVID-19\" vs \"COVID19\", parentheses, slashes\n",
    "   \n",
    "4. **Format Confusion** (2,050 relationships in OLD format)\n",
    "   - Model outputs sentence format when pipe-separated expected\n",
    "   \n",
    "5. **Synonym Generation** (13,710 vocabulary words)\n",
    "   - Examples: \"myocardial infarction\" vs \"heart attack\"\n",
    "\n",
    "### Diagnostic Approach:\n",
    "- Categorize each false positive by likely root cause\n",
    "- Compute statistics per category\n",
    "- Show representative examples for targeted improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb19209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== False Positive Categorization Functions =====\n",
    "\n",
    "def has_hyphen_variation(pred, gold_set):\n",
    "    \"\"\"\n",
    "    Check if FP is due to hyphen differences.\n",
    "    Based on data exploration: ~459 hyphenated entities at risk.\n",
    "    \"\"\"\n",
    "    # Try replacing hyphens with spaces and vice versa\n",
    "    pred_no_hyphen = pred.replace('-', ' ')\n",
    "    pred_with_hyphen = pred.replace(' ', '-')\n",
    "    \n",
    "    for gold in gold_set:\n",
    "        if pred_no_hyphen.lower() == gold.lower():\n",
    "            return True, gold\n",
    "        if pred_with_hyphen.lower() == gold.lower():\n",
    "            return True, gold\n",
    "    return False, None\n",
    "\n",
    "def is_partial_multiword(pred, gold_set):\n",
    "    \"\"\"\n",
    "    Check if FP is a partial extraction of a multi-word entity.\n",
    "    Based on data exploration: avg 1.7 words for diseases, 1.2 for chemicals.\n",
    "    \"\"\"\n",
    "    pred_lower = pred.lower()\n",
    "    for gold in gold_set:\n",
    "        gold_lower = gold.lower()\n",
    "        # Check if prediction is a substring of gold (or vice versa)\n",
    "        if pred_lower in gold_lower and pred_lower != gold_lower:\n",
    "            return True, gold\n",
    "        if gold_lower in pred_lower and pred_lower != gold_lower:\n",
    "            return True, gold\n",
    "    return False, None\n",
    "\n",
    "def has_special_char_mismatch(pred, gold_set):\n",
    "    \"\"\"\n",
    "    Check if FP is due to special character differences.\n",
    "    Based on data exploration: 13 types of special characters found.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Remove all non-alphanumeric except spaces\n",
    "    pred_clean = re.sub(r'[^a-zA-Z0-9\\s]', '', pred)\n",
    "    \n",
    "    for gold in gold_set:\n",
    "        gold_clean = re.sub(r'[^a-zA-Z0-9\\s]', '', gold)\n",
    "        if pred_clean.lower() == gold_clean.lower():\n",
    "            return True, gold\n",
    "    return False, None\n",
    "\n",
    "def is_likely_synonym(pred, gold_set, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Check if FP might be a synonym or alternative term.\n",
    "    Based on data exploration: 13,710 vocabulary words allow many alternatives.\n",
    "    \"\"\"\n",
    "    from difflib import SequenceMatcher\n",
    "    \n",
    "    pred_lower = pred.lower()\n",
    "    for gold in gold_set:\n",
    "        gold_lower = gold.lower()\n",
    "        # Check for high similarity (but not exact match)\n",
    "        similarity = SequenceMatcher(None, pred_lower, gold_lower).ratio()\n",
    "        if 0.5 < similarity < 1.0 and similarity > threshold:\n",
    "            return True, gold, similarity\n",
    "    return False, None, 0.0\n",
    "\n",
    "def categorize_false_positive(fp, gold_set):\n",
    "    \"\"\"\n",
    "    Categorize a false positive by likely root cause.\n",
    "    \n",
    "    Returns:\n",
    "        category: str - Type of FP\n",
    "        matched_gold: str or None - Gold entity it's similar to\n",
    "        details: dict - Additional diagnostic information\n",
    "    \"\"\"\n",
    "    # Check each category in order\n",
    "    is_hyphen, gold1 = has_hyphen_variation(fp, gold_set)\n",
    "    if is_hyphen:\n",
    "        return \"hyphen_variation\", gold1, {\"original\": fp, \"gold\": gold1}\n",
    "    \n",
    "    is_partial, gold2 = is_partial_multiword(fp, gold_set)\n",
    "    if is_partial:\n",
    "        return \"partial_multiword\", gold2, {\"original\": fp, \"gold\": gold2}\n",
    "    \n",
    "    is_special, gold3 = has_special_char_mismatch(fp, gold_set)\n",
    "    if is_special:\n",
    "        return \"special_char\", gold3, {\"original\": fp, \"gold\": gold3}\n",
    "    \n",
    "    is_synonym, gold4, sim = is_likely_synonym(fp, gold_set)\n",
    "    if is_synonym:\n",
    "        return \"synonym\", gold4, {\"original\": fp, \"gold\": gold4, \"similarity\": sim}\n",
    "    \n",
    "    # If none of the above, it's a true false positive (hallucination)\n",
    "    return \"hallucination\", None, {\"original\": fp}\n",
    "\n",
    "def analyze_false_positives(fps, gold_set):\n",
    "    \"\"\"\n",
    "    Analyze all false positives and categorize them.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics and examples for each FP category\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"hyphen_variation\": [],\n",
    "        \"partial_multiword\": [],\n",
    "        \"special_char\": [],\n",
    "        \"synonym\": [],\n",
    "        \"hallucination\": []\n",
    "    }\n",
    "    \n",
    "    for fp in fps:\n",
    "        category, matched_gold, details = categorize_false_positive(fp, gold_set)\n",
    "        categories[category].append(details)\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats = {\n",
    "        cat: {\n",
    "            \"count\": len(items),\n",
    "            \"percentage\": len(items) / len(fps) * 100 if fps else 0,\n",
    "            \"examples\": items[:5]  # First 5 examples\n",
    "        }\n",
    "        for cat, items in categories.items()\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"‚úì False positive categorization functions loaded\")\n",
    "print(\"\\n  Available Functions:\")\n",
    "print(\"    - has_hyphen_variation(): Check for hyphen differences\")\n",
    "print(\"    - is_partial_multiword(): Check for incomplete multi-word extraction\")\n",
    "print(\"    - has_special_char_mismatch(): Check for special character issues\")\n",
    "print(\"    - is_likely_synonym(): Check for synonym/alternative terms\")\n",
    "print(\"    - categorize_false_positive(): Classify single FP\")\n",
    "print(\"    - analyze_false_positives(): Batch analysis with statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78488df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Example: Analyze False Positives from Test Results =====\n",
    "\n",
    "# This cell demonstrates FP analysis on mock data\n",
    "# Replace with actual predictions from your evaluation results\n",
    "\n",
    "# Mock example data (replace with actual data from evaluation cells above)\n",
    "example_fps = [\n",
    "    \"type 2 diabetes\",      # Hyphen variation of \"type-2 diabetes\"\n",
    "    \"disease\",              # Partial of \"chronic kidney disease\"\n",
    "    \"COVID19\",              # Special char of \"COVID-19\"\n",
    "    \"heart attack\",         # Synonym of \"myocardial infarction\"\n",
    "    \"unicorn syndrome\"      # Hallucination (not in gold)\n",
    "]\n",
    "\n",
    "example_gold = {\n",
    "    \"type-2 diabetes\",\n",
    "    \"chronic kidney disease\",\n",
    "    \"COVID-19\",\n",
    "    \"myocardial infarction\",\n",
    "    \"hypertension\"\n",
    "}\n",
    "\n",
    "# Run analysis\n",
    "fp_stats = analyze_false_positives(example_fps, example_gold)\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 80)\n",
    "print(\"FALSE POSITIVE ANALYSIS REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal False Positives: {len(example_fps)}\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "for category, data in fp_stats.items():\n",
    "    if data[\"count\"] > 0:\n",
    "        print(f\"\\nüìä {category.upper().replace('_', ' ')}\")\n",
    "        print(f\"   Count: {data['count']} ({data['percentage']:.1f}%)\")\n",
    "        print(f\"   Examples:\")\n",
    "        for ex in data[\"examples\"]:\n",
    "            if \"gold\" in ex:\n",
    "                print(f\"      ‚Ä¢ Predicted: '{ex['original']}' ‚Üí Gold: '{ex['gold']}'\")\n",
    "                if \"similarity\" in ex:\n",
    "                    print(f\"        (Similarity: {ex['similarity']:.2f})\")\n",
    "            else:\n",
    "                print(f\"      ‚Ä¢ Predicted: '{ex['original']}' (No match in gold)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Usage Instructions:\")\n",
    "print(\"   1. After running evaluation cells above, collect actual false positives\")\n",
    "print(\"   2. Replace 'example_fps' with your actual FP list\")\n",
    "print(\"   3. Replace 'example_gold' with your gold standard entity set\")\n",
    "print(\"   4. Re-run this cell to see real FP breakdown\")\n",
    "print(\"\\n   Example:\")\n",
    "print(\"      # After evaluation\")\n",
    "print(\"      actual_fps = pred_set - gold_set  # Your false positives\")\n",
    "print(\"      fp_stats = analyze_false_positives(list(actual_fps), gold_set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6a7ac",
   "metadata": {},
   "source": [
    "## üîß Training Data Format Verification\n",
    "\n",
    "Based on data exploration: **2,050 relationships (68%)** were in OLD sentence format.\n",
    "\n",
    "This cell verifies that the format conversion worked correctly during training data preparation. Format confusion is a major source of false positives if the model learned inconsistent relationship representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Verify Training Data Format Conversion =====\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def check_format_conversion(train_file_path, num_samples=10):\n",
    "    \"\"\"\n",
    "    Verify that relationship format conversion was successful.\n",
    "    \n",
    "    Based on data exploration:\n",
    "    - 2,050 relationships (68%) were in OLD sentence format\n",
    "    - Should have been converted to NEW pipe-separated format\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TRAINING DATA FORMAT VERIFICATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Counters\n",
    "    old_format_count = 0\n",
    "    new_format_count = 0\n",
    "    samples_checked = 0\n",
    "    examples = []\n",
    "    \n",
    "    try:\n",
    "        with open(train_file_path, 'r') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                data = json.loads(line.strip())\n",
    "                output = data.get('output', '')\n",
    "                samples_checked += 1\n",
    "                \n",
    "                # Check for OLD format pattern: \"chemical X influences disease Y\"\n",
    "                old_pattern = re.findall(r'chemical\\s+.+?\\s+influences\\s+disease\\s+.+', output, re.I)\n",
    "                # Check for NEW format pattern: \"chemical | disease\"\n",
    "                new_pattern = re.findall(r'.+?\\s*\\|\\s*.+', output)\n",
    "                \n",
    "                if old_pattern:\n",
    "                    old_format_count += 1\n",
    "                    examples.append({\n",
    "                        'sample': idx + 1,\n",
    "                        'format': 'OLD',\n",
    "                        'snippet': old_pattern[0][:100] + '...' if len(old_pattern[0]) > 100 else old_pattern[0]\n",
    "                    })\n",
    "                elif new_pattern and '|' in output:\n",
    "                    new_format_count += 1\n",
    "                    examples.append({\n",
    "                        'sample': idx + 1,\n",
    "                        'format': 'NEW',\n",
    "                        'snippet': new_pattern[0][:100] + '...' if len(new_pattern[0]) > 100 else new_pattern[0]\n",
    "                    })\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìä Format Statistics (from {samples_checked} samples):\")\n",
    "        print(f\"   OLD format (sentence): {old_format_count}\")\n",
    "        print(f\"   NEW format (pipe):     {new_format_count}\")\n",
    "        print(f\"   Other/None:            {samples_checked - old_format_count - new_format_count}\")\n",
    "        \n",
    "        if old_format_count > 0:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNING: Found OLD format relationships in training data!\")\n",
    "            print(\"   This may cause format confusion during inference.\")\n",
    "        else:\n",
    "            print(\"\\n‚úì All relationship samples use consistent NEW (pipe) format\")\n",
    "        \n",
    "        print(\"\\nüìù Sample Outputs:\")\n",
    "        for ex in examples[:5]:\n",
    "            print(f\"   Sample {ex['sample']} ({ex['format']}): {ex['snippet']}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚ùå Training file not found: {train_file_path}\")\n",
    "        print(\"   Please update the path to your training data file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error reading training data: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    return {\n",
    "        'old_format': old_format_count,\n",
    "        'new_format': new_format_count,\n",
    "        'total_checked': samples_checked\n",
    "    }\n",
    "\n",
    "# Run verification\n",
    "# Update path to your actual training data file\n",
    "train_data_path = \"../data/train.jsonl\"\n",
    "format_stats = check_format_conversion(train_data_path, num_samples=50)\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   ‚Ä¢ If OLD format found: Re-prepare training data with consistent NEW format\")\n",
    "print(\"   ‚Ä¢ If all NEW format: Format conversion was successful\")\n",
    "print(\"   ‚Ä¢ Mixed formats may cause model to output inconsistent relationship formats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a3833",
   "metadata": {},
   "source": [
    "## üìà Comparative Evaluation: Standard vs Enhanced Filtering\n",
    "\n",
    "This section compares evaluation metrics using:\n",
    "1. **Standard Filtering**: Basic text verification (current approach)\n",
    "2. **Strict Filtering**: Enhanced word boundaries + length requirements\n",
    "3. **Fuzzy Matching**: Allows minor variations (hyphens, special chars)\n",
    "\n",
    "Goal: Quantify the impact of enhanced filtering on false positive reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cdfbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Comparative Evaluation Function =====\n",
    "\n",
    "def compare_filtering_strategies(predictions, gold_standard, prompt_text):\n",
    "    \"\"\"\n",
    "    Compare different filtering strategies on the same predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of predicted entities (before filtering)\n",
    "        gold_standard: Set of gold standard entities\n",
    "        prompt_text: Source text for verification\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics for each strategy\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Strategy 1: Standard filtering\n",
    "    standard_filtered = filter_items_against_text(predictions, prompt_text)\n",
    "    standard_set = set(standard_filtered)\n",
    "    \n",
    "    tp_standard = len(standard_set & gold_standard)\n",
    "    fp_standard = len(standard_set - gold_standard)\n",
    "    fn_standard = len(gold_standard - standard_set)\n",
    "    \n",
    "    results['standard'] = {\n",
    "        'filtered_count': len(standard_set),\n",
    "        'tp': tp_standard,\n",
    "        'fp': fp_standard,\n",
    "        'fn': fn_standard,\n",
    "        'precision': tp_standard / len(standard_set) if standard_set else 0,\n",
    "        'recall': tp_standard / len(gold_standard) if gold_standard else 0,\n",
    "        'f1': 2 * tp_standard / (2 * tp_standard + fp_standard + fn_standard) if (2 * tp_standard + fp_standard + fn_standard) > 0 else 0,\n",
    "        'fp_examples': list(standard_set - gold_standard)[:5]\n",
    "    }\n",
    "    \n",
    "    # Strategy 2: Strict filtering\n",
    "    strict_filtered = strict_filter_items_against_text(predictions, prompt_text, min_length=2)\n",
    "    strict_set = set(strict_filtered)\n",
    "    \n",
    "    tp_strict = len(strict_set & gold_standard)\n",
    "    fp_strict = len(strict_set - gold_standard)\n",
    "    fn_strict = len(gold_standard - strict_set)\n",
    "    \n",
    "    results['strict'] = {\n",
    "        'filtered_count': len(strict_set),\n",
    "        'tp': tp_strict,\n",
    "        'fp': fp_strict,\n",
    "        'fn': fn_strict,\n",
    "        'precision': tp_strict / len(strict_set) if strict_set else 0,\n",
    "        'recall': tp_strict / len(gold_standard) if gold_standard else 0,\n",
    "        'f1': 2 * tp_strict / (2 * tp_strict + fp_strict + fn_strict) if (2 * tp_strict + fp_strict + fn_strict) > 0 else 0,\n",
    "        'fp_examples': list(strict_set - gold_standard)[:5],\n",
    "        'fp_reduction': fp_standard - fp_strict\n",
    "    }\n",
    "    \n",
    "    # Strategy 3: Fuzzy matching (on standard filtered)\n",
    "    tp_fuzzy_set = enhanced_match_with_fuzzy(standard_set, gold_standard, threshold=0.9)\n",
    "    fp_fuzzy = len(standard_set - tp_fuzzy_set)\n",
    "    fn_fuzzy = len(gold_standard - tp_fuzzy_set)\n",
    "    \n",
    "    results['fuzzy'] = {\n",
    "        'filtered_count': len(standard_set),\n",
    "        'tp': len(tp_fuzzy_set),\n",
    "        'fp': fp_fuzzy,\n",
    "        'fn': fn_fuzzy,\n",
    "        'precision': len(tp_fuzzy_set) / len(standard_set) if standard_set else 0,\n",
    "        'recall': len(tp_fuzzy_set) / len(gold_standard) if gold_standard else 0,\n",
    "        'f1': 2 * len(tp_fuzzy_set) / (2 * len(tp_fuzzy_set) + fp_fuzzy + fn_fuzzy) if (2 * len(tp_fuzzy_set) + fp_fuzzy + fn_fuzzy) > 0 else 0,\n",
    "        'fp_examples': list(standard_set - tp_fuzzy_set)[:5],\n",
    "        'fp_reduction': fp_standard - fp_fuzzy\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_comparison_results(results):\n",
    "    \"\"\"Display comparative results in a formatted table.\"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(\"FILTERING STRATEGY COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    print(f\"\\n{'Strategy':<15} {'Filtered':<10} {'TP':<6} {'FP':<6} {'FN':<6} {'Precision':<12} {'Recall':<12} {'F1':<12} {'FP Reduction':<12}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for strategy, metrics in results.items():\n",
    "        fp_reduction = metrics.get('fp_reduction', 0)\n",
    "        print(f\"{strategy.capitalize():<15} \"\n",
    "              f\"{metrics['filtered_count']:<10} \"\n",
    "              f\"{metrics['tp']:<6} \"\n",
    "              f\"{metrics['fp']:<6} \"\n",
    "              f\"{metrics['fn']:<6} \"\n",
    "              f\"{metrics['precision']:<12.3f} \"\n",
    "              f\"{metrics['recall']:<12.3f} \"\n",
    "              f\"{metrics['f1']:<12.3f} \"\n",
    "              f\"{fp_reduction:<12}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    \n",
    "    # Show FP examples for each strategy\n",
    "    print(\"\\nüìã False Positive Examples by Strategy:\")\n",
    "    for strategy, metrics in results.items():\n",
    "        if metrics['fp_examples']:\n",
    "            print(f\"\\n{strategy.capitalize()} ({metrics['fp']} total FPs):\")\n",
    "            for ex in metrics['fp_examples']:\n",
    "                print(f\"   ‚Ä¢ {ex}\")\n",
    "\n",
    "print(\"‚úì Comparative evaluation functions loaded\")\n",
    "print(\"\\n  Usage:\")\n",
    "print(\"    results = compare_filtering_strategies(predictions, gold_set, prompt_text)\")\n",
    "print(\"    display_comparison_results(results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d694875",
   "metadata": {},
   "source": [
    "## üìö Summary of False Positive Reduction Improvements\n",
    "\n",
    "This notebook now includes comprehensive FP reduction strategies based on data exploration insights:\n",
    "\n",
    "### üéØ Enhancements Added:\n",
    "\n",
    "1. **Enhanced Post-Filtering** (Cell with `strict_filter_items_against_text`)\n",
    "   - Stricter word boundary matching\n",
    "   - Minimum entity length requirements\n",
    "   - Addresses ~459 hyphenated entities and multi-word complexity\n",
    "   \n",
    "2. **Fuzzy Matching Support** (Cell with `fuzzy_match`)\n",
    "   - Handles minor typos and formatting differences\n",
    "   - 90% similarity threshold by default\n",
    "   - Addresses 13 types of special characters\n",
    "   \n",
    "3. **False Positive Categorization** (Cell with `categorize_false_positive`)\n",
    "   - Classifies FPs by root cause:\n",
    "     * Hyphen variations (~459 at risk)\n",
    "     * Partial multi-word extraction (1.7 avg words)\n",
    "     * Special character mismatches (13 types)\n",
    "     * Synonym generation (13,710 vocab)\n",
    "     * True hallucinations\n",
    "   - Provides diagnostic statistics and examples\n",
    "   \n",
    "4. **Training Data Format Verification** (Cell with `check_format_conversion`)\n",
    "   - Validates that 2,050 OLD format relationships were converted\n",
    "   - Detects format inconsistencies that cause inference errors\n",
    "   - Sample inspection for quality assurance\n",
    "   \n",
    "5. **Comparative Evaluation** (Cell with `compare_filtering_strategies`)\n",
    "   - Quantifies impact of different filtering approaches\n",
    "   - Side-by-side metrics: Standard vs Strict vs Fuzzy\n",
    "   - FP reduction tracking\n",
    "\n",
    "### üìä Data Exploration Insights Applied:\n",
    "\n",
    "| Finding | Impact | Solution |\n",
    "|---------|--------|----------|\n",
    "| ~459 hyphenated entities | \"type-2\" vs \"type 2\" mismatches | Fuzzy matching + strict boundaries |\n",
    "| Avg 1.7 words (diseases) | Partial extractions | Multi-word validation |\n",
    "| 13 special char types | Format mismatches | Special char normalization |\n",
    "| 2,050 OLD format (68%) | Format confusion | Conversion verification |\n",
    "| 13,710 vocab words | Synonym generation | Synonym detection |\n",
    "\n",
    "### üöÄ Recommended Workflow:\n",
    "\n",
    "1. **Run standard evaluation** (existing cells)\n",
    "2. **Apply strict filtering** to reduce noise\n",
    "3. **Run FP analysis** to identify root causes\n",
    "4. **Verify format conversion** if relationship extraction has issues\n",
    "5. **Compare strategies** to quantify improvements\n",
    "6. **Apply fuzzy matching** if precision is stable but recall suffers\n",
    "\n",
    "### üí° Expected Outcomes:\n",
    "\n",
    "- **Precision improvement**: Strict filtering removes fragment/noise FPs\n",
    "- **Recall preservation**: Fuzzy matching recovers minor variation mismatches\n",
    "- **Diagnostic clarity**: FP categorization guides targeted fixes\n",
    "- **Format consistency**: Conversion verification prevents systematic errors\n",
    "\n",
    "### ‚öôÔ∏è Next Steps:\n",
    "\n",
    "1. Run evaluation with your fine-tuned model\n",
    "2. Collect actual false positives\n",
    "3. Use `analyze_false_positives()` to categorize them\n",
    "4. Apply appropriate filtering strategy based on FP breakdown\n",
    "5. Iterate on training data if format/quality issues detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51eef3b",
   "metadata": {},
   "source": [
    "## 10. Suggested Next Steps\n",
    "\n",
    "- **Compare results against exploration baselines**:\n",
    "  - Reference: [DATA_EXPLORATION_SUMMARY.md](../../docs/DATA_EXPLORATION_SUMMARY.md)\n",
    "  - Expected vocabulary: 13,710 words (ensure model covers this)\n",
    "  - Entity complexity: ~459 hyphenated entities (verify preservation)\n",
    "  - Format handling: 2,050 relationships trained in NEW pipe format\n",
    "\n",
    "- **Evaluate the full test set** (set `num_test_samples = len(test_data)`) to capture complete performance\n",
    "  - Test set should have ~300 samples (validated above)\n",
    "  - Distribution should be 33.3% / 33.3% / 33.3% across tasks (stratified)\n",
    "\n",
    "- **Compare with the base model** to quantify the lift from fine-tuning\n",
    "  - Baseline (pre-training): No medical entity recognition\n",
    "  - Expected improvement: Significant gain in precision/recall on medical terms\n",
    "\n",
    "- **Log metrics to Weights & Biases** or another tracker for experiment history\n",
    "  - Compare across different checkpoints (every 50 steps)\n",
    "  - Track how format conversion affects relationship extraction F1\n",
    "\n",
    "- **Export predictions for manual spot checks** with subject-matter experts\n",
    "  - Focus on false positives (predicted but not in gold)\n",
    "  - Focus on false negatives (in gold but missed)\n",
    "  - Verify hyphen preservation in entity names\n",
    "\n",
    "- **Analyze vocabulary coverage**:\n",
    "  - Expected: 13,710 unique words in training set\n",
    "  - Check if model generalizes to unseen entity combinations\n",
    "  - Validate entity complexity handling (multi-word, special chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebbaa6",
   "metadata": {},
   "source": [
    "## 11. Usage Example (Optional)\n",
    "\n",
    "How to load the model in a production script or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and use the model later\n",
    "usage_code = '''\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter from Hub\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"your-username/llama3-medical-ner-lora\"  # Your model ID\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Use the model\n",
    "prompt = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "Patient was treated with metformin and insulin for diabetes management.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "# ... (use the generate_response function from above)\n",
    "'''\n",
    "\n",
    "print(\"Usage Example:\")\n",
    "print(\"=\"*80)\n",
    "print(usage_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d1ad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ‚úÖ Configured environment variables and authentication for Hugging Face and W&B.\n",
    "2. ‚úÖ Installed required evaluation dependencies.\n",
    "3. ‚úÖ Loaded the fine-tuned medical NER model (base + LoRA adapter).\n",
    "4. ‚úÖ Evaluated performance on unseen test samples with detailed metrics.\n",
    "5. ‚úÖ Aggregated precision, recall, and F1 across all evaluated examples.\n",
    "6. ‚úÖ Validated behaviour on curated chemical, disease, and relationship prompts.\n",
    "7. ‚úÖ Outlined next steps and provided a ready-to-use inference snippet.\n",
    "\n",
    "**Your medical NER evaluation workflow is ready! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
