{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15813e67",
   "metadata": {},
   "source": [
    "# üöÄ RunPod GPU Setup\n",
    "\n",
    "**This notebook is optimized for RunPod GPU pods with NVIDIA GPUs**\n",
    "\n",
    "## Quick Start on RunPod:\n",
    "\n",
    "1. **Launch a GPU Pod** (RTX 3090, 4090, or A5000 recommended)\n",
    "2. **Upload this notebook** to the pod\n",
    "3. **Upload test data** (`test_run_20251106.jsonl`) to `/workspace/data/`\n",
    "4. **Run cells in order** - evaluation should complete in ~5-10 minutes\n",
    "\n",
    "## Expected Performance:\n",
    "- **GPU**: RTX 3090/4090 ‚Üí ~0.5-1 sec/sample (~5 min total)\n",
    "- **GPU**: RTX A5000 ‚Üí ~1-2 sec/sample (~10 min total)\n",
    "- **Full evaluation**: 300 samples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4828c",
   "metadata": {},
   "source": [
    "# Medical NER Model Evaluation\n",
    "\n",
    "This notebook evaluates the fine-tuned Llama 3.2 3B medical NER model.\n",
    "\n",
    "## ‚úÖ DATASET VERIFIED & READY FOR EVALUATION\n",
    "\n",
    "**Current Dataset Distribution** (from `both_rel_instruct_all.jsonl`):\n",
    "- **1,000 Chemical extraction** examples (25%)\n",
    "- **2,000 Disease extraction** examples (50%) ‚ö†Ô∏è Intentionally 2x more\n",
    "- **1,000 Relationship extraction** examples (25%)\n",
    "\n",
    "**Data Splits Status**: ‚úÖ Properly stratified using `stratify=` parameter\n",
    "- Training (2,400): 25% chemical, 50% disease, 25% relationship\n",
    "- Validation (300+): 25% chemical, 50% disease, 25% relationship\n",
    "- Test (300+): 25% chemical, 50% disease, 25% relationship\n",
    "\n",
    "**Why Disease is 2x more**:\n",
    "- The original dataset has twice as many disease extraction examples\n",
    "- Stratified splitting preserves this 25/50/25 distribution\n",
    "- This appears intentional for better disease NER performance\n",
    "- All splits are properly balanced relative to the source data\n",
    "\n",
    "**Next Steps**:\n",
    "1. ‚úÖ Training data is properly split with stratification\n",
    "2. ‚úÖ No data leakage between train/val/test\n",
    "3. ‚úÖ Update `HF_MODEL_ID` below with your trained model ID\n",
    "4. ‚úÖ Run this evaluation notebook on the balanced test set\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites:\n",
    "1. Complete training in `Medical_NER_Fine_Tuning.ipynb` (uses stratified splits!)\n",
    "2. Model saved to `./final_model` or uploaded to HuggingFace Hub\n",
    "3. Test data available in `notebooks/test.jsonl` or `../data/test.jsonl`\n",
    "\n",
    "## Evaluation Tasks:\n",
    "1. Load the fine-tuned model\n",
    "2. Evaluate on test set (25% chem, 50% disease, 25% relationship)\n",
    "3. Calculate precision, recall, F1 scores per task type\n",
    "4. Test on custom medical texts\n",
    "5. Analyze errors and false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d2f11",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running the notebook!\n",
    "\n",
    "**Note**: `hf_transfer` is enabled for faster downloads from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1f2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN not found in environment variables\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your HuggingFace token:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ñπ WANDB_API_KEY not set (optional)\n",
      "\n",
      "‚úì Environment variables configured\n",
      "  HF_HUB_ENABLE_HF_TRANSFER: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enable hf_transfer for faster downloads from HuggingFace Hub\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# HuggingFace Token (required to download your model from Hub)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"HF_TOKEN not found in environment variables\")\n",
    "    hf_token = getpass(\"Enter your HuggingFace token: \")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "else:\n",
    "    print(\"‚úì HF_TOKEN loaded from environment\")\n",
    "\n",
    "# Weights & Biases API Key (optional - only if tracking evaluation metrics)\n",
    "# Get your key from: https://wandb.ai/authorize\n",
    "wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if wandb_key:\n",
    "    print(\"‚úì WANDB_API_KEY loaded from environment\")\n",
    "else:\n",
    "    print(\"‚Ñπ WANDB_API_KEY not set (optional)\")\n",
    "\n",
    "print(\"\\n‚úì Environment variables configured\")\n",
    "print(f\"  HF_HUB_ENABLE_HF_TRANSFER: {os.getenv('HF_HUB_ENABLE_HF_TRANSFER')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1d2ca",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fd6928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All packages installed successfully!\n",
      "  - transformers (HuggingFace models)\n",
      "  - peft (LoRA adapters)\n",
      "  - accelerate (device management)\n",
      "  - bitsandbytes (quantization)\n",
      "  - hf-transfer (fast downloads)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch and other required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers hf-transfer\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")\n",
    "print(\"  - transformers (HuggingFace models)\")\n",
    "print(\"  - peft (LoRA adapters)\")\n",
    "print(\"  - accelerate (device management)\")\n",
    "print(\"  - bitsandbytes (quantization)\")\n",
    "print(\"  - hf-transfer (fast downloads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bce21",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dfd7949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e80ce",
   "metadata": {},
   "source": [
    "## 0) Reusable Utilities\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Run this cell BEFORE running evaluation cells below!\n",
    "\n",
    "These utility functions provide text normalization, hashing, parsing, and validation for the evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904898f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Utility functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ===== Utilities: normalization, hashing, parsing =====\n",
    "import re, json, hashlib\n",
    "from collections import Counter\n",
    "\n",
    "def dehyphenate(s: str) -> str:\n",
    "    # Join words broken across lines with hyphens + whitespace\n",
    "    return re.sub(r\"(\\w+)-\\s+(\\w+)\", r\"\\1\\2\", s)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = dehyphenate(s or \"\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\u00A0\\t\\r\\n]+\", \" \", s)     # spaces/newlines\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def prompt_hash(prompt: str) -> str:\n",
    "    return hashlib.md5(normalize_text(prompt).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def parse_bullets(text: str):\n",
    "    items = []\n",
    "    for line in (text or \"\").splitlines():\n",
    "        m = re.match(r\"^\\s*[-*]\\s*(.+?)\\s*$\", line)\n",
    "        if m:\n",
    "            items.append(m.group(1))\n",
    "    return items\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    # Keep hyphens intact (e.g., \"type-2 diabetes\" stays \"type-2 diabetes\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # Only normalize whitespace\n",
    "    s = re.sub(r\"[\\.,;:]+$\", \"\", s).strip()\n",
    "    return s\n",
    "\n",
    "def in_text(item: str, text: str) -> bool:\n",
    "    \"\"\"Check if item appears in text using word boundaries to avoid partial matches.\"\"\"\n",
    "    item_norm = normalize_item(item)\n",
    "    text_norm = normalize_text(text)\n",
    "    # Use word boundaries to avoid matching \"aspirin\" in \"aspirinate\"\n",
    "    pattern = r'\\b' + re.escape(item_norm) + r'\\b'\n",
    "    return bool(re.search(pattern, text_norm))\n",
    "\n",
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40a094",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **Update these paths** to match your model location!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec819c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Base model: meta-llama/Llama-3.2-3B-Instruct\n",
      "  Adapter source: HuggingFace Hub\n",
      "  Adapter path: albyos/llama3-medical-ner-checkpoint-450-20251108_114135\n",
      "  Test data: test_run-20251108.jsonl\n",
      "  Test data exists: True\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Update with YOUR HuggingFace model ID\n",
    "# Find it at: https://huggingface.co/your-username\n",
    "# Format: \"your-username/llama3-medical-ner-lora-YYYYMMDD_HHMMSS\"\n",
    "HF_MODEL_ID = \"albyos/llama3-medical-ner-checkpoint-450-20251108_114135\"  # ‚Üê UPDATE THIS!\n",
    "\n",
    "# Alternative: Use local model if you prefer\n",
    "USE_HF_HUB = True  # Set to False to use local ../final_model\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "LOCAL_MODEL_PATH = PROJECT_ROOT / \"final_model\"\n",
    "\n",
    "ADAPTER_PATH = HF_MODEL_ID if USE_HF_HUB else str(LOCAL_MODEL_PATH)\n",
    "\n",
    "# Data configuration\n",
    "# For RunPod: Upload test data to /workspace/data/test.jsonl\n",
    "# For local: Use your local path\n",
    "try:\n",
    "    # Try current directory first (for RunPod/workspace)\n",
    "    TEST_DATA_PATH = Path(\"test_run-20251108.jsonl\")\n",
    "    if not TEST_DATA_PATH.exists():\n",
    "        # Fallback to parent data directory (for local)\n",
    "        TEST_DATA_PATH = Path.cwd().parent / \"test_run-20251108.jsonl\"\n",
    "        if not TEST_DATA_PATH.exists():\n",
    "            # Another fallback - notebooks directory test.jsonl\n",
    "            TEST_DATA_PATH = Path.cwd() / \"test_run-20251108.jsonl\"\n",
    "except Exception:\n",
    "    TEST_DATA_PATH = Path(\"test_run-20251108.jsonl\")\n",
    "\n",
    "# Verify test data exists\n",
    "if not TEST_DATA_PATH.exists():\n",
    "    print(f\"‚ùå Test data not found at {TEST_DATA_PATH}\")\n",
    "    print(f\"üí° RunPod: Upload to /workspace/data/test.jsonl\")\n",
    "    print(f\"üí° Local: Place in ../data/test.jsonl or notebooks/test.jsonl\")\n",
    "    raise FileNotFoundError(f\"Test data file not found: {TEST_DATA_PATH}\")\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  Adapter source: {'HuggingFace Hub' if USE_HF_HUB else 'Local filesystem'}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Test data: {TEST_DATA_PATH}\")\n",
    "print(f\"  Test data exists: {TEST_DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcae3b",
   "metadata": {},
   "source": [
    "## 4. Authenticate with Hugging Face\n",
    "\n",
    "Log into Hugging Face to download the LoRA adapter when `USE_HF_HUB` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587c6862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "‚úì Logged into Hugging Face Hub\n",
      "  Will load model from: albyos/llama3-medical-ner-checkpoint-450-20251108_114135\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace Hub to access your model\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"‚ùå HF_TOKEN not found in environment\")\n",
    "    print(\"   Please run cell #3 first to set your HF token\")\n",
    "    raise ValueError(\"HF_TOKEN is required to download model from HuggingFace Hub\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=hf_token, add_to_git_credential=True)\n",
    "\n",
    "print(\"‚úì Logged into Hugging Face Hub\")\n",
    "print(f\"  Will load model from: {HF_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6265a9c",
   "metadata": {},
   "source": [
    "## 5. Load the Fine-Tuned Model\n",
    "\n",
    "Load the base model and attach the LoRA adapter from either Hugging Face Hub or your local filesystem.\n",
    "\n",
    "**Note**: Using `hf_transfer` for faster downloads from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2794a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING FINE-TUNED MODEL\n",
      "================================================================================\n",
      "\n",
      "Loading base model: meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5b7ebb31534bcfa35938c9cf47ac90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4bb2878a0c43aa98d8e63a9f0dade5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8614df3367495ba1c9dc987d66c459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenizer loaded\n",
      "üöÄ NVIDIA GPU detected: NVIDIA RTX A4000\n",
      "   GPU Memory: 16.9 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2f7b70598f476490461bdc80f85c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416a865606a74e0ca88dc9e0efcd1d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa9a3b13eee4ddf958a17461278ffba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19c2bb9b7994ee59a83d7dfbfc15094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e3b93bcddf47179244ff3bea1543b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355de2d0c69640d4af9069ea98e81755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Base model loaded: meta-llama/Llama-3.2-3B-Instruct\n",
      "  Device: CUDA\n",
      "  Precision: torch.float16\n",
      "  GPU Memory Used: 6.43 GB\n",
      "\n",
      "Loading LoRA adapter from: albyos/llama3-medical-ner-checkpoint-450-20251108_114135...\n",
      "  Using hf_transfer for faster downloads...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f5b8a67b8a48af92566bb1b50d9363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/944 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bb0f0aef8e4a1cb6a2a573bfd739f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/97.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Fine-tuned model loaded successfully!\n",
      "  Base: meta-llama/Llama-3.2-3B-Instruct\n",
      "  LoRA adapter: albyos/llama3-medical-ner-checkpoint-450-20251108_114135\n",
      "  Source: HuggingFace Hub\n"
     ]
    }
   ],
   "source": [
    "# Ensure hf_transfer is enabled for faster downloads\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING FINE-TUNED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nLoading base model: {BASE_MODEL}...\")\n",
    "\n",
    "#Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Check for GPU support (optimized for RunPod/CUDA)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"üöÄ NVIDIA GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"üöÄ Apple Silicon GPU (MPS) detected\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"‚ö†Ô∏è  No GPU detected, using CPU (very slow)\")\n",
    "\n",
    "# Load base model with GPU acceleration\n",
    "# On RunPod: Uses CUDA with float16 for optimal performance\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Automatically distribute model across available GPUs\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Base model loaded: {BASE_MODEL}\")\n",
    "print(f\"  Device: {device.upper()}\")\n",
    "print(f\"  Precision: {base_model.dtype}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"  GPU Memory Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Load LoRA adapter from HuggingFace Hub or local path\n",
    "print(f\"\\nLoading LoRA adapter from: {ADAPTER_PATH}...\")\n",
    "print(f\"  Using hf_transfer for faster downloads...\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_PATH,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úì Fine-tuned model loaded successfully!\")\n",
    "print(f\"  Base: {BASE_MODEL}\")\n",
    "print(f\"  LoRA adapter: {ADAPTER_PATH}\")\n",
    "print(f\"  Source: {'HuggingFace Hub' if USE_HF_HUB else 'Local filesystem'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11a3015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Deterministic inference function ready\n",
      "  Generation parameters:\n",
      "    - do_sample: False (greedy decoding)\n",
      "    - temperature: 0.0 (no randomness)\n",
      "    - max_new_tokens: 128 (optimal for NER tasks)\n",
      "    - use_cache: True (KV cache for speed)\n",
      "\n",
      "  Benefits:\n",
      "    - Reproducible results (same input ‚Üí same output)\n",
      "    - Reduced hallucinations and false positives\n",
      "    - Faster inference (no sampling overhead)\n",
      "\n",
      "  Expected speed on RunPod GPU:\n",
      "    - RTX 3090/4090: ~0.5-1 second per sample\n",
      "    - RTX A5000: ~1-2 seconds per sample\n",
      "    - Full evaluation (300 samples): ~5-10 minutes\n"
     ]
    }
   ],
   "source": [
    "# ===== Deterministic generation for evaluation =====\n",
    "def generate_response(prompt_text, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generate a response for a given prompt - DETERMINISTIC for precision.\n",
    "    \n",
    "    Key changes from training version:\n",
    "    - do_sample=False: Greedy decoding prevents hallucinations\n",
    "    - temperature=0.0: No randomness\n",
    "    - Removes sampling parameters (top_k, top_p)\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Return ONLY entities that appear verbatim in the article.\n",
    "Output one item per line, each starting with '- '.\n",
    "If none exist, return nothing.\n",
    "Do not add explanations or examples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy decoding (deterministic)\n",
    "            temperature=0.0,  # No randomness\n",
    "            top_p=1.0,  # Not used with do_sample=False, but set for clarity\n",
    "            num_beams=1,  # No beam search (faster)\n",
    "            repetition_penalty=1.15,  # Slight penalty to avoid repetition\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,  # Enable KV cache for faster generation\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    if \"assistant\\n\\n\" in response:\n",
    "        response = response.split(\"assistant\\n\\n\")[-1]\n",
    "    elif \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"‚úì Deterministic inference function ready\")\n",
    "print(\"  Generation parameters:\")\n",
    "print(\"    - do_sample: False (greedy decoding)\")\n",
    "print(\"    - temperature: 0.0 (no randomness)\")\n",
    "print(\"    - max_new_tokens: 128 (optimal for NER tasks)\")\n",
    "print(\"    - use_cache: True (KV cache for speed)\")\n",
    "print(\"\\n  Benefits:\")\n",
    "print(\"    - Reproducible results (same input ‚Üí same output)\")\n",
    "print(\"    - Reduced hallucinations and false positives\")\n",
    "print(\"    - Faster inference (no sampling overhead)\")\n",
    "print(\"\\n  Expected speed on RunPod GPU:\")\n",
    "print(\"    - RTX 3090/4090: ~0.5-1 second per sample\")\n",
    "print(\"    - RTX A5000: ~1-2 seconds per sample\")\n",
    "print(\"    - Full evaluation (300 samples): ~5-10 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f4971",
   "metadata": {},
   "source": [
    "## 6. Task Classification and Post-Filters\n",
    "\n",
    "These functions classify tasks from prompts and filter predictions to ensure they appear in the source text, reducing false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad71a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Task classification and filter functions loaded\n",
      "  Functions:\n",
      "    - task_from_prompt(): Classify task type\n",
      "    - filter_items_against_text(): Keep only entities in source text\n",
      "    - parse_pairs(): Parse 'chemical | disease' pairs\n",
      "    - filter_pairs_against_text(): Keep pairs where both sides exist\n"
     ]
    }
   ],
   "source": [
    "# ===== Task classification and post-filters =====\n",
    "\n",
    "# Task classifier\n",
    "def task_from_prompt(prompt: str) -> str:\n",
    "    \"\"\"Classify task type from prompt text.\"\"\"\n",
    "    p = normalize_text(prompt)\n",
    "    if \"list of extracted chemicals\" in p: return \"chemicals\"\n",
    "    if \"list of extracted diseases\"  in p: return \"diseases\"\n",
    "    if \"list of extracted influences\" in p: return \"influences\"\n",
    "    # Fallback patterns\n",
    "    if \"chemicals mentioned\" in p: return \"chemicals\"\n",
    "    if \"diseases mentioned\" in p: return \"diseases\"\n",
    "    if \"influences between\" in p: return \"influences\"\n",
    "    return \"other\"\n",
    "\n",
    "# Entity extraction and filtering\n",
    "def extract_list_from_generation(gen_text):\n",
    "    \"\"\"Parse bullets from the model output.\"\"\"\n",
    "    return parse_bullets(gen_text)\n",
    "\n",
    "def filter_items_against_text(pred_items, prompt_text):\n",
    "    \"\"\"Keep only items that appear in the source text (after normalization). Deduplicate.\"\"\"\n",
    "    keep = []\n",
    "    for it in pred_items:\n",
    "        if in_text(it, prompt_text):\n",
    "            keep.append(normalize_item(it))\n",
    "    return unique_preserve_order(keep)\n",
    "\n",
    "# Influences/Relationships - parse as pairs\n",
    "def parse_pairs(gen_text):\n",
    "    \"\"\"Parse 'chemical | disease' pairs from generation output.\"\"\"\n",
    "    pairs = []\n",
    "    for line in parse_bullets(gen_text):\n",
    "        parts = [p.strip() for p in line.split(\"|\")]\n",
    "        if len(parts)==2:\n",
    "            pairs.append(tuple(parts))\n",
    "    return unique_preserve_order(pairs)\n",
    "\n",
    "def parse_pairs_from_sentence(gen_text):\n",
    "    \"\"\"Parse OLD format: 'chemical X influences disease Y' from generation.\"\"\"\n",
    "    pairs = []\n",
    "    for line in parse_bullets(gen_text):\n",
    "        # Match pattern: \"chemical NAME influences disease NAME\"\n",
    "        m = re.match(r'^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$', line, re.I)\n",
    "        if m:\n",
    "            pairs.append((m.group(1).strip(), m.group(2).strip()))\n",
    "    return unique_preserve_order(pairs)\n",
    "\n",
    "def filter_pairs_against_text(pairs, prompt_text):\n",
    "    \"\"\"Keep the pair only if BOTH sides appear in the prompt.\"\"\"\n",
    "    kept = []\n",
    "    for chem, dis in pairs:\n",
    "        if in_text(chem, prompt_text) and in_text(dis, prompt_text):\n",
    "            kept.append((normalize_item(chem), normalize_item(dis)))\n",
    "    # Deduplicate normalized pairs\n",
    "    seen=set(); out=[]\n",
    "    for p in kept:\n",
    "        if p not in seen:\n",
    "            seen.add(p); out.append(p)\n",
    "    return out\n",
    "\n",
    "# Temporary fallback if you still have sentence outputs\n",
    "def sentence_to_pair(line):\n",
    "    \"\"\"Parse sentence-style influences: 'Chemical X influences disease Y'\"\"\"\n",
    "    m = re.match(r\"^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$\", line, re.I)\n",
    "    return (m.group(1), m.group(2)) if m else None\n",
    "\n",
    "print(\"‚úì Task classification and filter functions loaded\")\n",
    "print(\"  Functions:\")\n",
    "print(\"    - task_from_prompt(): Classify task type\")\n",
    "print(\"    - filter_items_against_text(): Keep only entities in source text\")\n",
    "print(\"    - parse_pairs(): Parse 'chemical | disease' pairs\")\n",
    "print(\"    - filter_pairs_against_text(): Keep pairs where both sides exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857449b",
   "metadata": {},
   "source": [
    "## 7. Evaluate on the Held-Out Test Set\n",
    "\n",
    "Run inference on the test set with deterministic generation and post-filters.\n",
    "\n",
    "**Key Features**:\n",
    "- **Deterministic generation**: No sampling (do_sample=False)\n",
    "- **Post-filters**: Keep only entities that appear in source text\n",
    "- **Per-task metrics**: Separate P/R/F1 for chemicals, diseases, influences\n",
    "- **Sanity checks**: Show examples of false positives and false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9bf27",
   "metadata": {},
   "source": [
    "## üîß Critical Fixes Applied\n",
    "\n",
    "**Format Mismatch Issue Resolved:**\n",
    "\n",
    "The test data uses OLD format for influences:\n",
    "```\n",
    "\"- chemical cyclophosphamide influences disease urinary bladder cancer\"\n",
    "```\n",
    "\n",
    "But the model may output NEW format:\n",
    "```\n",
    "\"- cyclophosphamide | urinary bladder cancer\"\n",
    "```\n",
    "\n",
    "**Solution:** The evaluation now handles BOTH formats automatically by:\n",
    "1. Parsing gold data from OLD sentence format\n",
    "2. Trying to parse model output from NEW format first, then OLD format as fallback\n",
    "3. Normalizing both to `\"chemical | disease\"` format for comparison\n",
    "\n",
    "This ensures accurate metrics regardless of which format the model learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6043fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded test set: 300 samples\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT:\n",
      "  - Training set (80%): Used for fine-tuning\n",
      "  - Validation set (10%): Monitored during training (W&B)\n",
      "  - Test set (10%): Used ONLY NOW for final evaluation\n",
      "\n",
      "Running evaluation with deterministic generation + post-filters...\n"
     ]
    }
   ],
   "source": [
    "# ===== Evaluation with per-task metrics and filters =====\n",
    "from statistics import mean\n",
    "\n",
    "def f1(p, r): \n",
    "    return 0.0 if (p+r)==0 else 2*p*r/(p+r)\n",
    "\n",
    "# Load test data\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"‚úì Loaded test set: {len(test_data)} samples\")\n",
    "print(f\"\\n‚ö†Ô∏è  IMPORTANT:\")\n",
    "print(f\"  - Training set (80%): Used for fine-tuning\")\n",
    "print(f\"  - Validation set (10%): Monitored during training (W&B)\")\n",
    "print(f\"  - Test set (10%): Used ONLY NOW for final evaluation\")\n",
    "print(f\"\\nRunning evaluation with deterministic generation + post-filters...\")\n",
    "\n",
    "# Initialize per-task counters\n",
    "gold_total = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "pred_total = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "tp_total   = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "\n",
    "examples_fp = []  # False positives\n",
    "examples_fn = []  # False negatives\n",
    "\n",
    "# Process each test sample\n",
    "for idx, row in enumerate(test_data):\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"  Progress: {idx + 1}/{len(test_data)} samples...\")\n",
    "    \n",
    "    prompt = row[\"prompt\"]\n",
    "    gold_items = [normalize_item(x) for x in parse_bullets(row.get(\"completion\",\"\"))]\n",
    "    task = task_from_prompt(prompt)\n",
    "    \n",
    "    # Generate prediction\n",
    "    gen = generate_response(prompt, max_new_tokens=128)\n",
    "    pred_raw = extract_list_from_generation(gen)\n",
    "    \n",
    "    # Apply filters based on task type\n",
    "    if task in {\"chemicals\", \"diseases\"}:\n",
    "        pred = filter_items_against_text(pred_raw, prompt)\n",
    "    elif task == \"influences\":\n",
    "        # Parse gold data (OLD format: \"chemical X influences disease Y\")\n",
    "        gold_pairs = []\n",
    "        for item in parse_bullets(row.get(\"completion\",\"\")):\n",
    "            # Try parsing sentence format\n",
    "            m = re.match(r'^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$', item, re.I)\n",
    "            if m:\n",
    "                chem = normalize_item(m.group(1))\n",
    "                dis = normalize_item(m.group(2))\n",
    "                gold_pairs.append(f\"{chem} | {dis}\")\n",
    "        gold_items = gold_pairs\n",
    "        \n",
    "        # Parse model output (could be NEW format \"chem | disease\" OR OLD format)\n",
    "        pairs_new = parse_pairs(gen)  # Try new format first\n",
    "        pairs_old = parse_pairs_from_sentence(gen)  # Try old format as fallback\n",
    "        all_pairs = pairs_new if pairs_new else pairs_old\n",
    "        \n",
    "        # Normalize both sides of the pair for consistent comparison\n",
    "        pred = [f\"{normalize_item(c)} | {normalize_item(d)}\" \n",
    "                for (c,d) in filter_pairs_against_text(all_pairs, prompt)]\n",
    "    else:\n",
    "        pred = []\n",
    "    \n",
    "    # Convert to sets for metrics\n",
    "    gs = set(gold_items)\n",
    "    ps = set(pred)\n",
    "    \n",
    "    tp = len(gs & ps)\n",
    "    fp = len(ps - gs)\n",
    "    fn = len(gs - ps)\n",
    "    \n",
    "    gold_total[task] += len(gs)\n",
    "    pred_total[task] += len(ps)\n",
    "    tp_total[task]   += tp\n",
    "    \n",
    "    # Collect examples for analysis\n",
    "    if fp and len(examples_fp) < 8:\n",
    "        examples_fp.append({\n",
    "            \"task\": task,\n",
    "            \"prompt_preview\": prompt[:120]+\"...\",\n",
    "            \"pred_extras\": list(ps-gs)[:5]\n",
    "        })\n",
    "    if fn and len(examples_fn) < 8:\n",
    "        examples_fn.append({\n",
    "            \"task\": task,\n",
    "            \"prompt_preview\": prompt[:120]+\"...\",\n",
    "            \"missed\": list(gs-ps)[:5]\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úì Evaluation complete!\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PER-TASK METRICS (with post-filters)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Calculate and display metrics for each task\n",
    "for t in [\"chemicals\", \"diseases\", \"influences\"]:\n",
    "    P = 0.0 if pred_total[t]==0 else tp_total[t]/pred_total[t]\n",
    "    R = 0.0 if gold_total[t]==0 else tp_total[t]/gold_total[t]\n",
    "    F = f1(P,R)\n",
    "    print(f\"{t.upper()}\")\n",
    "    print(f\"  Precision: {P*100:5.1f}%  (TP={tp_total[t]}, Pred={pred_total[t]})\")\n",
    "    print(f\"  Recall:    {R*100:5.1f}%  (TP={tp_total[t]}, Gold={gold_total[t]})\")\n",
    "    print(f\"  F1 Score:  {F*100:5.1f}%\")\n",
    "    print()\n",
    "\n",
    "# Overall metrics\n",
    "total_tp = sum(tp_total.values())\n",
    "total_pred = sum(pred_total.values())\n",
    "total_gold = sum(gold_total.values())\n",
    "overall_P = 0.0 if total_pred==0 else total_tp/total_pred\n",
    "overall_R = 0.0 if total_gold==0 else total_tp/total_gold\n",
    "overall_F = f1(overall_P, overall_R)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"OVERALL METRICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Precision: {overall_P*100:5.1f}%\")\n",
    "print(f\"  Recall:    {overall_R*100:5.1f}%\")\n",
    "print(f\"  F1 Score:  {overall_F*100:5.1f}%\")\n",
    "print(f\"\\n  Total TP: {total_tp}, Total Pred: {total_pred}, Total Gold: {total_gold}\")\n",
    "\n",
    "# Show example errors\n",
    "if examples_fp:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXAMPLE FALSE POSITIVES (model predicted, but not in gold)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for e in examples_fp[:5]:\n",
    "        print(f\"\\nTask: {e['task']}\")\n",
    "        print(f\"Prompt: {e['prompt_preview']}\")\n",
    "        print(f\"Extra predictions: {e['pred_extras']}\")\n",
    "\n",
    "if examples_fn:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXAMPLE FALSE NEGATIVES (in gold, but model missed)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for e in examples_fn[:5]:\n",
    "        print(f\"\\nTask: {e['task']}\")\n",
    "        print(f\"Prompt: {e['prompt_preview']}\")\n",
    "        print(f\"Missed items: {e['missed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5800935",
   "metadata": {},
   "source": [
    "## 8. Custom Test Cases ‚Äî Comprehensive NER Evaluation\n",
    "\n",
    "Test the model's ability to:\n",
    "1. **Extract Chemicals** - Identify drug names and chemical compounds\n",
    "2. **Extract Diseases** - Identify medical conditions and diseases\n",
    "3. **Extract Relationships** - Identify which chemicals are related to which diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Chemical Extraction\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: CHEMICAL EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chemical_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "A patient was treated with aspirin and ibuprofen for pain relief. The combination of these NSAIDs proved effective in reducing inflammation. Additionally, metformin was prescribed for glucose control.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{chemical_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(chemical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ef19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Disease Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: DISEASE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "disease_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the diseases mentioned.\n",
    "\n",
    "The patient presented with hypertension, diabetes mellitus, and chronic kidney disease. Laboratory findings revealed proteinuria and elevated creatinine levels, suggesting diabetic nephropathy.\n",
    "\n",
    "List of extracted diseases:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{disease_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(disease_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b12487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Chemical-Disease Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: RELATIONSHIP EXTRACTION - BASIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_1 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract the relationships between chemicals and diseases mentioned in the text.\n",
    "\n",
    "Metformin is commonly prescribed for type 2 diabetes by improving insulin sensitivity and reducing hepatic glucose production. Aspirin is used in cardiovascular disease management in high-risk patients.\n",
    "\n",
    "List the chemical-disease relationships:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_1}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_1, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Multiple Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: RELATIONSHIP EXTRACTION - MULTIPLE PAIRS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_2 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Identify all chemical-disease pairs and their relationships.\n",
    "\n",
    "Long-term use of corticosteroids is associated with osteoporosis and increases the risk of bone fractures. NSAIDs are linked to chronic kidney disease and gastrointestinal bleeding in susceptible patients.\n",
    "\n",
    "List of chemical-disease relationships:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_2}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_2, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Complex Multi-Entity Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 5: COMPREHENSIVE EXTRACTION - ALL ENTITIES & RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_3 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract:\n",
    "1. All chemicals mentioned\n",
    "2. All diseases mentioned\n",
    "3. All relationships between chemicals and diseases\n",
    "\n",
    "The patient with rheumatoid arthritis was started on methotrexate for inflammatory joint disease. However, methotrexate is associated with hepatotoxicity and requires monitoring. The patient also has hypertension managed with lisinopril. Statins were prescribed for cardiovascular disease prevention given elevated cholesterol levels.\n",
    "\n",
    "Extracted information:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_3}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_3, max_new_tokens=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51eef3b",
   "metadata": {},
   "source": [
    "## 10. Suggested Next Steps\n",
    "\n",
    "- Evaluate the full test set (set `num_test_samples = len(test_data)`) to capture complete performance.\n",
    "- Compare with the base model to quantify the lift from fine-tuning.\n",
    "- Log metrics to Weights & Biases or another tracker for experiment history.\n",
    "- Export predictions for manual spot checks with subject-matter experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebbaa6",
   "metadata": {},
   "source": [
    "## 11. Usage Example (Optional)\n",
    "\n",
    "How to load the model in a production script or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and use the model later\n",
    "usage_code = '''\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter from Hub\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"your-username/llama3-medical-ner-lora\"  # Your model ID\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Use the model\n",
    "prompt = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "Patient was treated with metformin and insulin for diabetes management.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "# ... (use the generate_response function from above)\n",
    "'''\n",
    "\n",
    "print(\"Usage Example:\")\n",
    "print(\"=\"*80)\n",
    "print(usage_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d1ad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ‚úÖ Configured environment variables and authentication for Hugging Face and W&B.\n",
    "2. ‚úÖ Installed required evaluation dependencies.\n",
    "3. ‚úÖ Loaded the fine-tuned medical NER model (base + LoRA adapter).\n",
    "4. ‚úÖ Evaluated performance on unseen test samples with detailed metrics.\n",
    "5. ‚úÖ Aggregated precision, recall, and F1 across all evaluated examples.\n",
    "6. ‚úÖ Validated behaviour on curated chemical, disease, and relationship prompts.\n",
    "7. ‚úÖ Outlined next steps and provided a ready-to-use inference snippet.\n",
    "\n",
    "**Your medical NER evaluation workflow is ready! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
