{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb06cd0",
   "metadata": {},
   "source": [
    "# Medical NER Fine-Tuning with Llama 3.2 3B + LoRA\n",
    "\n",
    "This notebook implements fine-tuning of Llama 3.2 3B Instruct for medical Named Entity Recognition (NER) using:\n",
    "- **SFT** (Supervised Fine-Tuning)\n",
    "- **LoRA** (Low-Rank Adaptation)\n",
    "- **Hugging Face Hub** integration for checkpoint uploads\n",
    "\n",
    "## Tasks:\n",
    "1. Chemical entity extraction\n",
    "2. Disease entity extraction\n",
    "3. Chemical-Disease relationship extraction\n",
    "\n",
    "## Dataset:\n",
    "- 3,000 medical text examples\n",
    "- 80/10/10 train/validation/test split\n",
    "- **‚ö†Ô∏è CRITICAL**: Data is shuffled before splitting to ensure balanced task distribution\n",
    "- Weights & Biases tracking enabled\n",
    "\n",
    "## Important Note:\n",
    "**Data splitting MUST use `shuffle=True`** to prevent task imbalance. Without shuffling, all relationship extraction examples may cluster in validation/test sets, leading to poor model performance on the most important task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f6f14",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running this notebook!\n",
    "\n",
    "Required:\n",
    "- `HF_TOKEN`: Your Hugging Face token (needed to save models to HF Hub)\n",
    "\n",
    "Optional:\n",
    "- `WANDB_API_KEY`: Your Weights & Biases API key (for training tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e71987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment variables set\n",
      "  HF_TOKEN: ‚úì Set\n",
      "  WANDB_API_KEY: ‚úì Set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set your Hugging Face token (required for uploading to HF Hub)\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Set your Weights & Biases API key (optional, for training tracking)\n",
    "os.environ[\"WANDB_API_KEY\"] = \"d88df098d85360ac924ec2bf8dcf5520d745c411\"\n",
    "\n",
    "# Verify environment variables\n",
    "print(\"‚úì Environment variables set\")\n",
    "print(f\"  HF_TOKEN: {'‚úì Set' if os.environ.get('HF_TOKEN') and os.environ['HF_TOKEN'] != 'hf_YOUR_TOKEN_HERE' else '‚úó Not set - UPDATE THIS!'}\")\n",
    "print(f\"  WANDB_API_KEY: {'‚úì Set' if os.environ.get('WANDB_API_KEY') else '‚óã Optional (will use wandb login cache)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82d981",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9477574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers trl scikit-learn\n",
    "!pip install -q scipy sentencepiece protobuf wandb hf_transfer\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84c1fe",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5945c-9f62-4951-8f29-4bcf67268b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56ad90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A40\n",
      "CUDA version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2c7f4",
   "metadata": {},
   "source": [
    "## 2.5 Utility Functions for Data Processing\n",
    "\n",
    "Functions for normalization, deduplication, and text parsing to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51827af6",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Update `HF_USERNAME` with your Hugging Face username!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c8d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Base model: meta-llama/Llama-3.2-3B-Instruct\n",
      "  HF model ID: albyos/llama3-medical-ner-lora-20251106_140813\n",
      "  Training timestamp: 20251106_140813\n",
      "  LoRA rank: 16\n",
      "  Training epochs: 3\n",
      "  Effective batch size: 16\n",
      "  Data split seed: 136660 (reshuffled)\n"
     ]
    }
   ],
   "source": [
    "# Configuration Section\n",
    "from datetime import datetime\n",
    "\n",
    "HF_USERNAME = \"albyos\"  # Replace with your HF username\n",
    "\n",
    "# Generate timestamp for checkpoint naming\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "HF_MODEL_ID = f\"{HF_USERNAME}/llama3-medical-ner-lora-{TIMESTAMP}\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MODEL_NAME = BASE_MODEL  # Alias for consistency\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training Configuration\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Data Configuration\n",
    "TRAIN_SPLIT_RATIO = 0.9\n",
    "RANDOM_SEED = 42\n",
    "RESHUFFLE_SPLITS_EACH_RUN = True  # When True, create a fresh validation split every run\n",
    "SPLIT_SEED = random.randint(0, 1_000_000) if RESHUFFLE_SPLITS_EACH_RUN else RANDOM_SEED\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  Training timestamp: {TIMESTAMP}\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Training epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Data split seed: {SPLIT_SEED} ({'reshuffled' if RESHUFFLE_SPLITS_EACH_RUN else 'fixed'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc274801",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Authentication\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b6aa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logged in to Hugging Face\n"
     ]
    }
   ],
   "source": [
    "# Authenticate with Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token and hf_token != \"hf_YOUR_TOKEN_HERE\":\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úì Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"‚ö† HF_TOKEN not set. Please update Cell 3 before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb723a0b",
   "metadata": {},
   "source": [
    "## 4b. Weights & Biases Setup\n",
    "\n",
    "Initialize W&B to track training metrics, validation loss, and experiments.\n",
    "Get your API key from: https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740959ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mclemalb\u001b[0m (\u001b[33malberto-clemente\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logged in to Weights & Biases using WANDB_API_KEY\n"
     ]
    }
   ],
   "source": [
    "# Login to Weights & Biases\n",
    "wandb_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "if wandb_key and wandb_key != 'your_wandb_key_here':\n",
    "    wandb.login(key=wandb_key)\n",
    "    print('‚úì Logged in to Weights & Biases using WANDB_API_KEY')\n",
    "else:\n",
    "    print('‚ö† Warning: WANDB_API_KEY not set. Attempting to use cached login...')\n",
    "    try:\n",
    "        wandb.login()\n",
    "        print('‚úì Logged in to Weights & Biases using cached credentials')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö† Warning: Could not login to W&B: {e}')\n",
    "        print('  Run wandb.login() interactively or set WANDB_API_KEY environment variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51134743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251106_140823-5c3fdqrh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alberto-clemente/medical-ner-finetuning/runs/5c3fdqrh' target=\"_blank\">llama3-medical-ner-20251106_140813</a></strong> to <a href='https://wandb.ai/alberto-clemente/medical-ner-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alberto-clemente/medical-ner-finetuning' target=\"_blank\">https://wandb.ai/alberto-clemente/medical-ner-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alberto-clemente/medical-ner-finetuning/runs/5c3fdqrh' target=\"_blank\">https://wandb.ai/alberto-clemente/medical-ner-finetuning/runs/5c3fdqrh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Weights & Biases initialized\n",
      "  Project: medical-ner-finetuning\n",
      "  Run name: llama3-medical-ner-20251106_140813\n",
      "  Dashboard: https://wandb.ai\n"
     ]
    }
   ],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"medical-ner-finetuning\",\n",
    "    name=f\"llama3-medical-ner-{TIMESTAMP}\",\n",
    "    config={\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"lora_rank\": LORA_RANK,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Weights & Biases initialized\")\n",
    "print(f\"  Project: medical-ner-finetuning\")\n",
    "print(f\"  Run name: llama3-medical-ner-{TIMESTAMP}\")\n",
    "print(f\"  Dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7c1b6",
   "metadata": {},
   "source": [
    "## 5. Data Exploration\n",
    "\n",
    "Let's examine the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6c174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3000\n",
      "\n",
      "Sample structure:\n",
      "{\n",
      "  \"prompt\": \"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\\n\\nIn unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms/kg, was inhibited or reversed by nalozone, 0.2 to 2 mg/kg. The hypotensive effect of 100 mg/kg alpha-methyldopa was also partially reversed by naloxone. Naloxone alone did not affect either blood ...\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect the dataset\n",
    "# Load data\n",
    "with open('both_rel_instruct_all.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"\\nSample structure:\")\n",
    "print(json.dumps(data[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc23a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Distribution:\n",
      "  Chemical Extraction: 1000 (33.3%)\n",
      "  Disease Extraction: 1000 (33.3%)\n",
      "  Relationship Extraction: 1000 (33.3%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze task distribution\n",
    "# IMPORTANT: Check for \"influences between\" FIRST because those prompts also contain \"chemicals\" and \"diseases\"\n",
    "task_counts = {}\n",
    "for sample in data:\n",
    "    if \"influences between\" in sample['prompt']:\n",
    "        task = \"Relationship Extraction\"\n",
    "    elif \"chemicals mentioned\" in sample['prompt']:\n",
    "        task = \"Chemical Extraction\"\n",
    "    elif \"diseases mentioned\" in sample['prompt']:\n",
    "        task = \"Disease Extraction\"\n",
    "    else:\n",
    "        task = \"Other\"\n",
    "    \n",
    "    task_counts[task] = task_counts.get(task, 0) + 1\n",
    "\n",
    "print(\"Task Distribution:\")\n",
    "for task, count in task_counts.items():\n",
    "    print(f\"  {task}: {count} ({count/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03b8398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE: Chemical Extraction\n",
      "================================================================================\n",
      "Prompt:\n",
      "The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
      "\n",
      "In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms/kg, was inhib...\n",
      "\n",
      "Completion:\n",
      "- clonidine\n",
      "- nalozone\n",
      "- alpha-methyldopa\n",
      "- naloxone\n",
      "- Naloxone\n",
      "- [3H]-naloxone\n",
      "- [3H]-dihydroergocryptine\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE: Disease Extraction\n",
      "================================================================================\n",
      "Prompt:\n",
      "The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the diseases mentioned.\n",
      "\n",
      "In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms/kg, was inhibi...\n",
      "\n",
      "Completion:\n",
      "- hypertensive\n",
      "- hypotensive\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE: Relationship Extraction\n",
      "================================================================================\n",
      "Prompt:\n",
      "The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the influences between the chemicals and diseases mentioned.\n",
      "\n",
      "In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidi...\n",
      "\n",
      "Completion:\n",
      "- Chemical alpha-methyldopa influences disease hypotensive\n"
     ]
    }
   ],
   "source": [
    "# Show example from each task type\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE: Chemical Extraction\")\n",
    "print(\"=\"*80)\n",
    "chem_example = [s for s in data if \"chemicals mentioned\" in s['prompt'] and \"influences between\" not in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{chem_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{chem_example['completion']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: Disease Extraction\")\n",
    "print(\"=\"*80)\n",
    "disease_example = [s for s in data if \"diseases mentioned\" in s['prompt'] and \"influences between\" not in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{disease_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{disease_example['completion']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: Relationship Extraction\")\n",
    "print(\"=\"*80)\n",
    "rel_example = [s for s in data if \"influences between\" in s['prompt']][0]\n",
    "print(f\"Prompt:\\n{rel_example['prompt'][:300]}...\")\n",
    "print(f\"\\nCompletion:\\n{rel_example['completion']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e86457",
   "metadata": {},
   "source": [
    "## 6. Dataset Splitting\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL**: Using **stratified splitting** for guaranteed balanced task distribution!\n",
    "\n",
    "**Previous Issue**: Without shuffling, all relationship extraction examples ended up in validation/test sets, causing poor model performance.\n",
    "\n",
    "**New Solution**: Stratified splitting ensures EXACT proportions in all splits (not just probabilistic).\n",
    "\n",
    "Split into:\n",
    "- **80% Training** (2,400 samples) - for fine-tuning\n",
    "- **10% Validation** (300 samples) - for monitoring during training (W&B)\n",
    "- **10% Test** (300 samples) - for final evaluation after training\n",
    "\n",
    "**Guaranteed distribution in each split** (with stratification):\n",
    "- **Exactly 33.3%** Chemical extraction\n",
    "- **Exactly 33.3%** Disease extraction  \n",
    "- **Exactly 33.3%** Relationship extraction\n",
    "\n",
    "**Why stratified?**\n",
    "- `shuffle=True` gives ~33% ¬± 2-3% (probabilistic, good enough)\n",
    "- `stratify=labels` gives **exactly 33.3%** (guaranteed, better!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b378c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stratified splits to guarantee balanced task distribution...\n",
      "Original task distribution: {'chemical', 'disease', 'relationship'}\n",
      "‚úì Dataset split complete (seed=136660, stratified=True)\n",
      "  Train samples: 2400 (80.0%)\n",
      "  Validation samples: 300 (10.0%) - for training monitoring\n",
      "  Test samples: 300 (10.0%) - for final evaluation\n",
      "\n",
      "üìä Usage:\n",
      "  - Train: Used for fine-tuning\n",
      "  - Validation: Monitored during training (shown in W&B)\n",
      "  - Test: Used ONLY after training for final evaluation\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/val/test (80/10/10) with STRATIFIED sampling\n",
    "random.seed(SPLIT_SEED)\n",
    "\n",
    "# Helper function to classify task type for stratification\n",
    "def get_task_type(prompt):\n",
    "    \"\"\"Classify the task type based on prompt for stratification.\"\"\"\n",
    "    prompt_lower = prompt.lower()\n",
    "    if \"influences between\" in prompt_lower:\n",
    "        return \"relationship\"\n",
    "    elif \"chemicals mentioned\" in prompt_lower:\n",
    "        return \"chemical\"\n",
    "    elif \"diseases mentioned\" in prompt_lower:\n",
    "        return \"disease\"\n",
    "    return \"other\"\n",
    "\n",
    "# Create stratification labels for all data\n",
    "stratify_labels = [get_task_type(sample['prompt']) for sample in data]\n",
    "\n",
    "print(f\"Creating stratified splits to guarantee balanced task distribution...\")\n",
    "print(f\"Original task distribution: {set(stratify_labels)}\")\n",
    "\n",
    "# First split: 80% train, 20% temp (for val + test)\n",
    "# Using stratify= ensures EXACT proportions in both splits\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "    data,\n",
    "    stratify_labels,\n",
    "    test_size=0.2,  # 20% for validation + test\n",
    "    random_state=SPLIT_SEED,\n",
    "    stratify=stratify_labels  # ‚úÖ GUARANTEES exact 33.3% in both train and temp!\n",
    ")\n",
    "\n",
    "# Second split: split the 20% into 10% val, 10% test\n",
    "# Stratify again to ensure exact proportions in val and test\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    temp_data,\n",
    "    temp_labels,\n",
    "    test_size=0.5,  # 50% of 20% = 10% of total\n",
    "    random_state=SPLIT_SEED + 1,\n",
    "    stratify=temp_labels  # ‚úÖ GUARANTEES exact 33.3% in both val and test!\n",
    ")\n",
    "\n",
    "# Save splits\n",
    "with open('train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('validation.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('test.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"‚úì Dataset split complete (seed={SPLIT_SEED}, stratified=True)\")\n",
    "print(f\"  Train samples: {len(train_data)} ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"  Validation samples: {len(val_data)} ({len(val_data)/len(data)*100:.1f}%) - for training monitoring\")\n",
    "print(f\"  Test samples: {len(test_data)} ({len(test_data)/len(data)*100:.1f}%) - for final evaluation\")\n",
    "print(f\"\\nüìä Usage:\")\n",
    "print(f\"  - Train: Used for fine-tuning\")\n",
    "print(f\"  - Validation: Monitored during training (shown in W&B)\")\n",
    "print(f\"  - Test: Used ONLY after training for final evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "808f2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK DISTRIBUTION VERIFICATION (STRATIFIED SPLITTING)\n",
      "================================================================================\n",
      "\n",
      "Train (2400 samples):\n",
      "  ‚úÖ Chemical Extraction: 800 (33.3%)\n",
      "  ‚úÖ Disease Extraction: 800 (33.3%)\n",
      "  ‚úÖ Relationship Extraction: 800 (33.3%)\n",
      "\n",
      "Validation (300 samples):\n",
      "  ‚úÖ Chemical Extraction: 100 (33.3%)\n",
      "  ‚úÖ Disease Extraction: 100 (33.3%)\n",
      "  ‚úÖ Relationship Extraction: 100 (33.3%)\n",
      "\n",
      "Test (300 samples):\n",
      "  ‚úÖ Chemical Extraction: 100 (33.3%)\n",
      "  ‚úÖ Disease Extraction: 100 (33.3%)\n",
      "  ‚úÖ Relationship Extraction: 100 (33.3%)\n",
      "\n",
      "================================================================================\n",
      "DATA INTEGRITY CHECK\n",
      "================================================================================\n",
      "Train-Validation overlap: 0 samples ‚úÖ Perfect!\n",
      "Train-Test overlap: 0 samples ‚úÖ Perfect!\n",
      "Validation-Test overlap: 0 samples ‚úÖ Perfect!\n",
      "\n",
      "‚úÖ All splits are properly separated - no data leakage detected!\n",
      "‚úÖ Stratified splitting guarantees exact task proportions in all splits!\n"
     ]
    }
   ],
   "source": [
    "# Verify task distribution across splits\n",
    "def get_task_type_display(prompt):\n",
    "    \"\"\"Classify the task type based on prompt for display.\"\"\"\n",
    "    prompt_lower = prompt.lower()\n",
    "    if \"influences between\" in prompt_lower:\n",
    "        return \"Relationship Extraction\"\n",
    "    elif \"chemicals mentioned\" in prompt_lower:\n",
    "        return \"Chemical Extraction\"\n",
    "    elif \"diseases mentioned\" in prompt_lower:\n",
    "        return \"Disease Extraction\"\n",
    "    return \"Other\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK DISTRIBUTION VERIFICATION (STRATIFIED SPLITTING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split_name, split_data in [(\"Train\", train_data), (\"Validation\", val_data), (\"Test\", test_data)]:\n",
    "    task_counts = {}\n",
    "    for sample in split_data:\n",
    "        task = get_task_type_display(sample['prompt'])\n",
    "        task_counts[task] = task_counts.get(task, 0) + 1\n",
    "    \n",
    "    print(f\"\\n{split_name} ({len(split_data)} samples):\")\n",
    "    for task, count in sorted(task_counts.items()):\n",
    "        percentage = count / len(split_data) * 100\n",
    "        # Check if exactly balanced (within 0.5% tolerance)\n",
    "        is_perfect = abs(percentage - 33.33) < 0.5\n",
    "        marker = \"‚úÖ\" if is_perfect else \"‚ö†Ô∏è\"\n",
    "        print(f\"  {marker} {task}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Verify no data leakage between splits\n",
    "train_prompts = set(s['prompt'] for s in train_data)\n",
    "val_prompts = set(s['prompt'] for s in val_data)\n",
    "test_prompts = set(s['prompt'] for s in test_data)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATA INTEGRITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "overlap_train_val = len(train_prompts & val_prompts)\n",
    "overlap_train_test = len(train_prompts & test_prompts)\n",
    "overlap_val_test = len(val_prompts & test_prompts)\n",
    "\n",
    "print(f\"Train-Validation overlap: {overlap_train_val} samples {'‚úÖ Perfect!' if overlap_train_val == 0 else '‚ö†Ô∏è  WARNING - Data leakage detected!'}\")\n",
    "print(f\"Train-Test overlap: {overlap_train_test} samples {'‚úÖ Perfect!' if overlap_train_test == 0 else '‚ö†Ô∏è  WARNING - Data leakage detected!'}\")\n",
    "print(f\"Validation-Test overlap: {overlap_val_test} samples {'‚úÖ Perfect!' if overlap_val_test == 0 else '‚ö†Ô∏è  WARNING - Data leakage detected!'}\")\n",
    "\n",
    "if overlap_train_val == 0 and overlap_train_test == 0 and overlap_val_test == 0:\n",
    "    print(\"\\n‚úÖ All splits are properly separated - no data leakage detected!\")\n",
    "    print(\"‚úÖ Stratified splitting guarantees exact task proportions in all splits!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Data leakage detected! Splits contain overlapping samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e0e62",
   "metadata": {},
   "source": [
    "## 7. Data Formatting\n",
    "\n",
    "Format data into Llama 3 chat format with system, user, and assistant roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2711dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Example:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the influences between the chemicals and diseases mentioned.\n",
      "\n",
      "We describe a case of transient neurological deficit that occurred after unilateral spinal anaesthesia with 8 mg of 1% hyper...\n"
     ]
    }
   ],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format data into Llama 3 chat format.\"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{sample['prompt']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{sample['completion']}<|eot_id|>\"\"\"\n",
    "\n",
    "# Test formatting\n",
    "formatted_example = format_instruction(train_data[0])\n",
    "print(\"Formatted Example:\")\n",
    "print(formatted_example[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7aca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Datasets formatted:\n",
      "  Train: 2400 samples\n",
      "  Validation: 300 samples\n",
      "  Test: 300 samples\n"
     ]
    }
   ],
   "source": [
    "# Format all data\n",
    "train_formatted = [{\"text\": format_instruction(sample)} for sample in train_data]\n",
    "val_formatted = [{\"text\": format_instruction(sample)} for sample in val_data]\n",
    "test_formatted = [{\"text\": format_instruction(sample)} for sample in test_data]\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_formatted)\n",
    "val_dataset = Dataset.from_list(val_formatted)\n",
    "test_dataset = Dataset.from_list(test_formatted)\n",
    "\n",
    "print(f\"‚úì Datasets formatted:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7616fb",
   "metadata": {},
   "source": [
    "## 8. Load Model and Tokenizer\n",
    "\n",
    "Load Llama 3.2 3B with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa3981a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Quantization config created (4-bit NF4)\n"
     ]
    }
   ],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Quantization config created (4-bit NF4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b55b7-2f21-4dab-8257-5c919f7cf1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hf_transfer\n",
    "import hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30c092e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenizer loaded: meta-llama/Llama-3.2-3B-Instruct\n",
      "  Vocab size: 128256\n",
      "  PAD token: <|eot_id|>\n",
      "  EOS token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd890d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... (this may take a few minutes)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bcc2d3044b4b65a0765517b753b75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Base model loaded: meta-llama/Llama-3.2-3B-Instruct\n",
      "  Model size: 2.20 GB\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "print(\"Loading model... (this may take a few minutes)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62ceb88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model prepared for k-bit training\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"‚úì Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b2d6c",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA\n",
    "\n",
    "Apply Low-Rank Adaptation for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57b69361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LoRA configuration:\n",
      "  Rank (r): 16\n",
      "  Alpha: 32\n",
      "  Dropout: 0.05\n",
      "  Target modules: 7\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,                   # LoRA rank\n",
    "    lora_alpha=LORA_ALPHA,         # LoRA alpha (scaling)\n",
    "    target_modules=[               # Layers to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,             # Dropout for regularization\n",
    "    bias=\"none\",                   # No bias training\n",
    "    task_type=\"CAUSAL_LM\"          # Causal language modeling\n",
    ")\n",
    "\n",
    "print(f\"‚úì LoRA configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {len(lora_config.target_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd0e2cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LoRA applied to model\n",
      "\n",
      "Trainable parameters:\n",
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úì LoRA applied to model\")\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665501b",
   "metadata": {},
   "source": [
    "## 10. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab6201d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7fe00be42744ea9baafe0e361495b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train set:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09567f17c32d4849aad454c9bb07057d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation set:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Train set tokenized: 2400 samples\n",
      "‚úì Validation set tokenized: 300 samples\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the texts.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set tokenized: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úì Validation set tokenized: {len(tokenized_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2941de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data collator created\n"
     ]
    }
   ],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚úì Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417fc9",
   "metadata": {},
   "source": [
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9cb39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training configuration:\n",
      "  Epochs: 3\n",
      "  Batch size (per device): 4\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 16\n",
      "  Learning rate: 0.0002\n",
      "  Checkpoint frequency: Every 50 steps\n",
      "  Base HF model ID: albyos/llama3-medical-ner-lora-20251106_140813\n",
      "  ‚ö†Ô∏è Checkpoints will be pushed to HF with timestamp suffix\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./llama3-medical-ner-lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluate every 50 steps\n",
    "    \n",
    "    # Checkpointing - Save every 50 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Checkpoint every 50 steps\n",
    "    save_total_limit=None,  # Keep all checkpoints (will push to HF with unique names)\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=True,\n",
    "    \n",
    "    # Hugging Face Hub - Disable default push (we'll use custom callback)\n",
    "    push_to_hub=False,  # Custom callback will handle timestamped uploads\n",
    "    \n",
    "    # Misc\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",  # Enable Weights & Biases logging\n",
    "    run_name=f\"llama3-medical-ner-{TIMESTAMP}\",  # W&B run name\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Checkpoint frequency: Every {training_args.save_steps} steps\")\n",
    "print(f\"  Base HF model ID: {HF_MODEL_ID}\")\n",
    "print(f\"  ‚ö†Ô∏è Checkpoints will be pushed to HF with timestamp suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3b101",
   "metadata": {},
   "source": [
    "## 11b. Custom Checkpoint Upload Callback\n",
    "\n",
    "This callback will automatically push each checkpoint to Hugging Face Hub with a unique timestamped name every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90a78c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Checkpoint upload callback initialized\n",
      "  Checkpoints will be uploaded to: albyos/llama3-medical-ner-checkpoint-<step>-<timestamp>\n",
      "  Upload frequency: Every 50 steps\n",
      "  Local deletion: Enabled (checkpoints deleted after upload)\n",
      "  Auto-create repos: Enabled (repos will be created automatically)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "from huggingface_hub import HfApi\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "class CheckpointUploadCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to upload checkpoints to Hugging Face Hub with timestamped names.\n",
    "    Deletes local checkpoints after successful upload to save disk space.\n",
    "    \n",
    "    Each checkpoint will be saved with format:\n",
    "    {HF_USERNAME}/llama3-medical-ner-checkpoint-{step}-{timestamp}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_id, hf_username, delete_after_upload=True):\n",
    "        self.base_model_id = base_model_id\n",
    "        self.hf_username = hf_username\n",
    "        self.delete_after_upload = delete_after_upload\n",
    "        self.api = HfApi()\n",
    "        \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Called when a checkpoint is saved.\n",
    "        Uploads the checkpoint to HF Hub with a timestamped name, then deletes local copy.\n",
    "        \"\"\"\n",
    "        # Get the checkpoint directory that was just saved\n",
    "        checkpoint_dir = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n",
    "        \n",
    "        if not Path(checkpoint_dir).exists():\n",
    "            print(f\"‚ö†Ô∏è Checkpoint directory not found: {checkpoint_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Create timestamped model ID\n",
    "        checkpoint_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_model_id = f\"{self.hf_username}/llama3-medical-ner-checkpoint-{state.global_step}-{checkpoint_timestamp}\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üì§ Uploading checkpoint to Hugging Face Hub\")\n",
    "        print(f\"   Step: {state.global_step}\")\n",
    "        print(f\"   Model ID: {checkpoint_model_id}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Create repository if it doesn't exist\n",
    "            try:\n",
    "                self.api.create_repo(\n",
    "                    repo_id=checkpoint_model_id,\n",
    "                    repo_type=\"model\",\n",
    "                    exist_ok=True,\n",
    "                    private=False\n",
    "                )\n",
    "                print(f\"‚úì Repository created/verified: {checkpoint_model_id}\")\n",
    "            except Exception as create_error:\n",
    "                print(f\"‚ö†Ô∏è  Repository creation note: {create_error}\")\n",
    "            \n",
    "            # Upload the checkpoint folder to HF Hub\n",
    "            self.api.upload_folder(\n",
    "                folder_path=checkpoint_dir,\n",
    "                repo_id=checkpoint_model_id,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=f\"Checkpoint at step {state.global_step}\",\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Checkpoint uploaded successfully!\")\n",
    "            print(f\"   URL: https://huggingface.co/{checkpoint_model_id}\")\n",
    "            \n",
    "            # Delete local checkpoint after successful upload\n",
    "            if self.delete_after_upload:\n",
    "                try:\n",
    "                    shutil.rmtree(checkpoint_dir)\n",
    "                    print(f\"üóëÔ∏è  Local checkpoint deleted to save disk space\")\n",
    "                except Exception as delete_error:\n",
    "                    print(f\"‚ö†Ô∏è  Could not delete local checkpoint: {delete_error}\")\n",
    "            \n",
    "            print()  # Empty line for readability\n",
    "            \n",
    "            # Log to wandb if available\n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\n",
    "                    \"checkpoint_step\": state.global_step,\n",
    "                    \"checkpoint_url\": f\"https://huggingface.co/{checkpoint_model_id}\"\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to upload checkpoint: {e}\")\n",
    "            print(f\"   Checkpoint saved locally at: {checkpoint_dir}\\n\")\n",
    "\n",
    "# Initialize the callback with automatic deletion after upload\n",
    "checkpoint_upload_callback = CheckpointUploadCallback(\n",
    "    base_model_id=HF_MODEL_ID,\n",
    "    hf_username=HF_USERNAME,\n",
    "    delete_after_upload=True  # Delete local checkpoints after upload\n",
    ")\n",
    "\n",
    "print(f\"‚úì Checkpoint upload callback initialized\")\n",
    "print(f\"  Checkpoints will be uploaded to: {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"  Upload frequency: Every {training_args.save_steps} steps\")\n",
    "print(f\"  Local deletion: Enabled (checkpoints deleted after upload)\")\n",
    "print(f\"  Auto-create repos: Enabled (repos will be created automatically)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cadccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHECKPOINT UPLOAD PREVIEW\n",
      "================================================================================\n",
      "\n",
      "Training Configuration:\n",
      "  Total samples: 2400\n",
      "  Batch size: 4\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 16\n",
      "  Epochs: 3\n",
      "  Estimated total steps: ~450\n",
      "\n",
      "Checkpoint Configuration:\n",
      "  Frequency: Every 50 steps\n",
      "  Expected checkpoints: ~9\n",
      "  Local storage: ./llama3-medical-ner-lora/checkpoint-<step>/\n",
      "\n",
      "Hugging Face Upload:\n",
      "  Format: albyos/llama3-medical-ner-checkpoint-<step>-<timestamp>\n",
      "\n",
      "Example checkpoint names:\n",
      "  1. albyos/llama3-medical-ner-checkpoint-50-20251106_140953\n",
      "  2. albyos/llama3-medical-ner-checkpoint-100-20251106_140953\n",
      "  3. albyos/llama3-medical-ner-checkpoint-150-20251106_140953\n",
      "  4. albyos/llama3-medical-ner-checkpoint-200-20251106_140953\n",
      "  ... (~5 more checkpoints)\n",
      "\n",
      "Final model:\n",
      "  albyos/llama3-medical-ner-lora-final-<timestamp>\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Preview expected checkpoint uploads\n",
    "total_steps_estimate = (len(train_data) // (BATCH_SIZE * GRADIENT_ACCUMULATION)) * NUM_EPOCHS\n",
    "checkpoint_count = total_steps_estimate // 50\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHECKPOINT UPLOAD PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Total samples: {len(train_data)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Estimated total steps: ~{total_steps_estimate}\")\n",
    "print(f\"\\nCheckpoint Configuration:\")\n",
    "print(f\"  Frequency: Every 50 steps\")\n",
    "print(f\"  Expected checkpoints: ~{checkpoint_count}\")\n",
    "print(f\"  Local storage: ./llama3-medical-ner-lora/checkpoint-<step>/\")\n",
    "print(f\"\\nHugging Face Upload:\")\n",
    "print(f\"  Format: {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"\\nExample checkpoint names:\")\n",
    "for i, step in enumerate(range(50, min(250, total_steps_estimate), 50), 1):\n",
    "    example_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"  {i}. {HF_USERNAME}/llama3-medical-ner-checkpoint-{step}-{example_time}\")\n",
    "if checkpoint_count > 4:\n",
    "    print(f\"  ... (~{checkpoint_count - 4} more checkpoints)\")\n",
    "    \n",
    "print(f\"\\nFinal model:\")\n",
    "print(f\"  {HF_USERNAME}/llama3-medical-ner-lora-final-<timestamp>\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a993c",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9b88564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Trainer initialized\n",
      "‚úì Expected training steps: ~450\n",
      "‚úì Expected checkpoints: ~9\n",
      "‚úì Checkpoint upload callback enabled\n",
      "‚úì Early stopping enabled (patience = 3 evaluations)\n",
      "\n",
      "üìã Checkpoint naming format:\n",
      "   albyos/llama3-medical-ner-checkpoint-<step>-<timestamp>\n",
      "\n",
      "   Example: albyos/llama3-medical-ner-checkpoint-50-20251104_143022\n"
     ]
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[checkpoint_upload_callback],  # Add custom checkpoint upload callback\n",
    ")\n",
    "\n",
    "# Configure early stopping to prevent overfitting\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0))\n",
    "\n",
    "# Calculate training steps\n",
    "total_steps = (len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n",
    "\n",
    "print(f\"‚úì Trainer initialized\")\n",
    "print(f\"‚úì Expected training steps: ~{total_steps}\")\n",
    "print(f\"‚úì Expected checkpoints: ~{max(1, total_steps // training_args.save_steps)}\")\n",
    "print(f\"‚úì Checkpoint upload callback enabled\")\n",
    "print(\"‚úì Early stopping enabled (patience = 3 evaluations)\")\n",
    "print(f\"\\nüìã Checkpoint naming format:\")\n",
    "print(f\"   {HF_USERNAME}/llama3-medical-ner-checkpoint-<step>-<timestamp>\")\n",
    "print(f\"\\n   Example: {HF_USERNAME}/llama3-medical-ner-checkpoint-50-20251104_143022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b853b4",
   "metadata": {},
   "source": [
    "## 13. Start Training\n",
    "\n",
    "‚ö†Ô∏è **This will take 2-3 hours on an A100 GPU**\n",
    "\n",
    "The training will:\n",
    "- **Save checkpoints every 50 steps** to local disk\n",
    "- **Upload each checkpoint to Hugging Face Hub** with timestamped names\n",
    "  - Format: `{username}/llama3-medical-ner-checkpoint-{step}-{timestamp}`\n",
    "  - Example: `albyos/llama3-medical-ner-checkpoint-50-20251104_143022`\n",
    "- Evaluate on validation set every 50 steps\n",
    "- Save the best model based on validation loss\n",
    "- Log all metrics to Weights & Biases\n",
    "\n",
    "**Checkpoint URLs will be printed during training and logged to W&B.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39341273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "This may take 2-3 hours on A100 GPU...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 44:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.508600</td>\n",
       "      <td>1.456114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.378200</td>\n",
       "      <td>1.357059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.298000</td>\n",
       "      <td>1.258915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.092600</td>\n",
       "      <td>1.166320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.975300</td>\n",
       "      <td>1.064335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.983286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.654100</td>\n",
       "      <td>0.940445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.909123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.662900</td>\n",
       "      <td>0.902038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 50\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-50-20251106_141453\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-50-20251106_141453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9958affb7b44a459c79a4938c717ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77675e18ae4e4209b767779a35e67d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-50-20251106_141453\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 100\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-100-20251106_141946\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-100-20251106_141946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e5a33111f546849f4f4f7b52994af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043a7e5fce3748cdaf2663764293f010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-100-20251106_141946\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 150\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-150-20251106_142438\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-150-20251106_142438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d46234c254acbb1f29ba7281d04ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54376912a65044b492a22b69ae10d645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-150-20251106_142438\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 200\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-200-20251106_142933\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-200-20251106_142933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b007b20e1a7e489b8c5e212f25a327ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9cd66f03b34f1eacea3755b14719d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-200-20251106_142933\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 250\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-250-20251106_143431\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-250-20251106_143431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1396d8490b4c2fa184d0e684119736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da56d21a9eec40a1b7d1e2d53b73ae68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-250-20251106_143431\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 300\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-300-20251106_143920\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-300-20251106_143920\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b57c93e4b5343bab5de98d63c6af605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c570b13570014bf68b4b98c066f0f248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-300-20251106_143920\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 350\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-350-20251106_144415\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-350-20251106_144415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d03979ce444b838bfb019152000312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15aa3275bf7040e7abd72c6dbc882b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-350-20251106_144415\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 400\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-400-20251106_144911\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-400-20251106_144911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c9789ae79849e2ac96f19a2f9b1d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42327c5763343c398707864053fe232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-400-20251106_144911\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì§ Uploading checkpoint to Hugging Face Hub\n",
      "   Step: 450\n",
      "   Model ID: albyos/llama3-medical-ner-checkpoint-450-20251106_145403\n",
      "================================================================================\n",
      "\n",
      "‚úì Repository created/verified: albyos/llama3-medical-ner-checkpoint-450-20251106_145403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54436a0da5cf40b490da7a5df0fccf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5a95cb451c42b79320bcdce8c04c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./llama3-medical-ner-lora/checkpoint-450/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint uploaded successfully!\n",
      "   URL: https://huggingface.co/albyos/llama3-medical-ner-checkpoint-450-20251106_145403\n",
      "üóëÔ∏è  Local checkpoint deleted to save disk space\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=1.0608325248294406, metrics={'train_runtime': 2649.155, 'train_samples_per_second': 2.718, 'train_steps_per_second': 0.17, 'total_flos': 6.109447492480205e+16, 'train_loss': 1.0608325248294406, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take 2-3 hours on A100 GPU...\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b3b39",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93373e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "\n",
    "print(f\"‚úì Model saved to: ./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push final model to Hugging Face Hub with timestamped name\n",
    "print(\"Pushing final model to Hugging Face Hub...\")\n",
    "\n",
    "# Create final model ID with timestamp\n",
    "final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_model_id = f\"{HF_USERNAME}/llama3-medical-ner-lora-final-{final_timestamp}\"\n",
    "\n",
    "try:\n",
    "    # Push the final model\n",
    "    model.push_to_hub(\n",
    "        final_model_id,\n",
    "        commit_message=\"Training complete - final model\"\n",
    "    )\n",
    "    tokenizer.push_to_hub(\n",
    "        final_model_id,\n",
    "        commit_message=\"Training complete - final tokenizer\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Final model pushed successfully!\")\n",
    "    print(f\"   Model ID: {final_model_id}\")\n",
    "    print(f\"   URL: https://huggingface.co/{final_model_id}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"final_model_url\": f\"https://huggingface.co/{final_model_id}\",\n",
    "            \"final_model_id\": final_model_id\n",
    "        })\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Failed to push to hub: {e}\")\n",
    "    print(\"  Final model saved locally at: ./final_model\")\n",
    "    print(f\"  You can manually push later using:\")\n",
    "    print(f\"    model.push_to_hub('{final_model_id}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed8e1e",
   "metadata": {},
   "source": [
    "## 15. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract losses\n",
    "train_loss = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "eval_loss = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_loss, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Evaluation Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training metrics plotted and saved to: training_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training steps: {len(train_loss)}\")\n",
    "print(f\"Final training loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {eval_loss[-1]:.4f}\")\n",
    "print(f\"Best validation loss: {min(eval_loss):.4f}\")\n",
    "print(f\"Loss reduction: {((train_loss[0] - train_loss[-1]) / train_loss[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdcba1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Training is complete! Your model has been saved.\n",
    "\n",
    "**To evaluate your model:**\n",
    "1. Open `Medical_NER_Evaluation.ipynb`\n",
    "2. Run the evaluation on the test set\n",
    "3. Test custom examples\n",
    "\n",
    "**Model locations:**\n",
    "- Local: `./final_model`\n",
    "- HuggingFace Hub: Check the output above for your model URL\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
