{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15813e67",
   "metadata": {},
   "source": [
    "# üöÄ RunPod GPU Setup\n",
    "\n",
    "**This notebook is optimized for RunPod GPU pods with NVIDIA GPUs**\n",
    "\n",
    "## Quick Start on RunPod:\n",
    "\n",
    "1. **Launch a GPU Pod** (RTX 3090, 4090, or A5000 recommended)\n",
    "2. **Upload this notebook** to the pod\n",
    "3. **Upload test data** (`test_run_20251106.jsonl`) to `/workspace/data/`\n",
    "4. **Run cells in order** - evaluation should complete in ~5-10 minutes\n",
    "\n",
    "## Expected Performance:\n",
    "- **GPU**: RTX 3090/4090 ‚Üí ~0.5-1 sec/sample (~5 min total)\n",
    "- **GPU**: RTX A5000 ‚Üí ~1-2 sec/sample (~10 min total)\n",
    "- **Full evaluation**: 300 samples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4828c",
   "metadata": {},
   "source": [
    "# Medical NER Model Evaluation\n",
    "\n",
    "This notebook evaluates the fine-tuned Llama 3.2 3B medical NER model.\n",
    "\n",
    "## ‚úÖ DATASET VERIFIED & READY FOR EVALUATION\n",
    "\n",
    "**Current Dataset Distribution** (from `both_rel_instruct_all.jsonl`):\n",
    "- **1,000 Chemical extraction** examples (25%)\n",
    "- **2,000 Disease extraction** examples (50%) ‚ö†Ô∏è Intentionally 2x more\n",
    "- **1,000 Relationship extraction** examples (25%)\n",
    "\n",
    "**Data Splits Status**: ‚úÖ Properly stratified using `stratify=` parameter\n",
    "- Training (2,400): 25% chemical, 50% disease, 25% relationship\n",
    "- Validation (300+): 25% chemical, 50% disease, 25% relationship\n",
    "- Test (300+): 25% chemical, 50% disease, 25% relationship\n",
    "\n",
    "**Why Disease is 2x more**:\n",
    "- The original dataset has twice as many disease extraction examples\n",
    "- Stratified splitting preserves this 25/50/25 distribution\n",
    "- This appears intentional for better disease NER performance\n",
    "- All splits are properly balanced relative to the source data\n",
    "\n",
    "**Next Steps**:\n",
    "1. ‚úÖ Training data is properly split with stratification\n",
    "2. ‚úÖ No data leakage between train/val/test\n",
    "3. ‚úÖ Update `HF_MODEL_ID` below with your trained model ID\n",
    "4. ‚úÖ Run this evaluation notebook on the balanced test set\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites:\n",
    "1. Complete training in `Medical_NER_Fine_Tuning.ipynb` (uses stratified splits!)\n",
    "2. Model saved to `./final_model` or uploaded to HuggingFace Hub\n",
    "3. Test data available in `notebooks/test.jsonl` or `../data/test.jsonl`\n",
    "\n",
    "## Evaluation Tasks:\n",
    "1. Load the fine-tuned model\n",
    "2. Evaluate on test set (25% chem, 50% disease, 25% relationship)\n",
    "3. Calculate precision, recall, F1 scores per task type\n",
    "4. Test on custom medical texts\n",
    "5. Analyze errors and false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d2f11",
   "metadata": {},
   "source": [
    "## 0. Environment Variables Setup\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Set your credentials before running the notebook!\n",
    "\n",
    "**Note**: `hf_transfer` is enabled for faster downloads from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enable hf_transfer for faster downloads from HuggingFace Hub\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# HuggingFace Token (required to download your model from Hub)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"HF_TOKEN not found in environment variables\")\n",
    "    hf_token = getpass(\"Enter your HuggingFace token: \")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "else:\n",
    "    print(\"‚úì HF_TOKEN loaded from environment\")\n",
    "\n",
    "# Weights & Biases API Key (optional - only if tracking evaluation metrics)\n",
    "# Get your key from: https://wandb.ai/authorize\n",
    "wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if wandb_key:\n",
    "    print(\"‚úì WANDB_API_KEY loaded from environment\")\n",
    "else:\n",
    "    print(\"‚Ñπ WANDB_API_KEY not set (optional)\")\n",
    "\n",
    "print(\"\\n‚úì Environment variables configured\")\n",
    "print(f\"  HF_HUB_ENABLE_HF_TRANSFER: {os.getenv('HF_HUB_ENABLE_HF_TRANSFER')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1d2ca",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch and other required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface-hub tokenizers hf-transfer\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")\n",
    "print(\"  - transformers (HuggingFace models)\")\n",
    "print(\"  - peft (LoRA adapters)\")\n",
    "print(\"  - accelerate (device management)\")\n",
    "print(\"  - bitsandbytes (quantization)\")\n",
    "print(\"  - hf-transfer (fast downloads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bce21",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e80ce",
   "metadata": {},
   "source": [
    "## 0) Reusable Utilities\n",
    "\n",
    "These utility functions provide text normalization, hashing, parsing, and validation for the evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904898f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Utilities: normalization, hashing, parsing =====\n",
    "import re, json, hashlib\n",
    "from collections import Counter\n",
    "\n",
    "def dehyphenate(s: str) -> str:\n",
    "    # Join words broken across lines with hyphens + whitespace\n",
    "    return re.sub(r\"(\\w+)-\\s+(\\w+)\", r\"\\1\\2\", s)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = dehyphenate(s or \"\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\u00A0\\t\\r\\n]+\", \" \", s)     # spaces/newlines\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def prompt_hash(prompt: str) -> str:\n",
    "    return hashlib.md5(normalize_text(prompt).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def parse_bullets(text: str):\n",
    "    items = []\n",
    "    for line in (text or \"\").splitlines():\n",
    "        m = re.match(r\"^\\s*[-*]\\s*(.+?)\\s*$\", line)\n",
    "        if m:\n",
    "            items.append(m.group(1))\n",
    "    return items\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    # Keep hyphens intact (e.g., \"type-2 diabetes\" stays \"type-2 diabetes\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # Only normalize whitespace\n",
    "    s = re.sub(r\"[\\.,;:]+$\", \"\", s).strip()\n",
    "    return s\n",
    "\n",
    "def in_text(item: str, text: str) -> bool:\n",
    "    \"\"\"Check if item appears in text using word boundaries to avoid partial matches.\"\"\"\n",
    "    item_norm = normalize_item(item)\n",
    "    text_norm = normalize_text(text)\n",
    "    # Use word boundaries to avoid matching \"aspirin\" in \"aspirinate\"\n",
    "    pattern = r'\\b' + re.escape(item_norm) + r'\\b'\n",
    "    return bool(re.search(pattern, text_norm))\n",
    "\n",
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40a094",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **Update these paths** to match your model location!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec819c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Update with YOUR HuggingFace model ID\n",
    "# Find it at: https://huggingface.co/your-username\n",
    "# Format: \"your-username/llama3-medical-ner-lora-YYYYMMDD_HHMMSS\"\n",
    "HF_MODEL_ID = \"albyos/llama3-medical-ner-checkpoint-450-20251106_145403\"  # ‚Üê UPDATE THIS!\n",
    "\n",
    "# Alternative: Use local model if you prefer\n",
    "USE_HF_HUB = True  # Set to False to use local ../final_model\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "LOCAL_MODEL_PATH = PROJECT_ROOT / \"final_model\"\n",
    "\n",
    "ADAPTER_PATH = HF_MODEL_ID if USE_HF_HUB else str(LOCAL_MODEL_PATH)\n",
    "\n",
    "# Data configuration\n",
    "# For RunPod: Upload test data to /workspace/data/test_run_20251106.jsonl\n",
    "# For local: Use your local path\n",
    "try:\n",
    "    # Try RunPod/workspace path first\n",
    "    TEST_DATA_PATH = Path(\"test_run_20251106.jsonl\")\n",
    "    if not TEST_DATA_PATH.exists():\n",
    "        # Fallback to local path\n",
    "        TEST_DATA_PATH = Path.cwd().parent / \"data\" / \"test_run_20251106.jsonl\"\n",
    "        if not TEST_DATA_PATH.exists():\n",
    "            # Another fallback\n",
    "            TEST_DATA_PATH = Path(\"/Users/alberto/projects/courses/building_llms/ch_10_fine_tuning/data/test_run_20251106.jsonl\")\n",
    "except Exception:\n",
    "    TEST_DATA_PATH = Path(\"test_run_20251106.jsonl\")\n",
    "\n",
    "# Verify test data exists\n",
    "if not TEST_DATA_PATH.exists():\n",
    "    print(f\"‚ùå Test data not found at {TEST_DATA_PATH}\")\n",
    "    print(f\"üí° RunPod: Upload to /workspace/data/test_run_20251106.jsonl\")\n",
    "    print(f\"üí° Local: Place in ../data/test_run_20251106.jsonl\")\n",
    "    raise FileNotFoundError(f\"Test data file not found: {TEST_DATA_PATH}\")\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  Adapter source: {'HuggingFace Hub' if USE_HF_HUB else 'Local filesystem'}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Test data: {TEST_DATA_PATH}\")\n",
    "print(f\"  Test data exists: {TEST_DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcae3b",
   "metadata": {},
   "source": [
    "## 4. Authenticate with Hugging Face\n",
    "\n",
    "Log into Hugging Face to download the LoRA adapter when `USE_HF_HUB` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace Hub to access your model\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"‚ùå HF_TOKEN not found in environment\")\n",
    "    print(\"   Please run cell #3 first to set your HF token\")\n",
    "    raise ValueError(\"HF_TOKEN is required to download model from HuggingFace Hub\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=hf_token, add_to_git_credential=True)\n",
    "\n",
    "print(\"‚úì Logged into Hugging Face Hub\")\n",
    "print(f\"  Will load model from: {HF_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6265a9c",
   "metadata": {},
   "source": [
    "## 5. Load the Fine-Tuned Model\n",
    "\n",
    "Load the base model and attach the LoRA adapter from either Hugging Face Hub or your local filesystem.\n",
    "\n",
    "**Note**: Using `hf_transfer` for faster downloads from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure hf_transfer is enabled for faster downloads\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING FINE-TUNED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nLoading base model: {BASE_MODEL}...\")\n",
    "\n",
    "#Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Check for GPU support (optimized for RunPod/CUDA)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"üöÄ NVIDIA GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"üöÄ Apple Silicon GPU (MPS) detected\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"‚ö†Ô∏è  No GPU detected, using CPU (very slow)\")\n",
    "\n",
    "# Load base model with GPU acceleration\n",
    "# On RunPod: Uses CUDA with float16 for optimal performance\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Automatically distribute model across available GPUs\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Base model loaded: {BASE_MODEL}\")\n",
    "print(f\"  Device: {device.upper()}\")\n",
    "print(f\"  Precision: {base_model.dtype}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"  GPU Memory Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Load LoRA adapter from HuggingFace Hub or local path\n",
    "print(f\"\\nLoading LoRA adapter from: {ADAPTER_PATH}...\")\n",
    "print(f\"  Using hf_transfer for faster downloads...\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_PATH,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úì Fine-tuned model loaded successfully!\")\n",
    "print(f\"  Base: {BASE_MODEL}\")\n",
    "print(f\"  LoRA adapter: {ADAPTER_PATH}\")\n",
    "print(f\"  Source: {'HuggingFace Hub' if USE_HF_HUB else 'Local filesystem'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Deterministic generation for evaluation =====\n",
    "def generate_response(prompt_text, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generate a response for a given prompt - DETERMINISTIC for precision.\n",
    "    \n",
    "    Key changes from training version:\n",
    "    - do_sample=False: Greedy decoding prevents hallucinations\n",
    "    - temperature=0.0: No randomness\n",
    "    - Removes sampling parameters (top_k, top_p)\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical NER expert. Extract the requested entities from medical texts accurately.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy decoding (deterministic)\n",
    "            temperature=0.0,  # No randomness\n",
    "            top_p=1.0,  # Not used with do_sample=False, but set for clarity\n",
    "            num_beams=1,  # No beam search (faster)\n",
    "            repetition_penalty=1.15,  # Slight penalty to avoid repetition\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,  # Enable KV cache for faster generation\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    if \"assistant\\n\\n\" in response:\n",
    "        response = response.split(\"assistant\\n\\n\")[-1]\n",
    "    elif \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"‚úì Deterministic inference function ready\")\n",
    "print(\"  Generation parameters:\")\n",
    "print(\"    - do_sample: False (greedy decoding)\")\n",
    "print(\"    - temperature: 0.0 (no randomness)\")\n",
    "print(\"    - max_new_tokens: 128 (optimal for NER tasks)\")\n",
    "print(\"    - use_cache: True (KV cache for speed)\")\n",
    "print(\"\\n  Benefits:\")\n",
    "print(\"    - Reproducible results (same input ‚Üí same output)\")\n",
    "print(\"    - Reduced hallucinations and false positives\")\n",
    "print(\"    - Faster inference (no sampling overhead)\")\n",
    "print(\"\\n  Expected speed on RunPod GPU:\")\n",
    "print(\"    - RTX 3090/4090: ~0.5-1 second per sample\")\n",
    "print(\"    - RTX A5000: ~1-2 seconds per sample\")\n",
    "print(\"    - Full evaluation (300 samples): ~5-10 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f4971",
   "metadata": {},
   "source": [
    "## 6. Task Classification and Post-Filters\n",
    "\n",
    "These functions classify tasks from prompts and filter predictions to ensure they appear in the source text, reducing false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad71a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Task classification and post-filters =====\n",
    "\n",
    "# Task classifier\n",
    "def task_from_prompt(prompt: str) -> str:\n",
    "    \"\"\"Classify task type from prompt text.\"\"\"\n",
    "    p = normalize_text(prompt)\n",
    "    if \"list of extracted chemicals\" in p: return \"chemicals\"\n",
    "    if \"list of extracted diseases\"  in p: return \"diseases\"\n",
    "    if \"list of extracted influences\" in p: return \"influences\"\n",
    "    # Fallback patterns\n",
    "    if \"chemicals mentioned\" in p: return \"chemicals\"\n",
    "    if \"diseases mentioned\" in p: return \"diseases\"\n",
    "    if \"influences between\" in p: return \"influences\"\n",
    "    return \"other\"\n",
    "\n",
    "# Entity extraction and filtering\n",
    "def extract_list_from_generation(gen_text):\n",
    "    \"\"\"Parse bullets from the model output.\"\"\"\n",
    "    return parse_bullets(gen_text)\n",
    "\n",
    "def filter_items_against_text(pred_items, prompt_text):\n",
    "    \"\"\"Keep only items that appear in the source text (after normalization). Deduplicate.\"\"\"\n",
    "    keep = []\n",
    "    for it in pred_items:\n",
    "        if in_text(it, prompt_text):\n",
    "            keep.append(normalize_item(it))\n",
    "    return unique_preserve_order(keep)\n",
    "\n",
    "# Influences/Relationships - parse as pairs\n",
    "def parse_pairs(gen_text):\n",
    "    \"\"\"Parse 'chemical | disease' pairs from generation output.\"\"\"\n",
    "    pairs = []\n",
    "    for line in parse_bullets(gen_text):\n",
    "        parts = [p.strip() for p in line.split(\"|\")]\n",
    "        if len(parts)==2:\n",
    "            pairs.append(tuple(parts))\n",
    "    return unique_preserve_order(pairs)\n",
    "\n",
    "def filter_pairs_against_text(pairs, prompt_text):\n",
    "    \"\"\"Keep the pair only if BOTH sides appear in the prompt.\"\"\"\n",
    "    kept = []\n",
    "    for chem, dis in pairs:\n",
    "        if in_text(chem, prompt_text) and in_text(dis, prompt_text):\n",
    "            kept.append((normalize_item(chem), normalize_item(dis)))\n",
    "    # Deduplicate normalized pairs\n",
    "    seen=set(); out=[]\n",
    "    for p in kept:\n",
    "        if p not in seen:\n",
    "            seen.add(p); out.append(p)\n",
    "    return out\n",
    "\n",
    "# Temporary fallback if you still have sentence outputs\n",
    "def sentence_to_pair(line):\n",
    "    \"\"\"Parse sentence-style influences: 'Chemical X influences disease Y'\"\"\"\n",
    "    m = re.match(r\"^\\s*chemical\\s+(.+?)\\s+influences\\s+disease\\s+(.+?)\\s*$\", line, re.I)\n",
    "    return (m.group(1), m.group(2)) if m else None\n",
    "\n",
    "print(\"‚úì Task classification and filter functions loaded\")\n",
    "print(\"  Functions:\")\n",
    "print(\"    - task_from_prompt(): Classify task type\")\n",
    "print(\"    - filter_items_against_text(): Keep only entities in source text\")\n",
    "print(\"    - parse_pairs(): Parse 'chemical | disease' pairs\")\n",
    "print(\"    - filter_pairs_against_text(): Keep pairs where both sides exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857449b",
   "metadata": {},
   "source": [
    "## 7. Evaluate on the Held-Out Test Set\n",
    "\n",
    "Run inference on the test set with deterministic generation and post-filters.\n",
    "\n",
    "**Key Features**:\n",
    "- **Deterministic generation**: No sampling (do_sample=False)\n",
    "- **Post-filters**: Keep only entities that appear in source text\n",
    "- **Per-task metrics**: Separate P/R/F1 for chemicals, diseases, influences\n",
    "- **Sanity checks**: Show examples of false positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6043fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Evaluation with per-task metrics and filters =====\n",
    "from statistics import mean\n",
    "\n",
    "def f1(p, r): \n",
    "    return 0.0 if (p+r)==0 else 2*p*r/(p+r)\n",
    "\n",
    "# Load test data\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"‚úì Loaded test set: {len(test_data)} samples\")\n",
    "print(f\"\\n‚ö†Ô∏è  IMPORTANT:\")\n",
    "print(f\"  - Training set (80%): Used for fine-tuning\")\n",
    "print(f\"  - Validation set (10%): Monitored during training (W&B)\")\n",
    "print(f\"  - Test set (10%): Used ONLY NOW for final evaluation\")\n",
    "print(f\"\\nRunning evaluation with deterministic generation + post-filters...\")\n",
    "\n",
    "# Initialize per-task counters\n",
    "gold_total = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "pred_total = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "tp_total   = {\"chemicals\":0, \"diseases\":0, \"influences\":0}\n",
    "\n",
    "examples_fp = []  # False positives\n",
    "examples_fn = []  # False negatives\n",
    "\n",
    "# Process each test sample\n",
    "for idx, row in enumerate(test_data):\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"  Progress: {idx + 1}/{len(test_data)} samples...\")\n",
    "    \n",
    "    prompt = row[\"prompt\"]\n",
    "    gold_items = [normalize_item(x) for x in parse_bullets(row.get(\"completion\",\"\"))]\n",
    "    task = task_from_prompt(prompt)\n",
    "    \n",
    "    # Generate prediction\n",
    "    gen = generate_response(prompt, max_new_tokens=128)\n",
    "    pred_raw = extract_list_from_generation(gen)\n",
    "    \n",
    "    # Apply filters based on task type\n",
    "    if task in {\"chemicals\", \"diseases\"}:\n",
    "        pred = filter_items_against_text(pred_raw, prompt)\n",
    "    elif task == \"influences\":\n",
    "        pairs = parse_pairs(gen)  # expecting new spec: \"chemical | disease\"\n",
    "        # Normalize both sides of the pair for consistent comparison\n",
    "        pred = [f\"{normalize_item(c)} | {normalize_item(d)}\" \n",
    "                for (c,d) in filter_pairs_against_text(pairs, prompt)]\n",
    "        # gold_items already normalized on line 507 - don't normalize again\n",
    "    else:\n",
    "        pred = []\n",
    "    \n",
    "    # Convert to sets for metrics\n",
    "    gs = set(gold_items)\n",
    "    ps = set(pred)\n",
    "    \n",
    "    tp = len(gs & ps)\n",
    "    fp = len(ps - gs)\n",
    "    fn = len(gs - ps)\n",
    "    \n",
    "    gold_total[task] += len(gs)\n",
    "    pred_total[task] += len(ps)\n",
    "    tp_total[task]   += tp\n",
    "    \n",
    "    # Collect examples for analysis\n",
    "    if fp and len(examples_fp) < 8:\n",
    "        examples_fp.append({\n",
    "            \"task\": task,\n",
    "            \"prompt_preview\": prompt[:120]+\"...\",\n",
    "            \"pred_extras\": list(ps-gs)[:5]\n",
    "        })\n",
    "    if fn and len(examples_fn) < 8:\n",
    "        examples_fn.append({\n",
    "            \"task\": task,\n",
    "            \"prompt_preview\": prompt[:120]+\"...\",\n",
    "            \"missed\": list(gs-ps)[:5]\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úì Evaluation complete!\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PER-TASK METRICS (with post-filters)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Calculate and display metrics for each task\n",
    "for t in [\"chemicals\", \"diseases\", \"influences\"]:\n",
    "    P = 0.0 if pred_total[t]==0 else tp_total[t]/pred_total[t]\n",
    "    R = 0.0 if gold_total[t]==0 else tp_total[t]/gold_total[t]\n",
    "    F = f1(P,R)\n",
    "    print(f\"{t.upper()}\")\n",
    "    print(f\"  Precision: {P*100:5.1f}%  (TP={tp_total[t]}, Pred={pred_total[t]})\")\n",
    "    print(f\"  Recall:    {R*100:5.1f}%  (TP={tp_total[t]}, Gold={gold_total[t]})\")\n",
    "    print(f\"  F1 Score:  {F*100:5.1f}%\")\n",
    "    print()\n",
    "\n",
    "# Overall metrics\n",
    "total_tp = sum(tp_total.values())\n",
    "total_pred = sum(pred_total.values())\n",
    "total_gold = sum(gold_total.values())\n",
    "overall_P = 0.0 if total_pred==0 else total_tp/total_pred\n",
    "overall_R = 0.0 if total_gold==0 else total_tp/total_gold\n",
    "overall_F = f1(overall_P, overall_R)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"OVERALL METRICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Precision: {overall_P*100:5.1f}%\")\n",
    "print(f\"  Recall:    {overall_R*100:5.1f}%\")\n",
    "print(f\"  F1 Score:  {overall_F*100:5.1f}%\")\n",
    "print(f\"\\n  Total TP: {total_tp}, Total Pred: {total_pred}, Total Gold: {total_gold}\")\n",
    "\n",
    "# Show example errors\n",
    "if examples_fp:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXAMPLE FALSE POSITIVES (model predicted, but not in gold)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for e in examples_fp[:5]:\n",
    "        print(f\"\\nTask: {e['task']}\")\n",
    "        print(f\"Prompt: {e['prompt_preview']}\")\n",
    "        print(f\"Extra predictions: {e['pred_extras']}\")\n",
    "\n",
    "if examples_fn:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXAMPLE FALSE NEGATIVES (in gold, but model missed)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for e in examples_fn[:5]:\n",
    "        print(f\"\\nTask: {e['task']}\")\n",
    "        print(f\"Prompt: {e['prompt_preview']}\")\n",
    "        print(f\"Missed items: {e['missed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3d158",
   "metadata": {},
   "source": [
    "## 7.5 False Positive Analysis\n",
    "\n",
    "Analyze the types of errors the model is making to understand and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed False Positive Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"FALSE POSITIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Re-analyze test data to collect all false positives\n",
    "all_false_positives = []\n",
    "all_false_negatives = []\n",
    "all_true_positives = []\n",
    "\n",
    "for i, sample in enumerate(test_data[:num_test_samples]):\n",
    "    prediction = generate_response(sample['prompt'])\n",
    "    \n",
    "    # Normalize for comparison\n",
    "    expected_items = set([normalize_text(item) for item in sample['completion'].split('\\n') if item.strip()])\n",
    "    predicted_items = set([normalize_text(item) for item in prediction.split('\\n') if item.strip()])\n",
    "    \n",
    "    common = expected_items & predicted_items\n",
    "    false_positives = predicted_items - expected_items  # Model predicted but not in ground truth\n",
    "    false_negatives = expected_items - predicted_items  # In ground truth but model missed\n",
    "    \n",
    "    all_false_positives.extend(false_positives)\n",
    "    all_false_negatives.extend(false_negatives)\n",
    "    all_true_positives.extend(common)\n",
    "\n",
    "print(f\"\\nüìä Error Distribution:\")\n",
    "print(f\"  True Positives:   {len(all_true_positives)} (Correct predictions)\")\n",
    "print(f\"  False Positives:  {len(all_false_positives)} (Extra/wrong predictions)\")\n",
    "print(f\"  False Negatives:  {len(all_false_negatives)} (Missed entities)\")\n",
    "\n",
    "# Calculate error rates\n",
    "total_predictions = len(all_true_positives) + len(all_false_positives)\n",
    "total_ground_truth = len(all_true_positives) + len(all_false_negatives)\n",
    "\n",
    "false_positive_rate = len(all_false_positives) / total_predictions * 100 if total_predictions > 0 else 0\n",
    "false_negative_rate = len(all_false_negatives) / total_ground_truth * 100 if total_ground_truth > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà Error Rates:\")\n",
    "print(f\"  False Positive Rate: {false_positive_rate:.1f}% (of all predictions)\")\n",
    "print(f\"  False Negative Rate: {false_negative_rate:.1f}% (of all expected)\")\n",
    "\n",
    "# Show example false positives\n",
    "print(f\"\\n‚ö†Ô∏è  Example False Positives (Extra predictions):\")\n",
    "for i, fp in enumerate(all_false_positives[:10], 1):\n",
    "    print(f\"  {i}. {fp}\")\n",
    "\n",
    "# Show example false negatives\n",
    "print(f\"\\n‚ùå Example False Negatives (Missed entities):\")\n",
    "for i, fn in enumerate(all_false_negatives[:10], 1):\n",
    "    print(f\"  {i}. {fn}\")\n",
    "\n",
    "# Analysis insights\n",
    "print(f\"\\nüí° Insights:\")\n",
    "if false_positive_rate > 20:\n",
    "    print(f\"  ‚ö†Ô∏è  High false positive rate ({false_positive_rate:.1f}%)\")\n",
    "    print(f\"     ‚Üí Model is too aggressive, predicting entities that aren't in ground truth\")\n",
    "    print(f\"     ‚Üí Consider: More conservative prompting, post-processing filters, or additional training\")\n",
    "elif false_positive_rate < 10:\n",
    "    print(f\"  ‚úì Low false positive rate ({false_positive_rate:.1f}%)\")\n",
    "    print(f\"     ‚Üí Model is conservative and precise\")\n",
    "\n",
    "if false_negative_rate > 20:\n",
    "    print(f\"  ‚ö†Ô∏è  High false negative rate ({false_negative_rate:.1f}%)\")\n",
    "    print(f\"     ‚Üí Model is missing many expected entities\")\n",
    "    print(f\"     ‚Üí Consider: More training data, longer context, or prompt engineering\")\n",
    "elif false_negative_rate < 10:\n",
    "    print(f\"  ‚úì Low false negative rate ({false_negative_rate:.1f}%)\")\n",
    "    print(f\"     ‚Üí Model has good recall\")\n",
    "\n",
    "print(f\"\\nüéØ Recommendations:\")\n",
    "if false_positive_rate > false_negative_rate:\n",
    "    print(f\"  Primary issue: TOO MANY FALSE POSITIVES\")\n",
    "    print(f\"  Solutions:\")\n",
    "    print(f\"    1. Add post-processing to filter common false positives\")\n",
    "    print(f\"    2. Adjust generation parameters (lower temperature, higher top_p)\")\n",
    "    print(f\"    3. Fine-tune with more negative examples\")\n",
    "    print(f\"    4. Use stricter prompt instructions\")\n",
    "else:\n",
    "    print(f\"  Primary issue: TOO MANY FALSE NEGATIVES\")\n",
    "    print(f\"  Solutions:\")\n",
    "    print(f\"    1. Increase training data quantity\")\n",
    "    print(f\"    2. Improve prompt clarity\")\n",
    "    print(f\"    3. Check if test data format matches training data\")\n",
    "    print(f\"    4. Consider ensemble methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8545782",
   "metadata": {},
   "source": [
    "## 8. Interpret the Metrics\n",
    "\n",
    "### Accuracy\n",
    "- **Formula**: `Correct / Total Expected`\n",
    "- **Meaning**: Percentage of expected entities that were correctly predicted\n",
    "- **Limitation**: Doesn't account for false positives (extra predictions)\n",
    "\n",
    "### Precision\n",
    "- **Formula**: `Correct / Total Predicted`\n",
    "- **Meaning**: Of all entities the model predicted, how many were correct?\n",
    "- **High Precision**: Model rarely makes false positive errors (rarely predicts wrong entities)\n",
    "\n",
    "### Recall\n",
    "- **Formula**: `Correct / Total Expected`\n",
    "- **Meaning**: Of all actual entities, how many did the model find?\n",
    "- **High Recall**: Model rarely makes false negative errors (rarely misses entities)\n",
    "\n",
    "### F1 Score\n",
    "- **Formula**: `2 √ó (Precision √ó Recall) / (Precision + Recall)`\n",
    "- **Meaning**: Harmonic mean that balances precision and recall\n",
    "- **Best metric**: When you care equally about false positives and false negatives\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Ground truth: ['aspirin', 'ibuprofen', 'NSAIDs']\n",
    "Prediction:   ['aspirin', 'ibuprofen']\n",
    "\n",
    "Accuracy:  66.7% (2/3 found)\n",
    "Precision: 100% (2/2 predicted were correct)\n",
    "Recall:    66.7% (2/3 actual entities found)\n",
    "F1 Score:  80.0% (balanced metric)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5800935",
   "metadata": {},
   "source": [
    "## 9. Custom Test Cases ‚Äî Comprehensive NER Evaluation\n",
    "\n",
    "Test the model's ability to:\n",
    "1. **Extract Chemicals** - Identify drug names and chemical compounds\n",
    "2. **Extract Diseases** - Identify medical conditions and diseases\n",
    "3. **Extract Relationships** - Identify which chemicals are related to which diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Chemical Extraction\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: CHEMICAL EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chemical_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "A patient was treated with aspirin and ibuprofen for pain relief. The combination of these NSAIDs proved effective in reducing inflammation. Additionally, metformin was prescribed for glucose control.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{chemical_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(chemical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ef19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Disease Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: DISEASE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "disease_test = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the diseases mentioned.\n",
    "\n",
    "The patient presented with hypertension, diabetes mellitus, and chronic kidney disease. Laboratory findings revealed proteinuria and elevated creatinine levels, suggesting diabetic nephropathy.\n",
    "\n",
    "List of extracted diseases:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{disease_test}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(disease_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b12487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Chemical-Disease Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: RELATIONSHIP EXTRACTION - BASIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_1 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract the relationships between chemicals and diseases mentioned in the text.\n",
    "\n",
    "Metformin is commonly prescribed for type 2 diabetes by improving insulin sensitivity and reducing hepatic glucose production. Aspirin is used in cardiovascular disease management in high-risk patients.\n",
    "\n",
    "List the chemical-disease relationships:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_1}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_1, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Multiple Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: RELATIONSHIP EXTRACTION - MULTIPLE PAIRS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_2 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Identify all chemical-disease pairs and their relationships.\n",
    "\n",
    "Long-term use of corticosteroids is associated with osteoporosis and increases the risk of bone fractures. NSAIDs are linked to chronic kidney disease and gastrointestinal bleeding in susceptible patients.\n",
    "\n",
    "List of chemical-disease relationships:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_2}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_2, max_new_tokens=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Complex Multi-Entity Relationship Extraction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 5: COMPREHENSIVE EXTRACTION - ALL ENTITIES & RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relationship_test_3 = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Extract:\n",
    "1. All chemicals mentioned\n",
    "2. All diseases mentioned\n",
    "3. All relationships between chemicals and diseases\n",
    "\n",
    "The patient with rheumatoid arthritis was started on methotrexate for inflammatory joint disease. However, methotrexate is associated with hepatotoxicity and requires monitoring. The patient also has hypertension managed with lisinopril. Statins were prescribed for cardiovascular disease prevention given elevated cholesterol levels.\n",
    "\n",
    "Extracted information:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Prompt:\\n{relationship_test_3}\")\n",
    "print(\"\\nü§ñ Model Output:\")\n",
    "print(generate_response(relationship_test_3, max_new_tokens=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51eef3b",
   "metadata": {},
   "source": [
    "## 10. Suggested Next Steps\n",
    "\n",
    "- Evaluate the full test set (set `num_test_samples = len(test_data)`) to capture complete performance.\n",
    "- Compare with the base model to quantify the lift from fine-tuning.\n",
    "- Log metrics to Weights & Biases or another tracker for experiment history.\n",
    "- Export predictions for manual spot checks with subject-matter experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebbaa6",
   "metadata": {},
   "source": [
    "## 11. Usage Example (Optional)\n",
    "\n",
    "How to load the model in a production script or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and use the model later\n",
    "usage_code = '''\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter from Hub\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"your-username/llama3-medical-ner-lora\"  # Your model ID\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Use the model\n",
    "prompt = \"\"\"The following article contains technical terms including diseases, drugs and chemicals. Create a list only of the chemicals mentioned.\n",
    "\n",
    "Patient was treated with metformin and insulin for diabetes management.\n",
    "\n",
    "List of extracted chemicals:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "# ... (use the generate_response function from above)\n",
    "'''\n",
    "\n",
    "print(\"Usage Example:\")\n",
    "print(\"=\"*80)\n",
    "print(usage_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d1ad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ‚úÖ Configured environment variables and authentication for Hugging Face and W&B.\n",
    "2. ‚úÖ Installed required evaluation dependencies.\n",
    "3. ‚úÖ Loaded the fine-tuned medical NER model (base + LoRA adapter).\n",
    "4. ‚úÖ Evaluated performance on unseen test samples with detailed metrics.\n",
    "5. ‚úÖ Aggregated precision, recall, and F1 across all evaluated examples.\n",
    "6. ‚úÖ Validated behaviour on curated chemical, disease, and relationship prompts.\n",
    "7. ‚úÖ Outlined next steps and provided a ready-to-use inference snippet.\n",
    "\n",
    "**Your medical NER evaluation workflow is ready! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
